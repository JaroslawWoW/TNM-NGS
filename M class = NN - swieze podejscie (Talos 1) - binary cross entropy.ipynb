{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-iraqi",
   "metadata": {},
   "source": [
    "## 1. Biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compressed-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import talos as ta\n",
    "from talos.model.early_stopper import early_stopper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext tensorboard\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from focal_loss import BinaryFocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32aa6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define our custom loss function.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        # Define epsilon so that the back-propagation will not result in NaN for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        # y_pred = y_pred + epsilon\n",
    "        # Clip the prediciton value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        # Calculate alpha_t\n",
    "        alpha_factor = K.ones_like(y_true) * alpha\n",
    "        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -K.log(p_t)\n",
    "        weight = alpha_t * K.pow((1 - p_t), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.mean(K.sum(loss, axis=1))\n",
    "        return loss\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "\n",
    "def categorical_focal_loss(alpha, gamma=2.):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "    When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "    loss.\n",
    "           m\n",
    "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "      categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Compute mean loss in mini_batch\n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "    return categorical_focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-upset",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "humanitarian-desperate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/Dane_do_uczenia_M.csv\", encoding=\"utf-8\")\n",
    "del train_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b3fcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000212232.1</th>\n",
       "      <th>ENSG00000238741.1</th>\n",
       "      <th>ENSG00000252481.1</th>\n",
       "      <th>ENSG00000239002.3</th>\n",
       "      <th>ENSG00000212443.1</th>\n",
       "      <th>ENSG00000274012.1</th>\n",
       "      <th>ENSG00000252010.1</th>\n",
       "      <th>ENSG00000202198.1</th>\n",
       "      <th>ENSG00000251791.1</th>\n",
       "      <th>ENSG00000202058.1</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000241475.1</th>\n",
       "      <th>ENSG00000274618.1</th>\n",
       "      <th>ENSG00000227293.1</th>\n",
       "      <th>ENSG00000253526.1</th>\n",
       "      <th>ENSG00000270654.1</th>\n",
       "      <th>ENSG00000271394.1</th>\n",
       "      <th>ENSG00000265423.1</th>\n",
       "      <th>ENSG00000253165.1</th>\n",
       "      <th>ENSG00000201901.1</th>\n",
       "      <th>scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.356617</td>\n",
       "      <td>31.768974</td>\n",
       "      <td>27.356617</td>\n",
       "      <td>5.294829</td>\n",
       "      <td>8.824715</td>\n",
       "      <td>6.645010e+02</td>\n",
       "      <td>2.647415</td>\n",
       "      <td>4.235863e+02</td>\n",
       "      <td>7.059772</td>\n",
       "      <td>16.766959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.695633</td>\n",
       "      <td>1.086954</td>\n",
       "      <td>6.521724</td>\n",
       "      <td>2.173908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.758692e+03</td>\n",
       "      <td>1.086954</td>\n",
       "      <td>5.434770e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.185177</td>\n",
       "      <td>77.002713</td>\n",
       "      <td>7.475992</td>\n",
       "      <td>4.485595</td>\n",
       "      <td>8.971190</td>\n",
       "      <td>2.775836e+03</td>\n",
       "      <td>2.242797</td>\n",
       "      <td>2.609121e+02</td>\n",
       "      <td>6.728392</td>\n",
       "      <td>10.466388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.747599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.157930</td>\n",
       "      <td>17.431612</td>\n",
       "      <td>6.536855</td>\n",
       "      <td>2.178952</td>\n",
       "      <td>2.905269</td>\n",
       "      <td>2.338741e+02</td>\n",
       "      <td>3.631586</td>\n",
       "      <td>6.827382e+01</td>\n",
       "      <td>1.452634</td>\n",
       "      <td>1.452634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29912.168049</td>\n",
       "      <td>21631.677176</td>\n",
       "      <td>9554.333460</td>\n",
       "      <td>20332.131551</td>\n",
       "      <td>5136.495208</td>\n",
       "      <td>1.255850e+06</td>\n",
       "      <td>19221.760289</td>\n",
       "      <td>1.926818e+06</td>\n",
       "      <td>5198.182500</td>\n",
       "      <td>2655.637935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>953.068666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.562431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269.367843</td>\n",
       "      <td>64.771657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.374585</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000212232.1  ENSG00000238741.1  ENSG00000252481.1  ENSG00000239002.3  \\\n",
       "0          27.356617          31.768974          27.356617           5.294829   \n",
       "1           8.695633           1.086954           6.521724           2.173908   \n",
       "2          20.185177          77.002713           7.475992           4.485595   \n",
       "3          18.157930          17.431612           6.536855           2.178952   \n",
       "4       29912.168049       21631.677176        9554.333460       20332.131551   \n",
       "\n",
       "   ENSG00000212443.1  ENSG00000274012.1  ENSG00000252010.1  ENSG00000202198.1  \\\n",
       "0           8.824715       6.645010e+02           2.647415       4.235863e+02   \n",
       "1           0.000000       1.758692e+03           1.086954       5.434770e+01   \n",
       "2           8.971190       2.775836e+03           2.242797       2.609121e+02   \n",
       "3           2.905269       2.338741e+02           3.631586       6.827382e+01   \n",
       "4        5136.495208       1.255850e+06       19221.760289       1.926818e+06   \n",
       "\n",
       "   ENSG00000251791.1  ENSG00000202058.1  ...  ENSG00000241475.1  \\\n",
       "0           7.059772          16.766959  ...           0.000000   \n",
       "1           0.000000           0.000000  ...           0.000000   \n",
       "2           6.728392          10.466388  ...           0.000000   \n",
       "3           1.452634           1.452634  ...           0.726317   \n",
       "4        5198.182500        2655.637935  ...           0.000000   \n",
       "\n",
       "   ENSG00000274618.1  ENSG00000227293.1  ENSG00000253526.1  ENSG00000270654.1  \\\n",
       "0           0.000000                0.0           0.000000                0.0   \n",
       "1           0.000000                0.0           0.000000                0.0   \n",
       "2           0.747599                0.0           0.747599                0.0   \n",
       "3           0.000000                0.0           0.000000                0.0   \n",
       "4         953.068666                0.0          20.562431                0.0   \n",
       "\n",
       "   ENSG00000271394.1  ENSG00000265423.1  ENSG00000253165.1  ENSG00000201901.1  \\\n",
       "0           0.000000           0.000000                0.0           0.000000   \n",
       "1           0.000000           0.000000                0.0           0.000000   \n",
       "2           0.747599           0.000000                0.0           0.000000   \n",
       "3           0.000000           0.000000                0.0           0.000000   \n",
       "4         269.367843          64.771657                0.0         123.374585   \n",
       "\n",
       "   scale  \n",
       "0     M0  \n",
       "1     M0  \n",
       "2     M0  \n",
       "3     M0  \n",
       "4     M0  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c392d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['scale'].loc[(train_df['scale'] == 'M0')] = 0\n",
    "train_df['scale'].loc[(train_df['scale'] == 'M1')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db4b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.loc[(train_df['scale']!='MX')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33410b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42,stratify=train_df['scale'])\n",
    "#test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42,stratify=test_df['scale'])\n",
    "\n",
    "\n",
    "train_label=train_df['scale']\n",
    "test_label=test_df['scale']\n",
    "#val_label=val_df['scale']\n",
    "del train_df['scale']\n",
    "del test_df['scale']\n",
    "#del val_df['scale']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef6a0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6644f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cedcb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1cca4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-precipitation",
   "metadata": {},
   "source": [
    "## 1.2 Standaryzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indonesian-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df=scaler.fit_transform(train_df)\n",
    "test_df=scaler.fit_transform(test_df)\n",
    "#val_df=scaler.fit_transform(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcc66d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23532639, -0.28536286, -0.29189695, ..., -0.20273415,\n",
       "        -0.13658188, -0.25716709],\n",
       "       [-0.23488914, -0.28412701, -0.28766847, ..., -0.20273415,\n",
       "        -0.13658188, -0.25716709],\n",
       "       [-0.23513615, -0.28194648, -0.28119849, ..., -0.1127296 ,\n",
       "        -0.13658188, -0.18291094],\n",
       "       ...,\n",
       "       [-0.23560858, -0.28411483, -0.29158105, ..., -0.20273415,\n",
       "        -0.07110588, -0.25716709],\n",
       "       [13.92769305,  3.11477926,  6.81130447, ..., -0.05473062,\n",
       "        -0.13658188,  1.2895196 ],\n",
       "       [-0.23495945, -0.28021459, -0.28164044, ..., -0.20273415,\n",
       "        -0.07405669, -0.25716709]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "humanitarian-sperm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 110)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2486bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=np.asarray(train_label).astype(np.int)\n",
    "test_label=np.asarray(test_label).astype(np.int)\n",
    "#val_label=np.asarray(val_label).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23fc815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "263db0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633de496",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_label.reshape(-1,1)\n",
    "test_label=test_label.reshape(-1,1)\n",
    "#val_label=val_label.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "750b76b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-proportion",
   "metadata": {},
   "source": [
    "# 2 Moduł TALOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-stick",
   "metadata": {},
   "source": [
    "## 2.1 Słownik parametrów do wypróbowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed670b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[110,220], \n",
    "     'hidden_neuron':[50,100],\n",
    "\n",
    "     'hidden_layers':[1,3,5],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [16,32],\n",
    "\n",
    "    'lr':[0.01],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "#    'batc_normalization':[False,True],\n",
    "#    'dropout': [0,0.2,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "    #'optimizer': ['rmsprop','adam','adadelta','adamax','nadam','adagrad'],\n",
    "    #'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "    'kernel_initializer': ['orthogonal'],\n",
    "    #'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    "    'activation_layer':['relu'],\n",
    "    #'batc_normalization':[False,True]\n",
    "    'batc_normalization':[False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aeff143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.Adamax(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=1.0, threshold=0.5),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=0,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-married",
   "metadata": {},
   "source": [
    "## 2.3 Przeprowadzam skan, używając parametrów i funkcji wyżej\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec12a0dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C29AE5048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FBetaScore.update_state of <tensorflow_addons.metrics.f_scores.FBetaScore object at 0x0000018C2E1FD508>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FBetaScore.result of <tensorflow_addons.metrics.f_scores.FBetaScore object at 0x0000018C2E1FD508>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C36FFF0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▍                                                                               | 1/24 [00:05<02:15,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C342D98B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C38C9F8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▉                                                                            | 2/24 [00:11<02:06,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C2E1F23A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C39D6CC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▍                                                                        | 3/24 [00:18<02:16,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35CB55E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C38C40558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▊                                                                     | 4/24 [00:25<02:12,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35CB5B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C2E1F2AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▎                                                                 | 5/24 [00:36<02:31,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35E71048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35CB5318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 6/24 [00:45<02:29,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35EE3CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B36F558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████▏                                                          | 7/24 [00:50<02:06,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3A0C20D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C36FFF438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████▋                                                       | 8/24 [00:56<01:51,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3459D798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C342A9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                   | 9/24 [01:03<01:44,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3713A1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B6B09D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▏                                               | 10/24 [01:11<01:41,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C36FFFDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B64E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████▌                                            | 11/24 [01:25<02:01,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 16, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 5, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35703DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3713A8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 12/24 [01:35<01:53,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C29AE1678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3705EE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|████████████████████████████████████████████▍                                     | 13/24 [01:40<01:29,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B218828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C2E1F25E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▊                                  | 14/24 [01:47<01:16,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3600CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C36FFF048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████▎                              | 15/24 [01:53<01:05,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C342A9798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B2185E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████▋                           | 16/24 [01:59<00:55,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B6B05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35CF0558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████████                        | 17/24 [02:07<00:51,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C387E3708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3CB1DC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 18/24 [02:19<00:51,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C385A8AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C387E3D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▉                 | 19/24 [02:24<00:38,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35E71DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3A0C2048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 20/24 [02:30<00:28,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3600C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3A0C2798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 21/24 [02:37<00:20,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C36FFF798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C38415828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 22/24 [02:43<00:13,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3713A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35CFC708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████████▌   | 23/24 [02:51<00:07,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 5, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C342D9828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3703FEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [03:01<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b6201bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/M/Predykcja klasy M - podejscie z Focal-loss (nowe)/050522200516.csv')\n",
    "#binary cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49812ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stare_wart=df['val_fbeta_score']\n",
    "nowe_wart=[]\n",
    "for x in stare_wart:\n",
    "    wart=x.replace('[','')\n",
    "    wart=wart.replace(']','')\n",
    "    wart=float(wart)\n",
    "    nowe_wart.append(wart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc9a39a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14814815,\n",
       " 0.07407407,\n",
       " 0.13793103,\n",
       " 0.0,\n",
       " 0.14814815,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285715,\n",
       " 0.14814815,\n",
       " 0.07692307,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285715,\n",
       " 0.13793103,\n",
       " 0.14285715,\n",
       " 0.0,\n",
       " 0.16000001,\n",
       " 0.13793103,\n",
       " 0.07692307,\n",
       " 0.07692307,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.15384614]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05cb4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['val_fbeta_score']=nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c70d0a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a72ba34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['round_epochs', 'loss', 'fbeta_score', 'precision', 'recall',\n",
       "       'val_loss', 'val_fbeta_score', 'val_precision', 'val_recall',\n",
       "       'activation_layer', 'activity_regularizer', 'batc_normalization',\n",
       "       'batch_size', 'bias_regularizer', 'dropout', 'epochs', 'first_neuron',\n",
       "       'hidden_layers', 'hidden_neuron', 'kernel_initializer',\n",
       "       'kernel_regularizer_l1', 'kernel_regularizer_l2', 'last_activation',\n",
       "       'lr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5be1d39f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>111</td>\n",
       "      <td>0.393906</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.442490</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>113</td>\n",
       "      <td>0.404395</td>\n",
       "      <td>[0.13043478]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.450072</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>143</td>\n",
       "      <td>0.380517</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.459007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>137</td>\n",
       "      <td>0.380368</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.459688</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>0.381882</td>\n",
       "      <td>[0.2268041]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.461955</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>86</td>\n",
       "      <td>0.396855</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.469225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>77</td>\n",
       "      <td>0.395137</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.479357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>0.382846</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.480909</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70</td>\n",
       "      <td>0.392894</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.485268</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77</td>\n",
       "      <td>0.388409</td>\n",
       "      <td>[0.14893618]</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.488937</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>0.381456</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.492494</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>81</td>\n",
       "      <td>0.403722</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>81</td>\n",
       "      <td>0.375271</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.495854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>69</td>\n",
       "      <td>0.386907</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.496096</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71</td>\n",
       "      <td>0.387327</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.499419</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>103</td>\n",
       "      <td>0.371839</td>\n",
       "      <td>[0.20833333]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.500738</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>0.384119</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.502271</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>0.391275</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.505151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65</td>\n",
       "      <td>0.390594</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.506395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>78</td>\n",
       "      <td>0.391298</td>\n",
       "      <td>[0.1875]</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.507860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>69</td>\n",
       "      <td>0.391936</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78</td>\n",
       "      <td>0.396866</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.511005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>82</td>\n",
       "      <td>0.383829</td>\n",
       "      <td>[0.20618556]</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.513426</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>0.386292</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.515482</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "16           111  0.393906  [0.17021276]   1.000000  0.093023  0.442490   \n",
       "23           113  0.404395  [0.13043478]   1.000000  0.069767  0.450072   \n",
       "10           143  0.380517  [0.15053762]   1.000000  0.081395  0.459007   \n",
       "17           137  0.380368  [0.17021276]   1.000000  0.093023  0.459688   \n",
       "4            115  0.381882   [0.2268041]   1.000000  0.127907  0.461955   \n",
       "5             86  0.396855  [0.15053762]   1.000000  0.081395  0.469225   \n",
       "22            77  0.395137  [0.17021276]   1.000000  0.093023  0.479357   \n",
       "1             69  0.382846  [0.17021276]   1.000000  0.093023  0.480909   \n",
       "12            70  0.392894  [0.17021276]   1.000000  0.093023  0.485268   \n",
       "2             77  0.388409  [0.14893618]   0.875000  0.081395  0.488937   \n",
       "8             70  0.381456  [0.17021276]   1.000000  0.093023  0.492494   \n",
       "11            81  0.403722          [0.]   0.000000  0.000000  0.495430   \n",
       "20            81  0.375271  [0.17021276]   1.000000  0.093023  0.495854   \n",
       "19            69  0.386907  [0.17021276]   1.000000  0.093023  0.496096   \n",
       "9             71  0.387327  [0.17021276]   1.000000  0.093023  0.499419   \n",
       "13           103  0.371839  [0.20833333]   1.000000  0.116279  0.500738   \n",
       "7             64  0.384119  [0.17021276]   1.000000  0.093023  0.502271   \n",
       "3             69  0.391275  [0.17021276]   1.000000  0.093023  0.505151   \n",
       "6             65  0.390594  [0.17021276]   1.000000  0.093023  0.506395   \n",
       "15            78  0.391298      [0.1875]   0.900000  0.104651  0.507860   \n",
       "18            69  0.391936  [0.17021276]   1.000000  0.093023  0.508042   \n",
       "21            78  0.396866  [0.15053762]   1.000000  0.081395  0.511005   \n",
       "14            82  0.383829  [0.20618556]   0.909091  0.116279  0.513426   \n",
       "0             68  0.386292  [0.17021276]   1.000000  0.093023  0.515482   \n",
       "\n",
       "    val_fbeta_score  val_precision  val_recall activation_layer  ...  dropout  \\\n",
       "16         0.160000       0.666667    0.090909             relu  ...        0   \n",
       "23         0.153846       0.500000    0.090909             relu  ...        0   \n",
       "10         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "17         0.137931       0.285714    0.090909             relu  ...        0   \n",
       "4          0.148148       0.400000    0.090909             relu  ...        0   \n",
       "5          0.000000       0.000000    0.000000             relu  ...        0   \n",
       "22         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "1          0.074074       0.200000    0.045455             relu  ...        0   \n",
       "12         0.142857       0.333333    0.090909             relu  ...        0   \n",
       "2          0.137931       0.285714    0.090909             relu  ...        0   \n",
       "8          0.148148       0.400000    0.090909             relu  ...        0   \n",
       "11         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "20         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "19         0.076923       0.250000    0.045455             relu  ...        0   \n",
       "9          0.076923       0.250000    0.045455             relu  ...        0   \n",
       "13         0.137931       0.285714    0.090909             relu  ...        0   \n",
       "7          0.142857       0.333333    0.090909             relu  ...        0   \n",
       "3          0.000000       0.000000    0.000000             relu  ...        0   \n",
       "6          0.000000       0.000000    0.000000             relu  ...        0   \n",
       "15         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "18         0.076923       0.250000    0.045455             relu  ...        0   \n",
       "21         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "14         0.142857       0.333333    0.090909             relu  ...        0   \n",
       "0          0.148148       0.400000    0.090909             relu  ...        0   \n",
       "\n",
       "    epochs  first_neuron  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "16  100000           110              5             50          orthogonal   \n",
       "23  100000           220              5            100          orthogonal   \n",
       "10  100000           220              5             50          orthogonal   \n",
       "17  100000           110              5            100          orthogonal   \n",
       "4   100000           110              5             50          orthogonal   \n",
       "5   100000           110              5            100          orthogonal   \n",
       "22  100000           220              5             50          orthogonal   \n",
       "1   100000           110              1            100          orthogonal   \n",
       "12  100000           110              1             50          orthogonal   \n",
       "2   100000           110              3             50          orthogonal   \n",
       "8   100000           220              3             50          orthogonal   \n",
       "11  100000           220              5            100          orthogonal   \n",
       "20  100000           220              3             50          orthogonal   \n",
       "19  100000           220              1            100          orthogonal   \n",
       "9   100000           220              3            100          orthogonal   \n",
       "13  100000           110              1            100          orthogonal   \n",
       "7   100000           220              1            100          orthogonal   \n",
       "3   100000           110              3            100          orthogonal   \n",
       "6   100000           220              1             50          orthogonal   \n",
       "15  100000           110              3            100          orthogonal   \n",
       "18  100000           220              1             50          orthogonal   \n",
       "21  100000           220              3            100          orthogonal   \n",
       "14  100000           110              3             50          orthogonal   \n",
       "0   100000           110              1             50          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation    lr  \n",
       "16                 0.0001                 0.0001          sigmoid  0.01  \n",
       "23                 0.0001                 0.0001          sigmoid  0.01  \n",
       "10                 0.0001                 0.0001          sigmoid  0.01  \n",
       "17                 0.0001                 0.0001          sigmoid  0.01  \n",
       "4                  0.0001                 0.0001          sigmoid  0.01  \n",
       "5                  0.0001                 0.0001          sigmoid  0.01  \n",
       "22                 0.0001                 0.0001          sigmoid  0.01  \n",
       "1                  0.0001                 0.0001          sigmoid  0.01  \n",
       "12                 0.0001                 0.0001          sigmoid  0.01  \n",
       "2                  0.0001                 0.0001          sigmoid  0.01  \n",
       "8                  0.0001                 0.0001          sigmoid  0.01  \n",
       "11                 0.0001                 0.0001          sigmoid  0.01  \n",
       "20                 0.0001                 0.0001          sigmoid  0.01  \n",
       "19                 0.0001                 0.0001          sigmoid  0.01  \n",
       "9                  0.0001                 0.0001          sigmoid  0.01  \n",
       "13                 0.0001                 0.0001          sigmoid  0.01  \n",
       "7                  0.0001                 0.0001          sigmoid  0.01  \n",
       "3                  0.0001                 0.0001          sigmoid  0.01  \n",
       "6                  0.0001                 0.0001          sigmoid  0.01  \n",
       "15                 0.0001                 0.0001          sigmoid  0.01  \n",
       "18                 0.0001                 0.0001          sigmoid  0.01  \n",
       "21                 0.0001                 0.0001          sigmoid  0.01  \n",
       "14                 0.0001                 0.0001          sigmoid  0.01  \n",
       "0                  0.0001                 0.0001          sigmoid  0.01  \n",
       "\n",
       "[24 rows x 24 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2365cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['val_fbeta_score']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91335ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16    32\n",
       "23    32\n",
       "17    32\n",
       "4     16\n",
       "1     16\n",
       "12    32\n",
       "2     16\n",
       "8     16\n",
       "19    32\n",
       "9     16\n",
       "13    32\n",
       "7     16\n",
       "18    32\n",
       "14    32\n",
       "0     16\n",
       "Name: batch_size, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26dee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82c44fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af70bb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>111</td>\n",
       "      <td>0.393906</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.442490</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>113</td>\n",
       "      <td>0.404395</td>\n",
       "      <td>[0.13043478]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.450072</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>143</td>\n",
       "      <td>0.380517</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.459007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>137</td>\n",
       "      <td>0.380368</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.459688</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>0.381882</td>\n",
       "      <td>[0.2268041]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.461955</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>86</td>\n",
       "      <td>0.396855</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.469225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>77</td>\n",
       "      <td>0.395137</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.479357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>0.382846</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.480909</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70</td>\n",
       "      <td>0.392894</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.485268</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77</td>\n",
       "      <td>0.388409</td>\n",
       "      <td>[0.14893618]</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.488937</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>0.381456</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.492494</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>81</td>\n",
       "      <td>0.403722</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>81</td>\n",
       "      <td>0.375271</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.495854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>69</td>\n",
       "      <td>0.386907</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.496096</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71</td>\n",
       "      <td>0.387327</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.499419</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>103</td>\n",
       "      <td>0.371839</td>\n",
       "      <td>[0.20833333]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.500738</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>0.384119</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.502271</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>0.391275</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.505151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65</td>\n",
       "      <td>0.390594</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.506395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>78</td>\n",
       "      <td>0.391298</td>\n",
       "      <td>[0.1875]</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.507860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>69</td>\n",
       "      <td>0.391936</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78</td>\n",
       "      <td>0.396866</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.511005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>82</td>\n",
       "      <td>0.383829</td>\n",
       "      <td>[0.20618556]</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.513426</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>0.386292</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.515482</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "16           111  0.393906  [0.17021276]   1.000000  0.093023  0.442490   \n",
       "23           113  0.404395  [0.13043478]   1.000000  0.069767  0.450072   \n",
       "10           143  0.380517  [0.15053762]   1.000000  0.081395  0.459007   \n",
       "17           137  0.380368  [0.17021276]   1.000000  0.093023  0.459688   \n",
       "4            115  0.381882   [0.2268041]   1.000000  0.127907  0.461955   \n",
       "5             86  0.396855  [0.15053762]   1.000000  0.081395  0.469225   \n",
       "22            77  0.395137  [0.17021276]   1.000000  0.093023  0.479357   \n",
       "1             69  0.382846  [0.17021276]   1.000000  0.093023  0.480909   \n",
       "12            70  0.392894  [0.17021276]   1.000000  0.093023  0.485268   \n",
       "2             77  0.388409  [0.14893618]   0.875000  0.081395  0.488937   \n",
       "8             70  0.381456  [0.17021276]   1.000000  0.093023  0.492494   \n",
       "11            81  0.403722          [0.]   0.000000  0.000000  0.495430   \n",
       "20            81  0.375271  [0.17021276]   1.000000  0.093023  0.495854   \n",
       "19            69  0.386907  [0.17021276]   1.000000  0.093023  0.496096   \n",
       "9             71  0.387327  [0.17021276]   1.000000  0.093023  0.499419   \n",
       "13           103  0.371839  [0.20833333]   1.000000  0.116279  0.500738   \n",
       "7             64  0.384119  [0.17021276]   1.000000  0.093023  0.502271   \n",
       "3             69  0.391275  [0.17021276]   1.000000  0.093023  0.505151   \n",
       "6             65  0.390594  [0.17021276]   1.000000  0.093023  0.506395   \n",
       "15            78  0.391298      [0.1875]   0.900000  0.104651  0.507860   \n",
       "18            69  0.391936  [0.17021276]   1.000000  0.093023  0.508042   \n",
       "21            78  0.396866  [0.15053762]   1.000000  0.081395  0.511005   \n",
       "14            82  0.383829  [0.20618556]   0.909091  0.116279  0.513426   \n",
       "0             68  0.386292  [0.17021276]   1.000000  0.093023  0.515482   \n",
       "\n",
       "    val_fbeta_score  val_precision  val_recall activation_layer  ...  dropout  \\\n",
       "16         0.160000       0.666667    0.090909             relu  ...        0   \n",
       "23         0.153846       0.500000    0.090909             relu  ...        0   \n",
       "10         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "17         0.137931       0.285714    0.090909             relu  ...        0   \n",
       "4          0.148148       0.400000    0.090909             relu  ...        0   \n",
       "5          0.000000       0.000000    0.000000             relu  ...        0   \n",
       "22         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "1          0.074074       0.200000    0.045455             relu  ...        0   \n",
       "12         0.142857       0.333333    0.090909             relu  ...        0   \n",
       "2          0.137931       0.285714    0.090909             relu  ...        0   \n",
       "8          0.148148       0.400000    0.090909             relu  ...        0   \n",
       "11         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "20         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "19         0.076923       0.250000    0.045455             relu  ...        0   \n",
       "9          0.076923       0.250000    0.045455             relu  ...        0   \n",
       "13         0.137931       0.285714    0.090909             relu  ...        0   \n",
       "7          0.142857       0.333333    0.090909             relu  ...        0   \n",
       "3          0.000000       0.000000    0.000000             relu  ...        0   \n",
       "6          0.000000       0.000000    0.000000             relu  ...        0   \n",
       "15         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "18         0.076923       0.250000    0.045455             relu  ...        0   \n",
       "21         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "14         0.142857       0.333333    0.090909             relu  ...        0   \n",
       "0          0.148148       0.400000    0.090909             relu  ...        0   \n",
       "\n",
       "    epochs  first_neuron  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "16  100000           110              5             50          orthogonal   \n",
       "23  100000           220              5            100          orthogonal   \n",
       "10  100000           220              5             50          orthogonal   \n",
       "17  100000           110              5            100          orthogonal   \n",
       "4   100000           110              5             50          orthogonal   \n",
       "5   100000           110              5            100          orthogonal   \n",
       "22  100000           220              5             50          orthogonal   \n",
       "1   100000           110              1            100          orthogonal   \n",
       "12  100000           110              1             50          orthogonal   \n",
       "2   100000           110              3             50          orthogonal   \n",
       "8   100000           220              3             50          orthogonal   \n",
       "11  100000           220              5            100          orthogonal   \n",
       "20  100000           220              3             50          orthogonal   \n",
       "19  100000           220              1            100          orthogonal   \n",
       "9   100000           220              3            100          orthogonal   \n",
       "13  100000           110              1            100          orthogonal   \n",
       "7   100000           220              1            100          orthogonal   \n",
       "3   100000           110              3            100          orthogonal   \n",
       "6   100000           220              1             50          orthogonal   \n",
       "15  100000           110              3            100          orthogonal   \n",
       "18  100000           220              1             50          orthogonal   \n",
       "21  100000           220              3            100          orthogonal   \n",
       "14  100000           110              3             50          orthogonal   \n",
       "0   100000           110              1             50          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation    lr  \n",
       "16                 0.0001                 0.0001          sigmoid  0.01  \n",
       "23                 0.0001                 0.0001          sigmoid  0.01  \n",
       "10                 0.0001                 0.0001          sigmoid  0.01  \n",
       "17                 0.0001                 0.0001          sigmoid  0.01  \n",
       "4                  0.0001                 0.0001          sigmoid  0.01  \n",
       "5                  0.0001                 0.0001          sigmoid  0.01  \n",
       "22                 0.0001                 0.0001          sigmoid  0.01  \n",
       "1                  0.0001                 0.0001          sigmoid  0.01  \n",
       "12                 0.0001                 0.0001          sigmoid  0.01  \n",
       "2                  0.0001                 0.0001          sigmoid  0.01  \n",
       "8                  0.0001                 0.0001          sigmoid  0.01  \n",
       "11                 0.0001                 0.0001          sigmoid  0.01  \n",
       "20                 0.0001                 0.0001          sigmoid  0.01  \n",
       "19                 0.0001                 0.0001          sigmoid  0.01  \n",
       "9                  0.0001                 0.0001          sigmoid  0.01  \n",
       "13                 0.0001                 0.0001          sigmoid  0.01  \n",
       "7                  0.0001                 0.0001          sigmoid  0.01  \n",
       "3                  0.0001                 0.0001          sigmoid  0.01  \n",
       "6                  0.0001                 0.0001          sigmoid  0.01  \n",
       "15                 0.0001                 0.0001          sigmoid  0.01  \n",
       "18                 0.0001                 0.0001          sigmoid  0.01  \n",
       "21                 0.0001                 0.0001          sigmoid  0.01  \n",
       "14                 0.0001                 0.0001          sigmoid  0.01  \n",
       "0                  0.0001                 0.0001          sigmoid  0.01  \n",
       "\n",
       "[24 rows x 24 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d383b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "482df808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of first_neuron')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'first_neuron'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f6a5105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_neuron')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_neuron'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84d40133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_layers')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_layers'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5043b151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of kernel_initializer')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_initializer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52005f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of activation_layer')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEXCAYAAABYsbiOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgOklEQVR4nO3de7wdZX3v8c/XQLgJiiRS3AkkVEQjFaubCJ6DpqV6uClSUAgCB7XQ2AJqpYWeo9bWYyuvth6MgDEiIl4ISChSjII9lUtVaDYIQriGAM1OELYIchOSHb/nj3k2rCzW7Kwke7J3ku/79VqvPfPMM8/8ZvZa81vPzKwZ2SYiIqKTl4x2ABERMXYlSURERK0kiYiIqJUkERERtZIkIiKiVpJERETUSpIYAyRZ0qvL8BxJn+ym7jos5/2Srl7XODdlkj4s6WFJT0naaQMu939JOm9DLa9luYdLWlrW9/cbXM7+ku5uqO1Gt52kGZL6m2p/Y6H8TmL9SboKuNH2p9rKDwO+DEyyPTjM/Ab2sL24i2V1VVfSFOB+YMvhlj0SJM0Avml7UpPLaYqkLYEngH1t39rgcmYwRraTpPuAv7D93RFut+v38lq2O4MNvO3G0v9rNKUnMTIuAI6TpLby44BvNb2TjvW2M7A1sGi0A9mAdmPzWt+NgqQtRjuGF7Gd13q+gG2AXwNvaynbEXgW2BuYDvwUeBx4CDgbGN9S18Cry/AFwP9pmfaXZZ7lwAfb6h4C/IzqW/BS4NMt8/1XqftUee0HnAD8R0udtwILS+wLgbe2TLsG+AzwY+BJ4GpgQs36zwD6a6a9rrT1ONVO6d0t0w4G7ijtLwNOK+UTgCvLPL8CrgdeUtP+F8q6PwHcBOzfMm060FemPQx8vsP8rwGebtlW/w5MKeNbtG2PPynDJwD/AfwT8BhVj+2glrqvAL5W/mePAZcD2wG/AX7b8j95FfBpqm+rQ/O+u2ynx8syX9cy7QHgNODn5X92MbB1zXZ5CfAJ4EHgEeBC4GXAVmXZLut93zps13HA/wLuK/+7m4DJwHUt7T4FHNX63gDOAC7tsJzZZfgDwJ2lzSXAn5byDbrt6t7XJf6hdb4DOLyUb0X1Pv29lrqvLDFPLOOHAreU2H4CvKEtttNLbM/R8r4bC69RD2BTeQFfAc5rGf9T4JYy/GZgX2ALqh3QncBHW+p2TBLAgVQ7t73KB+XbbXVnAL9HtUN4Q6n7njJtCi/e0Z1ASRJUO7LHqHo7WwAzy/hOZfo15QPxGqokeA3wuZp1X+3D1FK+JbCYaocyHvjD8gHbs0x/iLLzoUqqbyrD/wDMKfNvCexPOTTaYRnHAjuVdfg48IuhDz9VYj6uDL+U6nBSpzZW21Y12+4aVk8SK4ETqXaYH6ZKCEOHb79HtRPascT/9rrtRMuOjhcS1jvKfH9Vtt/4Mv0B4D+pdpCvoHofzapZpw+WeXcv634Z8I1O77l12K5/CdwG7AmI6ovQTp3aZfUksRvwDLBDGR9X3gP7lvFDgN8tbb691H3Tht52de9r4L1l/pdQJcCngV3KtHOBM1vqfgT41zL8JqpE/Zayzv+zxLNVS2y3UCXabUZ7X9b+yuGmkfN14L2Stinjx5cybN9k+wbbg7YfoDpP8fYu2nwf8DXbt9t+mupD8Tzb19i+zfZvbf8cuKjLdqH6QN5r+xslrouAu4B3tdT5mu17bP8GuAR4Y5dtD9mXagf1OdsrbP87VQ9hZpm+EpgmaQfbj9m+uaV8F2A32yttX+/yaWpn+5u2Hy3r8M9U3+r2bGnn1ZIm2H7K9g1rGf9wHrT9FdurqP7PuwA7S9oFOIhqB/RYif/aLts8Cvie7R/aXknVU9mGqsc3ZLbt5bZ/Bfwr9f+T91P1nJbYfgr4a+Dobg9nrGG7/gnwCdt3u3Kr7Ue7aPNB4GbgPaXoD4Fnhv4vtr9n+77S5rVUvdf9u4mXkd12dfF/p8z/W9sXA/dS9Vaheg8cI2lon3oc8I0yfCLwZds32l5l++tUPYZ922JbWj5rY0qSxAix/R/AAHCYpN2Bfai++SPpNZKulPQLSU8Af091SGVNXkXV5R/yYOtESW+R9CNJA5J+Dczqst2hth9sK3sQ6GkZ/0XL8DNUO/y18Spgqe3f1izjCKpDTg9KulbSfqX8H6m+BV4taYmkM+oWIOnjku6U9GtJj1MdUhnaBh+i+oZ5l6SFkg5dy/iH8/y2sf1MGXwp1bfBX9l+bB3aXO1/UrbbUtbtf9L+/32QqlewczeBrGG7TqbqZa6Lb/PCl4RjyvjQMg+SdIOkX5VlHsw6vp/Xc9t1JOl4SbdIerzEt9dQfLZvpOpZvF3Sa4FXA1eUWXcDPj40X5l3col5SOvnfExJkhhZF1L1II4Drrb9cCn/EtW39D1s70B1+KX9JHcnD1G9mYbs2jb921RvxMm2X0Z1iGao3TVdtrac6s3baleqcwMjZTkwueXb1WrLsL3Q9mFUx28vp+qtYPtJ2x+3vTtVz+YvJB3Q3rik/amO5b4P2NH2y6mON6u0c6/tmaX9M4FLJW3XRdxPl7/btpT9TldrXH3YXyHp5R2mrdX/pFwIMZl1+5+0/393BQapDkkOa03blWodf3cdYgL4DjBD0iTgcF74IrUVMJ+qB7BzWeYC1vH9vJ7b7kUk7UZ1SPlkqkNrLwduZ/XP8depDtMdR3Xu5dlSvhT4rO2Xt7y2Lb33IWtav1GTJDGyLgT+iKp7+fWW8u2pTgA+Vb5lfLjL9i4BTpA0TdK2wN+0Td+e6lvrs5KmU30zGzJAdaJv95q2FwCvkXSMpC0kHQVMozoctE4kbd36ojoG/DTwV5K2LJcUvguYJ2l8+d3Gy8rhgSeAVaWdQyW9unzQh8pXdVjk9lQ7vgFgC0mfAnZoiedYSRPLt8rHS3GndlZje4Bq53KspHGSPkiXO0XbDwHfB86VtGNZ77eVyQ8DO0l6Wc3slwCHSDqgXJb7carDEj/pZtltLgI+JmmqpJdS9V4vdndX2g27XYHzgM9I2kOVN+iF35Y8TP17bmjbXkN1Yv9+23eWSeOpDmkNAIOSDgLe2TLrhtx2nWxHtSMfAJD0AaqeRKtvUCW+Y6n2BUO+AswqPX9J2k7SIZK2H6HYGpUkMYLK+YafUL2hrmiZdBrVDvxJqjfMxV22933gLKorbhaXv63+DPg7SU8Cn6J8Ey/zPgN8Fvhx6eK2Hv+kHEM+lOrD9CjVib5Dbf+ym9g66KG6mqP1NZnqipODgF9Sndw73vZdZZ7jgAfKIbhZVB8ugD2Af6O6iuWnwLm2r+mwzKuodsj3UB1qeJbVu+0HAoskPUV1Fc3RLd/u1uREqhO0jwKvZ+12NsdRnQ+5i+qE5UcBynpfBCwp/5PWww3YvptqG3yRanu9C3iX7RVrsewh51PttK6juvrqWeCULudd03b9PNV77WqqJP5VquP/UJ03+3pZv/fVtP9tqi9Tzx9qsv0kcGpp9zGqz8sVLdM35LZ7Edt3AP9M9X58mOqCkR+31emnOudiqivyhsr7qN5PZ5d1W0x18cNGIT+mi4gYIZLOB5bb/sRoxzJSxt4PNyIiNkKq7nLwx0BjtzkZDTncFBGbLVX3f3qqw+v7a9nOZ6hOZP+j7fubiXZ05HBTRETUSk8iIiJqbVLnJCZMmOApU6aMdhgRERuVm2666Ze2J3aatkkliSlTptDX1zfaYUREbFQktd994Xk53BQREbWSJCIiolaSRERE1EqSiIiIWkkSERFRK0kiIiJqJUlEREStTep3EjG2zJ8/n2XLRvIZRhuvgYEBACZO7Ph7pc1OT08PRxxxxGiHEV1IkojYAJ577rnRDiFinSRJRGPyTfEFs2fPBuDUU08d5Ugi1k7OSURERK0kiYiIqJUkERERtZIkIiKiVpJERETUSpKIiIhaSRIREVErSSIiImolSURERK0kiYiIqNV4kpB0oKS7JS2WdEaH6TMk/VrSLeX1qZZp50t6RNLtTccZEREv1miSkDQOOAc4CJgGzJQ0rUPV622/sbz+rqX8AuDAJmOMiIh6TfckpgOLbS+xvQKYBxzW7cy2rwN+1VRwERExvKaTRA+wtGW8v5S120/SrZK+L+n1a7MASSdJ6pPUN3TP/oiIGBlNJwl1KHPb+M3Abrb3Br4IXL42C7A913av7d480CUiYmQ1nST6gckt45OA5a0VbD9h+6kyvADYUtKEhuOKiIguNJ0kFgJ7SJoqaTxwNHBFawVJvyNJZXh6ienRhuOKiIguNJokbA8CJwNXAXcCl9heJGmWpFml2pHA7ZJuBWYDR9s2gKSLgJ8Ce0rql/ShJuONiIjVNf740nIIaUFb2ZyW4bOBs2vmndlsdBERMZz84joiImolSURERK0kiYiIqJUkERERtZIkIiKiVpJERETUSpKIiIhaSRIREVErSSIiImolSURERK0kiYiIqJUkERERtZIkIiKiVpJERETUSpKIiIhaSRIREVGr8YcObW7mz5/PsmXLRjuMGGP6+/sBmD179ihHEmNNT08PRxxxxGiHUStJYoQtW7aMpUvuY+fx2bTxgi1XrgJgRf+DoxxJjCUPrxgc7RDWqPE9maQDgS8A44DzbH+ubfoM4LvA/aXoMtt/1828Y9XO47fg+F12HO0wImKMu/Chx0Y7hDVqNElIGgecA7wD6AcWSrrC9h1tVa+3feg6zhsREQ1p+sT1dGCx7SW2VwDzgMM2wLwRETECmk4SPcDSlvH+UtZuP0m3Svq+pNevzbySTpLUJ6lvYGBgpOKOiAiaTxLqUOa28ZuB3WzvDXwRuHwt5sX2XNu9tnsnTpy4PrFGRESbppNEPzC5ZXwSsLy1gu0nbD9VhhcAW0qa0M28ERHRrKaTxEJgD0lTJY0HjgauaK0g6XckqQxPLzE92s28ERHRrEavbrI9KOlk4Cqqy1jPt71I0qwyfQ5wJPBhSYPAb4CjbRvoOG+T8UZExOoa/51EOYS0oK1sTsvw2cDZ3c4bEREbTu7dFBERtZIkIiKiVpJERETUSpKIiIhaSRIREVErSSIiImolSURERK0kiYiIqJUkERERtZIkIiKiVpJERETUSpKIiIhaSRIREVErSSIiImo1fqvwzc3AwADPPjfIhQ89NtqhRMQY9/Bzg2w9MDDaYQwrPYmIiKiVnsQImzhxIiuee4bjd9lxtEOJiDHuwoceY/zEiaMdxrDSk4iIiFqNJwlJB0q6W9JiSWcMU28fSaskHdlS9hFJt0taJOmjTccaERGrazRJSBoHnAMcBEwDZkqaVlPvTOCqlrK9gBOB6cDewKGS9mgy3oiIWF3TPYnpwGLbS2yvAOYBh3WodwowH3ikpex1wA22n7E9CFwLHN5wvBER0aLpJNEDLG0Z7y9lz5PUQ7Xzn9M27+3A2yTtJGlb4GBgcvsCJJ0kqU9S38AYv5QsImJj03SSUIcyt42fBZxue9Vqlew7qQ5B/RD4AXArMPiixuy5tntt904c41cJRERsbJq+BLaf1b/9TwKWt9XpBeZJApgAHCxp0Pbltr8KfBVA0t+X9iIiYgNpOkksBPaQNBVYBhwNHNNawfbUoWFJFwBX2r68jL/S9iOSdgX+GNiv4XgjIqJFo0nC9qCkk6muWhoHnG97kaRZZXr7eYh28yXtBKwE/tx27nUREbEBNf6La9sLgAVtZR2Tg+0T2sb3by6yiIhYk/ziOiIiaiVJRERErSSJiIiolSQRERG1kiQiIqJWkkRERNRKkoiIiFpJEhERUaurJCHpvZK2L8OfkHSZpDc1G1pERIy2bnsSn7T9pKT/DvwP4OvAl5oLKyIixoJuk8TQbbwPAb5k+7vA+GZCioiIsaLbJLFM0peB9wELJG21FvNGRMRGqtsd/fuo7uR6oO3HgVcAf9lUUBERMTZ0exfYXYDv2X5O0gzgDcCFTQUVERFjQ7c9ifnAKkmvpnpS3FTg241FFRERY0K3SeK3tgepng53lu2PUfUuIiJiE9ZtklgpaSZwPHBlKduymZAiImKs6DZJfIDq+dKftX1/eWb1N5sLKyIixoKukoTtO4DTgNsk7QX02/5cN/NKOlDS3ZIWSzpjmHr7SFol6ciWso9JWiTpdkkXSdq6m2VGRMTI6Pa2HDOAe4FzgHOBeyS9rYv5xpV5DgKmATMlTaupdybVZbZDZT3AqUCv7b2AccDR3cQbEREjo9tLYP8ZeKftuwEkvQa4CHjzGuabDiy2vaTMNw84DLijrd4pVFdQ7dMhvm0krQS2BZZ3GW9ERIyAbs9JbDmUIABs30N3J657gKUt4/2l7Hmlx3A4MKe13PYy4J+A/wIeAn5t++r2BUg6SVKfpL6BgYEuVyciIrrRbZLok/RVSTPK6yvATV3Mpw5lbhs/Czjd9qrWQkk7UvU6pgKvAraTdOyLGrPn2u613Ttx4sRu1iUiIrrU7eGmDwN/TnWOQMB1VOcm1qQfmNwyPokXHzLqBeZJApgAHCxpkKqncr/tAQBJlwFvJVdVRURsMF0lCdvPAZ8vr7WxENijXDK7jOrE8zFtbU8dGpZ0AXCl7cslvQXYV9K2wG+AA4C+tVx+RESsh2GThKTbePHhoefZfsNw89selHQy1VVL44DzbS+SNKtMnzPMvDdKuhS4GRgEfgbMHW55ERExstbUkzh0fRdgewGwoK2sY3KwfULb+N8Af7O+MURExLoZNknYfrCbRiT91PZ+IxNSRESMFSP14KD8EjoiYhM0Ukmi9rxFRERsvPII0oiIqDVSSaLTj+YiImIjN1JJ4rgRaiciIsaQNf1O4kk6n28QYNs7UA3c3kBsERExytZ0Cez2GyqQiIgYe7q9dxMAkl5Jy+Wutv9rxCOKiIgxo9uHDr1b0r3A/cC1wAPA9xuMKyIixoBuT1x/BtgXuKfckO8A4MeNRRUREWNCt0lipe1HgZdIeontHwFvbC6siIgYC7o9J/G4pJcC1wPfkvQI1Z1ZIyJiE9ZtT+I64OXAR4AfAPcB72oopoiIGCO6TRKieibENcBLgYvL4aeIiNiEdZUkbP+t7ddTPcL0VcC1kv6t0cgiImLUre1tOR4BfgE8Crxy5MOJiIixpKsT15I+DBwFTAQuBU60fUeTgW3MHl4xyIUPPTbaYcQY8tjKVQDsuOW4UY4kxpKHVwwyebSDWINur27aDfio7VvWdgGSDgS+QPWM6/Nsf66m3j7ADcBRti+VtCdwcUuV3YFP2T5rbWPYkHp6ekY7hBiDVvb3AzB+0qRRjiTGksmM/X2G7OaeFyRpHHAP8A6gH1gIzGzvhZR6PwSeBc63fWmH6cuAtwz3SNXe3l739fWN7EpEjIDZs2cDcOqpp45yJBEvJukm272dpjX90KHpwGLbS2yvAOYBh3Wodwown+qcRycHAPd1+8ztiIgYGU0niR5gact4fyl7nqQe4HBgzjDtHA1cNOLRRUTEsJpOEp2eWNd+fOss4HTbqzo2II0H3g18p2b6SZL6JPUNDAysT6wREdFmrW4Vvg76YbWT95OA5W11eoF5kgAmAAdLGrR9eZl+EHCz7Yc7LcD2XGAuVOckRi70iIhoOkksBPaQNJXqxPPRwDGtFcpdZQGQdAFwZUuCAJhJDjVFRIyKRpOE7UFJJ1Pd0mMc1ZVLiyTNKtOHOw+BpG2proz60ybjjIiIzpruSWB7AbCgraxjcrB9Qtv4M8BOjQUXERHDavrEdUREbMSSJCIiolaSRERE1EqSiIiIWkkSERFRK0kiIiJqJUlEREStJImIiKiVJBEREbWSJCIiolaSRERE1EqSiIiIWkkSERFRK0kiIiJqJUlEREStJImIiKiVJBEREbWSJCIiolbjSULSgZLulrRY0hnD1NtH0ipJR7aUvVzSpZLuknSnpP2ajjciIl7QaJKQNA44BzgImAbMlDStpt6ZwFVtk74A/MD2a4G9gTubjDciIlbXdE9iOrDY9hLbK4B5wGEd6p0CzAceGSqQtAPwNuCrALZX2H684XgjIqJF00miB1jaMt5fyp4nqQc4HJjTNu/uwADwNUk/k3SepO2aDDYiIlbXdJJQhzK3jZ8FnG57VVv5FsCbgC/Z/n3gaeBF5zQknSSpT1LfwMDACIQcERFDtmi4/X5gcsv4JGB5W51eYJ4kgAnAwZIGgRuAfts3lnqX0iFJ2J4LzAXo7e1tT0AREbEemk4SC4E9JE0FlgFHA8e0VrA9dWhY0gXAlbYvL+NLJe1p+27gAOCOhuONiIgWjSYJ24OSTqa6amkccL7tRZJmlent5yHanQJ8S9J4YAnwgSbjjYiI1TXdk8D2AmBBW1nH5GD7hLbxW6gOR0VExCjIL64jIqJWkkRERNRKkoiIiFpJEhERUStJIiIiaiVJRERErSSJiIiolSQRERG1kiQiIqJWkkRERNRKkoiIiFpJEhERUStJIiIiaiVJRERErSSJiIiolSQRERG1kiQiIqJWkkRERNRqPElIOlDS3ZIWSzpjmHr7SFol6ciWsgck3SbpFkl9TccaERGra/QZ15LGAecA7wD6gYWSrrB9R4d6ZwJXdWjmD2z/ssk4IyKis6Z7EtOBxbaX2F4BzAMO61DvFGA+8EjD8URExFpoOkn0AEtbxvtL2fMk9QCHA3M6zG/gakk3STqpsSgjIqKjRg83AepQ5rbxs4DTba+SXlT9v9leLumVwA8l3WX7utUWUCWPkwB23XXXkYk6IiKA5nsS/cDklvFJwPK2Or3APEkPAEcC50p6D4Dt5eXvI8C/UB2+Wo3tubZ7bfdOnDhxxFcgImJz1nSSWAjsIWmqpPHA0cAVrRVsT7U9xfYU4FLgz2xfLmk7SdsDSNoOeCdwe8PxRkREi0YPN9kelHQy1VVL44DzbS+SNKtM73QeYsjOwL+UQ1BbAN+2/YMm442IiNU1fU4C2wuABW1lHZOD7RNahpcAezcaXEREDCu/uI6IiFpJEhERUStJIiIiaiVJRERErSSJiIiolSQRERG1kiQiIqJWkkRERNRKkoiIiFpJEhERUStJIiIiaiVJRERErSSJiIiolSQRERG1Gr9VeGy+5s+fz7Jly0Y7jDGhv78fgNmzZ49yJGNDT08PRxxxxGiHEV1IkojYALbaaqvRDiFinSRJRGPyTTFi45dzEhERUStJIiIiajWeJCQdKOluSYslnTFMvX0krZJ0ZFv5OEk/k3Rl07FGRMTqGk0SksYB5wAHAdOAmZKm1dQ7E7iqQzMfAe5sMs6IiOis6Z7EdGCx7SW2VwDzgMM61DsFmA880looaRJwCHBew3FGREQHTSeJHmBpy3h/KXuepB7gcGBOh/nPAv4K+G3dAiSdJKlPUt/AwMB6BxwRES9oOkmoQ5nbxs8CTre9arUZpUOBR2zfNNwCbM+13Wu7d+LEiesVbERErK7p30n0A5NbxicBy9vq9ALzJAFMAA6WNAi8BXi3pIOBrYEdJH3T9rENxxwREYXs9i/2I9i4tAVwD3AAsAxYCBxje1FN/QuAK21f2lY+AzjN9qFrWN4A8OB6Bx7RjAnAL0c7iIgOdrPd8VBMoz0J24OSTqa6amkccL7tRZJmlemdzkOsz/JyvCnGLEl9tntHO46ItdFoTyIiXpAkERuj/OI6IiJqJUlEbDhzRzuAiLWVw00REVErPYmIiKiVJBEREbWSJCI2IElPjXYMEWsjSSJihKmSz1ZsEvJGjhgBkqZIulPSucDNwCclLZT0c0l/26H+jNZnpEg6W9IJGzDkiK4kSUSMnD2BC4HTqe52PB14I/BmSW8bxbgi1lmSRMTIedD2DcA7y+tnVL2K1wJ7jGZgEeuq6bvARmxOni5/BfyD7S8PU3eQ1b+kbd1YVBHrIT2JiJF3FfBBSS+F6sFakl7ZVudBYJqkrSS9jOpOyRFjTnoSESPM9tWSXgf8tDwn5SngWFoez2t7qaRLgJ8D91IdmooYc3JbjoiIqJXDTRERUStJIiIiaiVJRERErSSJiIiolSQRERG1kiQiIqJWkkRstspN9t7aMj5L0vHr2NYJkl7VMn6epGkjEWdp79OSThup9iK6lR/TxeZsBtUP3X4CYHvOerR1AnA7sLy09SfrGdsGJWkL24OjHUeMPelJxCZH0uWSbpK0SNJJpexASTdLulXS/5M0BZgFfEzSLZL2H/q2Lul1kv6zpb0pkn5ehj9VbgF+u6S55dkRRwK9wLdKW9tIukZSb5lnpqTbyjxntrT7lKTPlphukLRzl+t3YonhVknzJW0raXtJ90vastTZQdIDkraU9LuSflC2yfWSXlvqXCDp85J+BJw57EJjs5UkEZuiD9p+M9WO+9Sy8/0KcITtvYH32n4AmAP8X9tvtH390My27wTGS9q9FB0FXFKGz7a9j+29gG2AQ21fCvQB7y9t/WaorXII6kzgD6luG76PpPeUydsBN5SYrgNO7HL9Lisx7A3cCXzI9pPANcAhpc7RwHzbK4G5wCllm5wGnNvS1muAP7L98S6XHZuZJInYFJ0q6VbgBmAycBJwne37AWz/qos2LgHeV4aPAi4uw38g6UZJt1Ht+F+/hnb2Aa6xPVAO53wLGHq2xApg6MFDNwFTuogLYK/SI7gNeH9LDOcBHyjDHwC+Vm4y+FbgO5JuAb4M7NLS1ndsr+pyubEZyjmJ2KRImgH8EbCf7WckXQPcSvVAoLVxMdWO9TLAtu+VtDXVt/DecoO+T7PmW3xrmGkr/cLN01bR/efxAuA9tm8tT7ObQRXkj8uhsbcD42zfLmkH4HHbb6xp6+ma8gggPYnY9LwMeKwkiNcC+wJbAW+XNBVA0itK3SeB7Ts1Yvs+qh33J3mhFzGUEH5ZvqEf2TJLXVs3lmVPkDQOmAlcu64rV2wPPFTOP7y/bdqFwEXA18p6PAHcL+m98Pzzt/dez+XHZiRJIjY1PwC2KCeaP0N1yGmA6pDTZeUw1NBO/1+Bw4dOXHdo62KqW3xfAmD7capzG7cBlwMLW+peAMwZOnE9VGj7IeCvgR9R9Whutv3d9VzHT1Ilnx8Cd7VN+xawI1WiGPJ+4ENl3RcBh63n8mMzkluFR2xCypVWh9k+brRjiU1DzklEbCIkfRE4CDh4tGOJTUd6EhFjiKT/Dby3rfg7tj87GvFEJElEREStnLiOiIhaSRIREVErSSIiImolSURERK3/D3uHJchxjf6qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activation_layer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebf4864f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batc_normalization')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batc_normalization'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4eaa72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of dropout')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'dropout'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef966709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batch_size')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batch_size'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024b91e",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07fdf5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[110],  #\n",
    "     'hidden_neuron':[50], #\n",
    "\n",
    "     'hidden_layers':[5,7],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [32], #\n",
    "\n",
    "    #'lr':[0.01],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "#    'batc_normalization':[False,True],\n",
    "#    'dropout': [0,0.2,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "    'optimizer': ['rmsprop','adam','adadelta','adamax','nadam','adagrad'],\n",
    "   # 'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "    'kernel_initializer': ['orthogonal'],\n",
    "    'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    "   # 'activation_layer':['relu'],\n",
    "    #'batc_normalization':[False,True]\n",
    "    'batc_normalization':[False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8af776fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=params['optimizer'],#tf.keras.optimizers.Adamax(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=1.0, threshold=0.5),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=0,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdc73ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                           | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C46238798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▋                                                                    | 2/12 [11:44<58:42, 352.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3E156828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|█▍                                                                                 | 1/60 [00:07<07:50,  7.98s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35703CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B36FB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|██▊                                                                                | 2/60 [00:17<08:50,  9.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C46232708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35DD6948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  5%|████▏                                                                              | 3/60 [00:44<16:20, 17.21s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C46238AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C342A98B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|█████▌                                                                             | 4/60 [00:52<12:25, 13.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FCA4318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B36F168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  8%|██████▉                                                                            | 5/60 [01:00<10:41, 11.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3CC18DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C46238678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|████████▎                                                                          | 6/60 [01:08<09:19, 10.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A55828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4296A4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 12%|█████████▋                                                                         | 7/60 [01:16<08:22,  9.48s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3A0C2828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35AE89D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 13%|███████████                                                                        | 8/60 [01:26<08:24,  9.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F2AA1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35CB5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 15%|████████████▍                                                                      | 9/60 [01:55<13:17, 15.64s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35DD6E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3600C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█████████████▋                                                                    | 10/60 [02:04<11:29, 13.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42668CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3E156048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 18%|███████████████                                                                   | 11/60 [02:15<10:22, 12.71s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3CC18948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B6A29D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|████████████████▍                                                                 | 12/60 [02:25<09:33, 11.94s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B6A20D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4296A558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 22%|█████████████████▊                                                                | 13/60 [02:33<08:23, 10.72s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F467828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C462381F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 23%|███████████████████▏                                                              | 14/60 [02:40<07:28,  9.76s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35DD6678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B6A2948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|████████████████████▌                                                             | 15/60 [03:11<12:01, 16.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3CC18558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F2AA0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 27%|█████████████████████▊                                                            | 16/60 [03:19<10:01, 13.68s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C43144828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C39FEC9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 28%|███████████████████████▏                                                          | 17/60 [03:27<08:31, 11.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38415AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3CC18558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30%|████████████████████████▌                                                         | 18/60 [03:34<07:21, 10.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FCA4948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F4670D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 32%|█████████████████████████▉                                                        | 19/60 [03:41<06:31,  9.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F4673A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C38415AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███████████████████████████▎                                                      | 20/60 [03:51<06:22,  9.56s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A55DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C342D9D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 35%|████████████████████████████▋                                                     | 21/60 [04:16<09:18, 14.33s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4804D9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FCA45E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 37%|██████████████████████████████                                                    | 22/60 [04:26<08:06, 12.80s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B36F438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FCA4708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 38%|███████████████████████████████▍                                                  | 23/60 [04:37<07:41, 12.47s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3A0C2B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C38A55F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|████████████████████████████████▊                                                 | 24/60 [04:47<06:54, 11.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B8A7A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4804D168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 42%|██████████████████████████████████▏                                               | 25/60 [04:54<05:57, 10.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4804DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35DD6558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 43%|███████████████████████████████████▌                                              | 26/60 [05:02<05:24,  9.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3C905A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4804DB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 45%|████████████████████████████████████▉                                             | 27/60 [05:30<08:17, 15.06s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4804D5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C462381F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 47%|██████████████████████████████████████▎                                           | 28/60 [05:38<06:59, 13.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3E1564C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47A614C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 48%|███████████████████████████████████████▋                                          | 29/60 [05:47<06:02, 11.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C462381F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4804D708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████████████████████████████████████████                                         | 30/60 [05:54<05:10, 10.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B6A2678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3C905438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 52%|██████████████████████████████████████████▎                                       | 31/60 [06:03<04:52, 10.08s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35DD6168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3867D048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 53%|███████████████████████████████████████████▋                                      | 32/60 [06:15<04:56, 10.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35F228B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3867DF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 55%|█████████████████████████████████████████████                                     | 33/60 [06:50<08:01, 17.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B36F9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B44B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 57%|██████████████████████████████████████████████▍                                   | 34/60 [06:59<06:34, 15.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42775318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B44BE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 58%|███████████████████████████████████████████████▊                                  | 35/60 [07:10<05:50, 14.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B44B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3CA0B4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|█████████████████████████████████████████████████▏                                | 36/60 [07:18<04:53, 12.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FCA48B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B44B048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 62%|██████████████████████████████████████████████████▌                               | 37/60 [07:27<04:16, 11.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35AE8B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35CB5798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 63%|███████████████████████████████████████████████████▉                              | 38/60 [07:36<03:54, 10.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B509D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3C905B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 65%|█████████████████████████████████████████████████████▎                            | 39/60 [08:02<05:20, 15.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B8A7EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F467558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████████████████████████████████████████████████████▋                           | 40/60 [08:12<04:29, 13.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B8A70D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F467E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 68%|████████████████████████████████████████████████████████                          | 41/60 [08:19<03:41, 11.68s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F32E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35AE8B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 70%|█████████████████████████████████████████████████████████▍                        | 42/60 [08:30<03:23, 11.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C385A8DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B44B678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 72%|██████████████████████████████████████████████████████████▊                       | 43/60 [08:41<03:10, 11.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B6A20D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3600C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 73%|████████████████████████████████████████████████████████████▏                     | 44/60 [08:49<02:48, 10.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B509CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F3993A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 45/60 [09:19<04:04, 16.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B8A7F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C46238708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 77%|██████████████████████████████████████████████████████████████▊                   | 46/60 [09:29<03:20, 14.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4804D798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3A0C2EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 78%|████████████████████████████████████████████████████████████████▏                 | 47/60 [09:38<02:46, 12.81s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42B86168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35CB53A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 48/60 [09:48<02:20, 11.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C385A8168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B8A7DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 82%|██████████████████████████████████████████████████████████████████▉               | 49/60 [09:55<01:54, 10.43s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47A614C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F635708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 50/60 [10:04<01:38,  9.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C462380D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B8A7318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 51/60 [10:29<02:12, 14.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F399318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3CC189D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 87%|███████████████████████████████████████████████████████████████████████           | 52/60 [10:36<01:38, 12.30s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35AE8558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C46238CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████████▍         | 53/60 [10:46<01:20, 11.48s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B8A7798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4804DCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 54/60 [10:54<01:02, 10.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B487AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35A75F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 55/60 [11:03<00:50, 10.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C2DF04438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B8A7048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 93%|████████████████████████████████████████████████████████████████████████████▌     | 56/60 [11:14<00:41, 10.40s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3C996DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35CB5AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 57/60 [11:40<00:44, 14.89s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35703B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C35F22EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▎  | 58/60 [11:50<00:27, 13.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4804D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3CC18438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 98%|████████████████████████████████████████████████████████████████████████████████▋ | 59/60 [12:02<00:13, 13.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47FCF1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C38A55EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [12:13<00:00, 12.22s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d88e5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/M/Predykcja klasy M - podejscie z Focal-loss (nowe)/050522202709.csv')\n",
    "#binary cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1938c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stare_wart=df['val_fbeta_score']\n",
    "nowe_wart=[]\n",
    "for x in stare_wart:\n",
    "    wart=x.replace('[','')\n",
    "    wart=wart.replace(']','')\n",
    "    wart=float(wart)\n",
    "    nowe_wart.append(wart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa47b9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.408327</td>\n",
       "      <td>[0.24761903]</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.472344</td>\n",
       "      <td>[0.22222222]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157</td>\n",
       "      <td>0.389283</td>\n",
       "      <td>[0.1875]</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.469775</td>\n",
       "      <td>[0.19354838]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>573</td>\n",
       "      <td>0.719981</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.723887</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>0.458733</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.603184</td>\n",
       "      <td>[0.14285715]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>0.400272</td>\n",
       "      <td>[0.25490195]</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.535720</td>\n",
       "      <td>[0.13793103]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120</td>\n",
       "      <td>0.702675</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.711571</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>88</td>\n",
       "      <td>0.414603</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.510673</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>144</td>\n",
       "      <td>0.380351</td>\n",
       "      <td>[0.2990654]</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.476183</td>\n",
       "      <td>[0.20000002]</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>559</td>\n",
       "      <td>0.786715</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795366</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>141</td>\n",
       "      <td>0.460638</td>\n",
       "      <td>[0.27450982]</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.630301</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>125</td>\n",
       "      <td>0.396679</td>\n",
       "      <td>[0.28]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.496404</td>\n",
       "      <td>[0.14285715]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>156</td>\n",
       "      <td>0.774268</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789291</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>113</td>\n",
       "      <td>0.393193</td>\n",
       "      <td>[0.2268041]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.533381</td>\n",
       "      <td>[0.20689656]</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>107</td>\n",
       "      <td>0.391194</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.523199</td>\n",
       "      <td>[0.13793103]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>654</td>\n",
       "      <td>0.726051</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732795</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>125</td>\n",
       "      <td>0.444247</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.686537</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>91</td>\n",
       "      <td>0.405201</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.565380</td>\n",
       "      <td>[0.13793103]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>108</td>\n",
       "      <td>0.700496</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709835</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>79</td>\n",
       "      <td>0.420673</td>\n",
       "      <td>[0.14736842]</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.527807</td>\n",
       "      <td>[0.15384614]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>134</td>\n",
       "      <td>0.394937</td>\n",
       "      <td>[0.24242425]</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.466762</td>\n",
       "      <td>[0.20689656]</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>488</td>\n",
       "      <td>0.785902</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797109</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>117</td>\n",
       "      <td>0.468705</td>\n",
       "      <td>[0.26]</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.599337</td>\n",
       "      <td>[0.08695653]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>140</td>\n",
       "      <td>0.379418</td>\n",
       "      <td>[0.27450982]</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.518222</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>126</td>\n",
       "      <td>0.773499</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785092</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>87</td>\n",
       "      <td>0.407696</td>\n",
       "      <td>[0.185567]</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.489857</td>\n",
       "      <td>[0.15384614]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>108</td>\n",
       "      <td>0.392590</td>\n",
       "      <td>[0.26]</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.528366</td>\n",
       "      <td>[0.20689656]</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>551</td>\n",
       "      <td>0.719300</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.728458</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>124</td>\n",
       "      <td>0.443059</td>\n",
       "      <td>[0.24000001]</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.637970</td>\n",
       "      <td>[0.14814815]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100</td>\n",
       "      <td>0.395626</td>\n",
       "      <td>[0.26262626]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.554546</td>\n",
       "      <td>[0.13793103]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100</td>\n",
       "      <td>0.699711</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714648</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>116</td>\n",
       "      <td>0.409646</td>\n",
       "      <td>[0.24000001]</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.471433</td>\n",
       "      <td>[0.15384614]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>170</td>\n",
       "      <td>0.386059</td>\n",
       "      <td>[0.18947367]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.491638</td>\n",
       "      <td>[0.13793103]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>649</td>\n",
       "      <td>0.787931</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.796805</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>120</td>\n",
       "      <td>0.457088</td>\n",
       "      <td>[0.24489796]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.626391</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>138</td>\n",
       "      <td>0.398233</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.483211</td>\n",
       "      <td>[0.15384614]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>103</td>\n",
       "      <td>0.770705</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782402</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>117</td>\n",
       "      <td>0.394196</td>\n",
       "      <td>[0.30476192]</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.509274</td>\n",
       "      <td>[0.14285715]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>144</td>\n",
       "      <td>0.377558</td>\n",
       "      <td>[0.32142857]</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.645121</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>530</td>\n",
       "      <td>0.717887</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724876</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>144</td>\n",
       "      <td>0.427796</td>\n",
       "      <td>[0.30476192]</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.549133</td>\n",
       "      <td>[0.20000002]</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>82</td>\n",
       "      <td>0.412688</td>\n",
       "      <td>[0.2244898]</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.564276</td>\n",
       "      <td>[0.14814815]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>173</td>\n",
       "      <td>0.701585</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715421</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>154</td>\n",
       "      <td>0.399942</td>\n",
       "      <td>[0.24489796]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.491227</td>\n",
       "      <td>[0.07692307]</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>117</td>\n",
       "      <td>0.393038</td>\n",
       "      <td>[0.24242425]</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.509834</td>\n",
       "      <td>[0.14285715]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>562</td>\n",
       "      <td>0.786373</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.794553</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>130</td>\n",
       "      <td>0.451807</td>\n",
       "      <td>[0.20408164]</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.571990</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>101</td>\n",
       "      <td>0.403279</td>\n",
       "      <td>[0.27722773]</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.539967</td>\n",
       "      <td>[0.07407407]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>119</td>\n",
       "      <td>0.770334</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797165</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>86</td>\n",
       "      <td>0.410785</td>\n",
       "      <td>[0.22222222]</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.511918</td>\n",
       "      <td>[0.22222222]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>128</td>\n",
       "      <td>0.395917</td>\n",
       "      <td>[0.20202021]</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.591128</td>\n",
       "      <td>[0.13333334]</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>536</td>\n",
       "      <td>0.725505</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730909</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>88</td>\n",
       "      <td>0.471879</td>\n",
       "      <td>[0.18947367]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.620994</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>125</td>\n",
       "      <td>0.394649</td>\n",
       "      <td>[0.30188677]</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.589166</td>\n",
       "      <td>[0.13793103]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>118</td>\n",
       "      <td>0.705206</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.721926</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>123</td>\n",
       "      <td>0.403238</td>\n",
       "      <td>[0.24489796]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.463431</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>153</td>\n",
       "      <td>0.391120</td>\n",
       "      <td>[0.29126212]</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.487714</td>\n",
       "      <td>[0.16000001]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>455</td>\n",
       "      <td>0.784763</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790856</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>142</td>\n",
       "      <td>0.456050</td>\n",
       "      <td>[0.2244898]</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.555063</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>141</td>\n",
       "      <td>0.381737</td>\n",
       "      <td>[0.27722773]</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.530527</td>\n",
       "      <td>[0.08000001]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>130</td>\n",
       "      <td>0.769447</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784204</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "0            100  0.408327  [0.24761903]   0.684211  0.151163  0.472344   \n",
       "1            157  0.389283      [0.1875]   0.900000  0.104651  0.469775   \n",
       "2            573  0.719981          [0.]   0.000000  0.000000  0.723887   \n",
       "3            107  0.458733  [0.25742576]   0.866667  0.151163  0.603184   \n",
       "4            115  0.400272  [0.25490195]   0.812500  0.151163  0.535720   \n",
       "5            120  0.702675          [0.]   0.000000  0.000000  0.711571   \n",
       "6             88  0.414603  [0.17021276]   1.000000  0.093023  0.510673   \n",
       "7            144  0.380351   [0.2990654]   0.761905  0.186047  0.476183   \n",
       "8            559  0.786715          [0.]   0.000000  0.000000  0.795366   \n",
       "9            141  0.460638  [0.27450982]   0.875000  0.162791  0.630301   \n",
       "10           125  0.396679        [0.28]   1.000000  0.162791  0.496404   \n",
       "11           156  0.774268          [0.]   0.000000  0.000000  0.789291   \n",
       "12           113  0.393193   [0.2268041]   1.000000  0.127907  0.533381   \n",
       "13           107  0.391194  [0.25742576]   0.866667  0.151163  0.523199   \n",
       "14           654  0.726051          [0.]   0.000000  0.000000  0.732795   \n",
       "15           125  0.444247  [0.25742576]   0.866667  0.151163  0.686537   \n",
       "16            91  0.405201  [0.25742576]   0.866667  0.151163  0.565380   \n",
       "17           108  0.700496          [0.]   0.000000  0.000000  0.709835   \n",
       "18            79  0.420673  [0.14736842]   0.777778  0.081395  0.527807   \n",
       "19           134  0.394937  [0.24242425]   0.923077  0.139535  0.466762   \n",
       "20           488  0.785902          [0.]   0.000000  0.000000  0.797109   \n",
       "21           117  0.468705        [0.26]   0.928571  0.151163  0.599337   \n",
       "22           140  0.379418  [0.27450982]   0.875000  0.162791  0.518222   \n",
       "23           126  0.773499          [0.]   0.000000  0.000000  0.785092   \n",
       "24            87  0.407696    [0.185567]   0.818182  0.104651  0.489857   \n",
       "25           108  0.392590        [0.26]   0.928571  0.151163  0.528366   \n",
       "26           551  0.719300          [0.]   0.000000  0.000000  0.728458   \n",
       "27           124  0.443059  [0.24000001]   0.857143  0.139535  0.637970   \n",
       "28           100  0.395626  [0.26262626]   1.000000  0.151163  0.554546   \n",
       "29           100  0.699711          [0.]   0.000000  0.000000  0.714648   \n",
       "30           116  0.409646  [0.24000001]   0.857143  0.139535  0.471433   \n",
       "31           170  0.386059  [0.18947367]   1.000000  0.104651  0.491638   \n",
       "32           649  0.787931          [0.]   0.000000  0.000000  0.796805   \n",
       "33           120  0.457088  [0.24489796]   1.000000  0.139535  0.626391   \n",
       "34           138  0.398233  [0.25742576]   0.866667  0.151163  0.483211   \n",
       "35           103  0.770705          [0.]   0.000000  0.000000  0.782402   \n",
       "36           117  0.394196  [0.30476192]   0.842105  0.186047  0.509274   \n",
       "37           144  0.377558  [0.32142857]   0.692308  0.209302  0.645121   \n",
       "38           530  0.717887          [0.]   0.000000  0.000000  0.724876   \n",
       "39           144  0.427796  [0.30476192]   0.842105  0.186047  0.549133   \n",
       "40            82  0.412688   [0.2244898]   0.916667  0.127907  0.564276   \n",
       "41           173  0.701585          [0.]   0.000000  0.000000  0.715421   \n",
       "42           154  0.399942  [0.24489796]   1.000000  0.139535  0.491227   \n",
       "43           117  0.393038  [0.24242425]   0.923077  0.139535  0.509834   \n",
       "44           562  0.786373          [0.]   0.000000  0.000000  0.794553   \n",
       "45           130  0.451807  [0.20408164]   0.833333  0.116279  0.571990   \n",
       "46           101  0.403279  [0.27722773]   0.933333  0.162791  0.539967   \n",
       "47           119  0.770334          [0.]   0.000000  0.000000  0.797165   \n",
       "48            86  0.410785  [0.22222222]   0.846154  0.127907  0.511918   \n",
       "49           128  0.395917  [0.20202021]   0.769231  0.116279  0.591128   \n",
       "50           536  0.725505          [0.]   0.000000  0.000000  0.730909   \n",
       "51            88  0.471879  [0.18947367]   1.000000  0.104651  0.620994   \n",
       "52           125  0.394649  [0.30188677]   0.800000  0.186047  0.589166   \n",
       "53           118  0.705206          [0.]   0.000000  0.000000  0.721926   \n",
       "54           123  0.403238  [0.24489796]   1.000000  0.139535  0.463431   \n",
       "55           153  0.391120  [0.29126212]   0.882353  0.174419  0.487714   \n",
       "56           455  0.784763          [0.]   0.000000  0.000000  0.790856   \n",
       "57           142  0.456050   [0.2244898]   0.916667  0.127907  0.555063   \n",
       "58           141  0.381737  [0.27722773]   0.933333  0.162791  0.530527   \n",
       "59           130  0.769447          [0.]   0.000000  0.000000  0.784204   \n",
       "\n",
       "   val_fbeta_score  val_precision  val_recall activation_layer  ...  dropout  \\\n",
       "0     [0.22222222]       0.600000    0.136364          sigmoid  ...        0   \n",
       "1     [0.19354838]       0.333333    0.136364          sigmoid  ...        0   \n",
       "2             [0.]       0.000000    0.000000          sigmoid  ...        0   \n",
       "3     [0.14285715]       0.333333    0.090909          sigmoid  ...        0   \n",
       "4     [0.13793103]       0.285714    0.090909          sigmoid  ...        0   \n",
       "5             [0.]       0.000000    0.000000          sigmoid  ...        0   \n",
       "6             [0.]       0.000000    0.000000          sigmoid  ...        0   \n",
       "7     [0.20000002]       0.375000    0.136364          sigmoid  ...        0   \n",
       "8             [0.]       0.000000    0.000000          sigmoid  ...        0   \n",
       "9             [0.]       0.000000    0.000000          sigmoid  ...        0   \n",
       "10    [0.14285715]       0.333333    0.090909          sigmoid  ...        0   \n",
       "11            [0.]       0.000000    0.000000          sigmoid  ...        0   \n",
       "12    [0.20689656]       0.428571    0.136364             tanh  ...        0   \n",
       "13    [0.13793103]       0.285714    0.090909             tanh  ...        0   \n",
       "14            [0.]       0.000000    0.000000             tanh  ...        0   \n",
       "15            [0.]       0.000000    0.000000             tanh  ...        0   \n",
       "16    [0.13793103]       0.285714    0.090909             tanh  ...        0   \n",
       "17            [0.]       0.000000    0.000000             tanh  ...        0   \n",
       "18    [0.15384614]       0.500000    0.090909             tanh  ...        0   \n",
       "19    [0.20689656]       0.428571    0.136364             tanh  ...        0   \n",
       "20            [0.]       0.000000    0.000000             tanh  ...        0   \n",
       "21    [0.08695653]       1.000000    0.045455             tanh  ...        0   \n",
       "22            [0.]       0.000000    0.000000             tanh  ...        0   \n",
       "23            [0.]       0.000000    0.000000             tanh  ...        0   \n",
       "24    [0.15384614]       0.500000    0.090909             selu  ...        0   \n",
       "25    [0.20689656]       0.428571    0.136364             selu  ...        0   \n",
       "26            [0.]       0.000000    0.000000             selu  ...        0   \n",
       "27    [0.14814815]       0.400000    0.090909             selu  ...        0   \n",
       "28    [0.13793103]       0.285714    0.090909             selu  ...        0   \n",
       "29            [0.]       0.000000    0.000000             selu  ...        0   \n",
       "30    [0.15384614]       0.500000    0.090909             selu  ...        0   \n",
       "31    [0.13793103]       0.285714    0.090909             selu  ...        0   \n",
       "32            [0.]       0.000000    0.000000             selu  ...        0   \n",
       "33            [0.]       0.000000    0.000000             selu  ...        0   \n",
       "34    [0.15384614]       0.500000    0.090909             selu  ...        0   \n",
       "35            [0.]       0.000000    0.000000             selu  ...        0   \n",
       "36    [0.14285715]       0.333333    0.090909              elu  ...        0   \n",
       "37            [0.]       0.000000    0.000000              elu  ...        0   \n",
       "38            [0.]       0.000000    0.000000              elu  ...        0   \n",
       "39    [0.20000002]       0.375000    0.136364              elu  ...        0   \n",
       "40    [0.14814815]       0.400000    0.090909              elu  ...        0   \n",
       "41            [0.]       0.000000    0.000000              elu  ...        0   \n",
       "42    [0.07692307]       0.250000    0.045455              elu  ...        0   \n",
       "43    [0.14285715]       0.333333    0.090909              elu  ...        0   \n",
       "44            [0.]       0.000000    0.000000              elu  ...        0   \n",
       "45            [0.]       0.000000    0.000000              elu  ...        0   \n",
       "46    [0.07407407]       0.200000    0.045455              elu  ...        0   \n",
       "47            [0.]       0.000000    0.000000              elu  ...        0   \n",
       "48    [0.22222222]       0.600000    0.136364             relu  ...        0   \n",
       "49    [0.13333334]       0.250000    0.090909             relu  ...        0   \n",
       "50            [0.]       0.000000    0.000000             relu  ...        0   \n",
       "51            [0.]       0.000000    0.000000             relu  ...        0   \n",
       "52    [0.13793103]       0.285714    0.090909             relu  ...        0   \n",
       "53            [0.]       0.000000    0.000000             relu  ...        0   \n",
       "54            [0.]       0.000000    0.000000             relu  ...        0   \n",
       "55    [0.16000001]       0.666667    0.090909             relu  ...        0   \n",
       "56            [0.]       0.000000    0.000000             relu  ...        0   \n",
       "57            [0.]       0.000000    0.000000             relu  ...        0   \n",
       "58    [0.08000001]       0.333333    0.045455             relu  ...        0   \n",
       "59            [0.]       0.000000    0.000000             relu  ...        0   \n",
       "\n",
       "    epochs  first_neuron  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "0   100000           110              5             50          orthogonal   \n",
       "1   100000           110              5             50          orthogonal   \n",
       "2   100000           110              5             50          orthogonal   \n",
       "3   100000           110              5             50          orthogonal   \n",
       "4   100000           110              5             50          orthogonal   \n",
       "5   100000           110              5             50          orthogonal   \n",
       "6   100000           110              7             50          orthogonal   \n",
       "7   100000           110              7             50          orthogonal   \n",
       "8   100000           110              7             50          orthogonal   \n",
       "9   100000           110              7             50          orthogonal   \n",
       "10  100000           110              7             50          orthogonal   \n",
       "11  100000           110              7             50          orthogonal   \n",
       "12  100000           110              5             50          orthogonal   \n",
       "13  100000           110              5             50          orthogonal   \n",
       "14  100000           110              5             50          orthogonal   \n",
       "15  100000           110              5             50          orthogonal   \n",
       "16  100000           110              5             50          orthogonal   \n",
       "17  100000           110              5             50          orthogonal   \n",
       "18  100000           110              7             50          orthogonal   \n",
       "19  100000           110              7             50          orthogonal   \n",
       "20  100000           110              7             50          orthogonal   \n",
       "21  100000           110              7             50          orthogonal   \n",
       "22  100000           110              7             50          orthogonal   \n",
       "23  100000           110              7             50          orthogonal   \n",
       "24  100000           110              5             50          orthogonal   \n",
       "25  100000           110              5             50          orthogonal   \n",
       "26  100000           110              5             50          orthogonal   \n",
       "27  100000           110              5             50          orthogonal   \n",
       "28  100000           110              5             50          orthogonal   \n",
       "29  100000           110              5             50          orthogonal   \n",
       "30  100000           110              7             50          orthogonal   \n",
       "31  100000           110              7             50          orthogonal   \n",
       "32  100000           110              7             50          orthogonal   \n",
       "33  100000           110              7             50          orthogonal   \n",
       "34  100000           110              7             50          orthogonal   \n",
       "35  100000           110              7             50          orthogonal   \n",
       "36  100000           110              5             50          orthogonal   \n",
       "37  100000           110              5             50          orthogonal   \n",
       "38  100000           110              5             50          orthogonal   \n",
       "39  100000           110              5             50          orthogonal   \n",
       "40  100000           110              5             50          orthogonal   \n",
       "41  100000           110              5             50          orthogonal   \n",
       "42  100000           110              7             50          orthogonal   \n",
       "43  100000           110              7             50          orthogonal   \n",
       "44  100000           110              7             50          orthogonal   \n",
       "45  100000           110              7             50          orthogonal   \n",
       "46  100000           110              7             50          orthogonal   \n",
       "47  100000           110              7             50          orthogonal   \n",
       "48  100000           110              5             50          orthogonal   \n",
       "49  100000           110              5             50          orthogonal   \n",
       "50  100000           110              5             50          orthogonal   \n",
       "51  100000           110              5             50          orthogonal   \n",
       "52  100000           110              5             50          orthogonal   \n",
       "53  100000           110              5             50          orthogonal   \n",
       "54  100000           110              7             50          orthogonal   \n",
       "55  100000           110              7             50          orthogonal   \n",
       "56  100000           110              7             50          orthogonal   \n",
       "57  100000           110              7             50          orthogonal   \n",
       "58  100000           110              7             50          orthogonal   \n",
       "59  100000           110              7             50          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation optimizer  \n",
       "0                  0.0001                 0.0001          sigmoid   rmsprop  \n",
       "1                  0.0001                 0.0001          sigmoid      adam  \n",
       "2                  0.0001                 0.0001          sigmoid  adadelta  \n",
       "3                  0.0001                 0.0001          sigmoid    adamax  \n",
       "4                  0.0001                 0.0001          sigmoid     nadam  \n",
       "5                  0.0001                 0.0001          sigmoid   adagrad  \n",
       "6                  0.0001                 0.0001          sigmoid   rmsprop  \n",
       "7                  0.0001                 0.0001          sigmoid      adam  \n",
       "8                  0.0001                 0.0001          sigmoid  adadelta  \n",
       "9                  0.0001                 0.0001          sigmoid    adamax  \n",
       "10                 0.0001                 0.0001          sigmoid     nadam  \n",
       "11                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "12                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "13                 0.0001                 0.0001          sigmoid      adam  \n",
       "14                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "15                 0.0001                 0.0001          sigmoid    adamax  \n",
       "16                 0.0001                 0.0001          sigmoid     nadam  \n",
       "17                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "18                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "19                 0.0001                 0.0001          sigmoid      adam  \n",
       "20                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "21                 0.0001                 0.0001          sigmoid    adamax  \n",
       "22                 0.0001                 0.0001          sigmoid     nadam  \n",
       "23                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "24                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "25                 0.0001                 0.0001          sigmoid      adam  \n",
       "26                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "27                 0.0001                 0.0001          sigmoid    adamax  \n",
       "28                 0.0001                 0.0001          sigmoid     nadam  \n",
       "29                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "30                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "31                 0.0001                 0.0001          sigmoid      adam  \n",
       "32                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "33                 0.0001                 0.0001          sigmoid    adamax  \n",
       "34                 0.0001                 0.0001          sigmoid     nadam  \n",
       "35                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "36                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "37                 0.0001                 0.0001          sigmoid      adam  \n",
       "38                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "39                 0.0001                 0.0001          sigmoid    adamax  \n",
       "40                 0.0001                 0.0001          sigmoid     nadam  \n",
       "41                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "42                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "43                 0.0001                 0.0001          sigmoid      adam  \n",
       "44                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "45                 0.0001                 0.0001          sigmoid    adamax  \n",
       "46                 0.0001                 0.0001          sigmoid     nadam  \n",
       "47                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "48                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "49                 0.0001                 0.0001          sigmoid      adam  \n",
       "50                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "51                 0.0001                 0.0001          sigmoid    adamax  \n",
       "52                 0.0001                 0.0001          sigmoid     nadam  \n",
       "53                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "54                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "55                 0.0001                 0.0001          sigmoid      adam  \n",
       "56                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "57                 0.0001                 0.0001          sigmoid    adamax  \n",
       "58                 0.0001                 0.0001          sigmoid     nadam  \n",
       "59                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "\n",
       "[60 rows x 24 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a15439fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['val_fbeta_score']=nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c4dfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e05fa32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>123</td>\n",
       "      <td>0.403238</td>\n",
       "      <td>[0.24489796]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.463431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>134</td>\n",
       "      <td>0.394937</td>\n",
       "      <td>[0.24242425]</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.466762</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157</td>\n",
       "      <td>0.389283</td>\n",
       "      <td>[0.1875]</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.469775</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>116</td>\n",
       "      <td>0.409646</td>\n",
       "      <td>[0.24000001]</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.471433</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.408327</td>\n",
       "      <td>[0.24761903]</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.472344</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>144</td>\n",
       "      <td>0.380351</td>\n",
       "      <td>[0.2990654]</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.476183</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>138</td>\n",
       "      <td>0.398233</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.483211</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>153</td>\n",
       "      <td>0.391120</td>\n",
       "      <td>[0.29126212]</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.487714</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>87</td>\n",
       "      <td>0.407696</td>\n",
       "      <td>[0.185567]</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.489857</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>154</td>\n",
       "      <td>0.399942</td>\n",
       "      <td>[0.24489796]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.491227</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>170</td>\n",
       "      <td>0.386059</td>\n",
       "      <td>[0.18947367]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.491638</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>125</td>\n",
       "      <td>0.396679</td>\n",
       "      <td>[0.28]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.496404</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>117</td>\n",
       "      <td>0.394196</td>\n",
       "      <td>[0.30476192]</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.509274</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>117</td>\n",
       "      <td>0.393038</td>\n",
       "      <td>[0.24242425]</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.509834</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>88</td>\n",
       "      <td>0.414603</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.510673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>86</td>\n",
       "      <td>0.410785</td>\n",
       "      <td>[0.22222222]</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.511918</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>140</td>\n",
       "      <td>0.379418</td>\n",
       "      <td>[0.27450982]</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.518222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>107</td>\n",
       "      <td>0.391194</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.523199</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>79</td>\n",
       "      <td>0.420673</td>\n",
       "      <td>[0.14736842]</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.527807</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>108</td>\n",
       "      <td>0.392590</td>\n",
       "      <td>[0.26]</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.528366</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>141</td>\n",
       "      <td>0.381737</td>\n",
       "      <td>[0.27722773]</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.530527</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>113</td>\n",
       "      <td>0.393193</td>\n",
       "      <td>[0.2268041]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.533381</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>0.400272</td>\n",
       "      <td>[0.25490195]</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.535720</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>101</td>\n",
       "      <td>0.403279</td>\n",
       "      <td>[0.27722773]</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.539967</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>144</td>\n",
       "      <td>0.427796</td>\n",
       "      <td>[0.30476192]</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.549133</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100</td>\n",
       "      <td>0.395626</td>\n",
       "      <td>[0.26262626]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.554546</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>142</td>\n",
       "      <td>0.456050</td>\n",
       "      <td>[0.2244898]</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.555063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>82</td>\n",
       "      <td>0.412688</td>\n",
       "      <td>[0.2244898]</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.564276</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>91</td>\n",
       "      <td>0.405201</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.565380</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>130</td>\n",
       "      <td>0.451807</td>\n",
       "      <td>[0.20408164]</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.571990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>125</td>\n",
       "      <td>0.394649</td>\n",
       "      <td>[0.30188677]</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.589166</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>128</td>\n",
       "      <td>0.395917</td>\n",
       "      <td>[0.20202021]</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.591128</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>117</td>\n",
       "      <td>0.468705</td>\n",
       "      <td>[0.26]</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.599337</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>0.458733</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.603184</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>88</td>\n",
       "      <td>0.471879</td>\n",
       "      <td>[0.18947367]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.620994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>120</td>\n",
       "      <td>0.457088</td>\n",
       "      <td>[0.24489796]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.626391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>141</td>\n",
       "      <td>0.460638</td>\n",
       "      <td>[0.27450982]</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.630301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>124</td>\n",
       "      <td>0.443059</td>\n",
       "      <td>[0.24000001]</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.637970</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>144</td>\n",
       "      <td>0.377558</td>\n",
       "      <td>[0.32142857]</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.645121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>125</td>\n",
       "      <td>0.444247</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.686537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>108</td>\n",
       "      <td>0.700496</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120</td>\n",
       "      <td>0.702675</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.711571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100</td>\n",
       "      <td>0.699711</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>173</td>\n",
       "      <td>0.701585</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>118</td>\n",
       "      <td>0.705206</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.721926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>573</td>\n",
       "      <td>0.719981</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.723887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>530</td>\n",
       "      <td>0.717887</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>551</td>\n",
       "      <td>0.719300</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.728458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>536</td>\n",
       "      <td>0.725505</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>654</td>\n",
       "      <td>0.726051</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>103</td>\n",
       "      <td>0.770705</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>130</td>\n",
       "      <td>0.769447</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>126</td>\n",
       "      <td>0.773499</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>156</td>\n",
       "      <td>0.774268</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>455</td>\n",
       "      <td>0.784763</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>562</td>\n",
       "      <td>0.786373</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.794553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>559</td>\n",
       "      <td>0.786715</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>649</td>\n",
       "      <td>0.787931</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.796805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>488</td>\n",
       "      <td>0.785902</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>119</td>\n",
       "      <td>0.770334</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "54           123  0.403238  [0.24489796]   1.000000  0.139535  0.463431   \n",
       "19           134  0.394937  [0.24242425]   0.923077  0.139535  0.466762   \n",
       "1            157  0.389283      [0.1875]   0.900000  0.104651  0.469775   \n",
       "30           116  0.409646  [0.24000001]   0.857143  0.139535  0.471433   \n",
       "0            100  0.408327  [0.24761903]   0.684211  0.151163  0.472344   \n",
       "7            144  0.380351   [0.2990654]   0.761905  0.186047  0.476183   \n",
       "34           138  0.398233  [0.25742576]   0.866667  0.151163  0.483211   \n",
       "55           153  0.391120  [0.29126212]   0.882353  0.174419  0.487714   \n",
       "24            87  0.407696    [0.185567]   0.818182  0.104651  0.489857   \n",
       "42           154  0.399942  [0.24489796]   1.000000  0.139535  0.491227   \n",
       "31           170  0.386059  [0.18947367]   1.000000  0.104651  0.491638   \n",
       "10           125  0.396679        [0.28]   1.000000  0.162791  0.496404   \n",
       "36           117  0.394196  [0.30476192]   0.842105  0.186047  0.509274   \n",
       "43           117  0.393038  [0.24242425]   0.923077  0.139535  0.509834   \n",
       "6             88  0.414603  [0.17021276]   1.000000  0.093023  0.510673   \n",
       "48            86  0.410785  [0.22222222]   0.846154  0.127907  0.511918   \n",
       "22           140  0.379418  [0.27450982]   0.875000  0.162791  0.518222   \n",
       "13           107  0.391194  [0.25742576]   0.866667  0.151163  0.523199   \n",
       "18            79  0.420673  [0.14736842]   0.777778  0.081395  0.527807   \n",
       "25           108  0.392590        [0.26]   0.928571  0.151163  0.528366   \n",
       "58           141  0.381737  [0.27722773]   0.933333  0.162791  0.530527   \n",
       "12           113  0.393193   [0.2268041]   1.000000  0.127907  0.533381   \n",
       "4            115  0.400272  [0.25490195]   0.812500  0.151163  0.535720   \n",
       "46           101  0.403279  [0.27722773]   0.933333  0.162791  0.539967   \n",
       "39           144  0.427796  [0.30476192]   0.842105  0.186047  0.549133   \n",
       "28           100  0.395626  [0.26262626]   1.000000  0.151163  0.554546   \n",
       "57           142  0.456050   [0.2244898]   0.916667  0.127907  0.555063   \n",
       "40            82  0.412688   [0.2244898]   0.916667  0.127907  0.564276   \n",
       "16            91  0.405201  [0.25742576]   0.866667  0.151163  0.565380   \n",
       "45           130  0.451807  [0.20408164]   0.833333  0.116279  0.571990   \n",
       "52           125  0.394649  [0.30188677]   0.800000  0.186047  0.589166   \n",
       "49           128  0.395917  [0.20202021]   0.769231  0.116279  0.591128   \n",
       "21           117  0.468705        [0.26]   0.928571  0.151163  0.599337   \n",
       "3            107  0.458733  [0.25742576]   0.866667  0.151163  0.603184   \n",
       "51            88  0.471879  [0.18947367]   1.000000  0.104651  0.620994   \n",
       "33           120  0.457088  [0.24489796]   1.000000  0.139535  0.626391   \n",
       "9            141  0.460638  [0.27450982]   0.875000  0.162791  0.630301   \n",
       "27           124  0.443059  [0.24000001]   0.857143  0.139535  0.637970   \n",
       "37           144  0.377558  [0.32142857]   0.692308  0.209302  0.645121   \n",
       "15           125  0.444247  [0.25742576]   0.866667  0.151163  0.686537   \n",
       "17           108  0.700496          [0.]   0.000000  0.000000  0.709835   \n",
       "5            120  0.702675          [0.]   0.000000  0.000000  0.711571   \n",
       "29           100  0.699711          [0.]   0.000000  0.000000  0.714648   \n",
       "41           173  0.701585          [0.]   0.000000  0.000000  0.715421   \n",
       "53           118  0.705206          [0.]   0.000000  0.000000  0.721926   \n",
       "2            573  0.719981          [0.]   0.000000  0.000000  0.723887   \n",
       "38           530  0.717887          [0.]   0.000000  0.000000  0.724876   \n",
       "26           551  0.719300          [0.]   0.000000  0.000000  0.728458   \n",
       "50           536  0.725505          [0.]   0.000000  0.000000  0.730909   \n",
       "14           654  0.726051          [0.]   0.000000  0.000000  0.732795   \n",
       "35           103  0.770705          [0.]   0.000000  0.000000  0.782402   \n",
       "59           130  0.769447          [0.]   0.000000  0.000000  0.784204   \n",
       "23           126  0.773499          [0.]   0.000000  0.000000  0.785092   \n",
       "11           156  0.774268          [0.]   0.000000  0.000000  0.789291   \n",
       "56           455  0.784763          [0.]   0.000000  0.000000  0.790856   \n",
       "44           562  0.786373          [0.]   0.000000  0.000000  0.794553   \n",
       "8            559  0.786715          [0.]   0.000000  0.000000  0.795366   \n",
       "32           649  0.787931          [0.]   0.000000  0.000000  0.796805   \n",
       "20           488  0.785902          [0.]   0.000000  0.000000  0.797109   \n",
       "47           119  0.770334          [0.]   0.000000  0.000000  0.797165   \n",
       "\n",
       "    val_fbeta_score  val_precision  val_recall activation_layer  ...  dropout  \\\n",
       "54         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "19         0.206897       0.428571    0.136364             tanh  ...        0   \n",
       "1          0.193548       0.333333    0.136364          sigmoid  ...        0   \n",
       "30         0.153846       0.500000    0.090909             selu  ...        0   \n",
       "0          0.222222       0.600000    0.136364          sigmoid  ...        0   \n",
       "7          0.200000       0.375000    0.136364          sigmoid  ...        0   \n",
       "34         0.153846       0.500000    0.090909             selu  ...        0   \n",
       "55         0.160000       0.666667    0.090909             relu  ...        0   \n",
       "24         0.153846       0.500000    0.090909             selu  ...        0   \n",
       "42         0.076923       0.250000    0.045455              elu  ...        0   \n",
       "31         0.137931       0.285714    0.090909             selu  ...        0   \n",
       "10         0.142857       0.333333    0.090909          sigmoid  ...        0   \n",
       "36         0.142857       0.333333    0.090909              elu  ...        0   \n",
       "43         0.142857       0.333333    0.090909              elu  ...        0   \n",
       "6          0.000000       0.000000    0.000000          sigmoid  ...        0   \n",
       "48         0.222222       0.600000    0.136364             relu  ...        0   \n",
       "22         0.000000       0.000000    0.000000             tanh  ...        0   \n",
       "13         0.137931       0.285714    0.090909             tanh  ...        0   \n",
       "18         0.153846       0.500000    0.090909             tanh  ...        0   \n",
       "25         0.206897       0.428571    0.136364             selu  ...        0   \n",
       "58         0.080000       0.333333    0.045455             relu  ...        0   \n",
       "12         0.206897       0.428571    0.136364             tanh  ...        0   \n",
       "4          0.137931       0.285714    0.090909          sigmoid  ...        0   \n",
       "46         0.074074       0.200000    0.045455              elu  ...        0   \n",
       "39         0.200000       0.375000    0.136364              elu  ...        0   \n",
       "28         0.137931       0.285714    0.090909             selu  ...        0   \n",
       "57         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "40         0.148148       0.400000    0.090909              elu  ...        0   \n",
       "16         0.137931       0.285714    0.090909             tanh  ...        0   \n",
       "45         0.000000       0.000000    0.000000              elu  ...        0   \n",
       "52         0.137931       0.285714    0.090909             relu  ...        0   \n",
       "49         0.133333       0.250000    0.090909             relu  ...        0   \n",
       "21         0.086957       1.000000    0.045455             tanh  ...        0   \n",
       "3          0.142857       0.333333    0.090909          sigmoid  ...        0   \n",
       "51         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "33         0.000000       0.000000    0.000000             selu  ...        0   \n",
       "9          0.000000       0.000000    0.000000          sigmoid  ...        0   \n",
       "27         0.148148       0.400000    0.090909             selu  ...        0   \n",
       "37         0.000000       0.000000    0.000000              elu  ...        0   \n",
       "15         0.000000       0.000000    0.000000             tanh  ...        0   \n",
       "17         0.000000       0.000000    0.000000             tanh  ...        0   \n",
       "5          0.000000       0.000000    0.000000          sigmoid  ...        0   \n",
       "29         0.000000       0.000000    0.000000             selu  ...        0   \n",
       "41         0.000000       0.000000    0.000000              elu  ...        0   \n",
       "53         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "2          0.000000       0.000000    0.000000          sigmoid  ...        0   \n",
       "38         0.000000       0.000000    0.000000              elu  ...        0   \n",
       "26         0.000000       0.000000    0.000000             selu  ...        0   \n",
       "50         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "14         0.000000       0.000000    0.000000             tanh  ...        0   \n",
       "35         0.000000       0.000000    0.000000             selu  ...        0   \n",
       "59         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "23         0.000000       0.000000    0.000000             tanh  ...        0   \n",
       "11         0.000000       0.000000    0.000000          sigmoid  ...        0   \n",
       "56         0.000000       0.000000    0.000000             relu  ...        0   \n",
       "44         0.000000       0.000000    0.000000              elu  ...        0   \n",
       "8          0.000000       0.000000    0.000000          sigmoid  ...        0   \n",
       "32         0.000000       0.000000    0.000000             selu  ...        0   \n",
       "20         0.000000       0.000000    0.000000             tanh  ...        0   \n",
       "47         0.000000       0.000000    0.000000              elu  ...        0   \n",
       "\n",
       "    epochs  first_neuron  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "54  100000           110              7             50          orthogonal   \n",
       "19  100000           110              7             50          orthogonal   \n",
       "1   100000           110              5             50          orthogonal   \n",
       "30  100000           110              7             50          orthogonal   \n",
       "0   100000           110              5             50          orthogonal   \n",
       "7   100000           110              7             50          orthogonal   \n",
       "34  100000           110              7             50          orthogonal   \n",
       "55  100000           110              7             50          orthogonal   \n",
       "24  100000           110              5             50          orthogonal   \n",
       "42  100000           110              7             50          orthogonal   \n",
       "31  100000           110              7             50          orthogonal   \n",
       "10  100000           110              7             50          orthogonal   \n",
       "36  100000           110              5             50          orthogonal   \n",
       "43  100000           110              7             50          orthogonal   \n",
       "6   100000           110              7             50          orthogonal   \n",
       "48  100000           110              5             50          orthogonal   \n",
       "22  100000           110              7             50          orthogonal   \n",
       "13  100000           110              5             50          orthogonal   \n",
       "18  100000           110              7             50          orthogonal   \n",
       "25  100000           110              5             50          orthogonal   \n",
       "58  100000           110              7             50          orthogonal   \n",
       "12  100000           110              5             50          orthogonal   \n",
       "4   100000           110              5             50          orthogonal   \n",
       "46  100000           110              7             50          orthogonal   \n",
       "39  100000           110              5             50          orthogonal   \n",
       "28  100000           110              5             50          orthogonal   \n",
       "57  100000           110              7             50          orthogonal   \n",
       "40  100000           110              5             50          orthogonal   \n",
       "16  100000           110              5             50          orthogonal   \n",
       "45  100000           110              7             50          orthogonal   \n",
       "52  100000           110              5             50          orthogonal   \n",
       "49  100000           110              5             50          orthogonal   \n",
       "21  100000           110              7             50          orthogonal   \n",
       "3   100000           110              5             50          orthogonal   \n",
       "51  100000           110              5             50          orthogonal   \n",
       "33  100000           110              7             50          orthogonal   \n",
       "9   100000           110              7             50          orthogonal   \n",
       "27  100000           110              5             50          orthogonal   \n",
       "37  100000           110              5             50          orthogonal   \n",
       "15  100000           110              5             50          orthogonal   \n",
       "17  100000           110              5             50          orthogonal   \n",
       "5   100000           110              5             50          orthogonal   \n",
       "29  100000           110              5             50          orthogonal   \n",
       "41  100000           110              5             50          orthogonal   \n",
       "53  100000           110              5             50          orthogonal   \n",
       "2   100000           110              5             50          orthogonal   \n",
       "38  100000           110              5             50          orthogonal   \n",
       "26  100000           110              5             50          orthogonal   \n",
       "50  100000           110              5             50          orthogonal   \n",
       "14  100000           110              5             50          orthogonal   \n",
       "35  100000           110              7             50          orthogonal   \n",
       "59  100000           110              7             50          orthogonal   \n",
       "23  100000           110              7             50          orthogonal   \n",
       "11  100000           110              7             50          orthogonal   \n",
       "56  100000           110              7             50          orthogonal   \n",
       "44  100000           110              7             50          orthogonal   \n",
       "8   100000           110              7             50          orthogonal   \n",
       "32  100000           110              7             50          orthogonal   \n",
       "20  100000           110              7             50          orthogonal   \n",
       "47  100000           110              7             50          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation optimizer  \n",
       "54                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "19                 0.0001                 0.0001          sigmoid      adam  \n",
       "1                  0.0001                 0.0001          sigmoid      adam  \n",
       "30                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "0                  0.0001                 0.0001          sigmoid   rmsprop  \n",
       "7                  0.0001                 0.0001          sigmoid      adam  \n",
       "34                 0.0001                 0.0001          sigmoid     nadam  \n",
       "55                 0.0001                 0.0001          sigmoid      adam  \n",
       "24                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "42                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "31                 0.0001                 0.0001          sigmoid      adam  \n",
       "10                 0.0001                 0.0001          sigmoid     nadam  \n",
       "36                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "43                 0.0001                 0.0001          sigmoid      adam  \n",
       "6                  0.0001                 0.0001          sigmoid   rmsprop  \n",
       "48                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "22                 0.0001                 0.0001          sigmoid     nadam  \n",
       "13                 0.0001                 0.0001          sigmoid      adam  \n",
       "18                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "25                 0.0001                 0.0001          sigmoid      adam  \n",
       "58                 0.0001                 0.0001          sigmoid     nadam  \n",
       "12                 0.0001                 0.0001          sigmoid   rmsprop  \n",
       "4                  0.0001                 0.0001          sigmoid     nadam  \n",
       "46                 0.0001                 0.0001          sigmoid     nadam  \n",
       "39                 0.0001                 0.0001          sigmoid    adamax  \n",
       "28                 0.0001                 0.0001          sigmoid     nadam  \n",
       "57                 0.0001                 0.0001          sigmoid    adamax  \n",
       "40                 0.0001                 0.0001          sigmoid     nadam  \n",
       "16                 0.0001                 0.0001          sigmoid     nadam  \n",
       "45                 0.0001                 0.0001          sigmoid    adamax  \n",
       "52                 0.0001                 0.0001          sigmoid     nadam  \n",
       "49                 0.0001                 0.0001          sigmoid      adam  \n",
       "21                 0.0001                 0.0001          sigmoid    adamax  \n",
       "3                  0.0001                 0.0001          sigmoid    adamax  \n",
       "51                 0.0001                 0.0001          sigmoid    adamax  \n",
       "33                 0.0001                 0.0001          sigmoid    adamax  \n",
       "9                  0.0001                 0.0001          sigmoid    adamax  \n",
       "27                 0.0001                 0.0001          sigmoid    adamax  \n",
       "37                 0.0001                 0.0001          sigmoid      adam  \n",
       "15                 0.0001                 0.0001          sigmoid    adamax  \n",
       "17                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "5                  0.0001                 0.0001          sigmoid   adagrad  \n",
       "29                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "41                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "53                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "2                  0.0001                 0.0001          sigmoid  adadelta  \n",
       "38                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "26                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "50                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "14                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "35                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "59                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "23                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "11                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "56                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "44                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "8                  0.0001                 0.0001          sigmoid  adadelta  \n",
       "32                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "20                 0.0001                 0.0001          sigmoid  adadelta  \n",
       "47                 0.0001                 0.0001          sigmoid   adagrad  \n",
       "\n",
       "[60 rows x 24 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "056c4bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of optimizer')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'optimizer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "adb6bd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of activation_layer')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activation_layer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d0113",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "912b9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[110],  #\n",
    "     'hidden_neuron':[50], #\n",
    "\n",
    "     'hidden_layers':[5,7],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [32], #\n",
    "\n",
    "    'lr':[0.01,0.001,0.0001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "    'dropout': [0],\n",
    "    \n",
    "    'optimizer': ['rmsprop','adam','adadelta','adamax','nadam','adagrad'],\n",
    "    'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "    'activation_layer':['elu'],\n",
    "#    'batc_normalization':[False,True]\n",
    "    'batc_normalization':[False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b2fe2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.RMSprop(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=1.0, threshold=0.5),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=0,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9c6a1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/180 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C479E3798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1080 [00:57<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C44C02678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▍                                                                                 | 1/180 [00:07<23:18,  7.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47945438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C431A19D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▉                                                                                 | 2/180 [00:13<19:59,  6.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B6165E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42EADC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|█▎                                                                                | 3/180 [00:19<18:40,  6.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C479E33A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C43268A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|█▊                                                                                | 4/180 [00:25<17:50,  6.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F4B25E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42A9A318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|██▎                                                                               | 5/180 [00:30<17:13,  5.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42EAD5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C45F493A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|██▋                                                                               | 6/180 [00:36<17:09,  5.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C43268DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C433A0EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|███▏                                                                              | 7/180 [00:45<19:50,  6.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F2F5288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47945168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|███▋                                                                              | 8/180 [00:52<19:26,  6.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F2F55E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42EAD558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|████                                                                              | 9/180 [00:59<19:43,  6.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47350B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C411A9558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|████▌                                                                            | 10/180 [01:08<21:05,  7.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C431A1948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42A9AC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|████▉                                                                            | 11/180 [01:14<20:08,  7.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FDAD438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F2F5EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|█████▍                                                                           | 12/180 [01:21<19:29,  6.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C479E3828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42EADC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|█████▊                                                                           | 13/180 [01:31<22:35,  8.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C43268558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C44C02F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▎                                                                          | 14/180 [01:42<24:16,  8.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C411A9C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C431A1678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▊                                                                          | 15/180 [01:53<26:13,  9.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4339D048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4378BF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|███████▏                                                                         | 16/180 [02:03<26:38,  9.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C44C020D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C411A94C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|███████▋                                                                         | 17/180 [02:15<28:04, 10.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C479458B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4282C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|████████                                                                         | 18/180 [02:27<29:04, 10.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C432688B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C41399048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|████████▌                                                                        | 19/180 [02:33<25:00,  9.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C411A9AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C44C02438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█████████                                                                        | 20/180 [02:39<22:21,  8.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C44C02168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F2F5948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█████████▍                                                                       | 21/180 [02:45<20:43,  7.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F2F51F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C411A9168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█████████▉                                                                       | 22/180 [02:52<19:28,  7.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C426A3AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F2F54C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|██████████▎                                                                      | 23/180 [02:58<18:29,  7.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C413993A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C411A9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|██████████▊                                                                      | 24/180 [03:04<17:37,  6.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4282C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C426A3F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|███████████▎                                                                     | 25/180 [03:11<17:10,  6.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C479E3DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C41282678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|███████████▋                                                                     | 26/180 [03:17<16:39,  6.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C432685E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4282C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|████████████▏                                                                    | 27/180 [03:24<17:09,  6.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C414A1F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47C63C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|████████████▌                                                                    | 28/180 [03:31<17:26,  6.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C35B70438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A13A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█████████████                                                                    | 29/180 [03:38<17:11,  6.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C44C02318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4733E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█████████████▌                                                                   | 30/180 [03:45<16:57,  6.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4113DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4733E1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█████████████▉                                                                   | 31/180 [03:52<17:13,  6.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C478DA3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4282C1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|██████████████▍                                                                  | 32/180 [03:59<17:11,  6.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C45F49168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4113D1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|██████████████▊                                                                  | 33/180 [04:06<17:21,  7.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C473508B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A1048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|███████████████▎                                                                 | 34/180 [04:13<17:11,  7.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C411A9A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4378B1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|███████████████▊                                                                 | 35/180 [04:20<16:56,  7.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B6163A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A14C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▏                                                                | 36/180 [04:27<16:59,  7.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C411A9948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FB185E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|████████████████▋                                                                | 37/180 [04:33<15:54,  6.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A1CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C411A9168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|█████████████████                                                                | 38/180 [04:39<15:06,  6.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4282C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C411A9CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|█████████████████▌                                                               | 39/180 [04:45<14:31,  6.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3E038F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F2F58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██████████████████                                                               | 40/180 [04:50<14:06,  6.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F2F5438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4282CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██████████████████▍                                                              | 41/180 [04:56<13:48,  5.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FB18E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3B6164C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██████████████████▉                                                              | 42/180 [05:02<13:33,  5.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47945438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A1B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|███████████████████▎                                                             | 43/180 [05:12<16:05,  7.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C44C02288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A1AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|███████████████████▊                                                             | 44/180 [05:21<17:33,  7.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FB185E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C479454C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|████████████████████▎                                                            | 45/180 [05:30<18:26,  8.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4282C048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C479459D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|████████████████████▋                                                            | 46/180 [05:39<19:04,  8.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C478DA5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FB18948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|█████████████████████▏                                                           | 47/180 [05:49<19:36,  8.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C435E99D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FB18438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|█████████████████████▌                                                           | 48/180 [05:59<19:53,  9.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B6169D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4612F318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██████████████████████                                                           | 49/180 [06:29<33:57, 15.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FB189D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4733E5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██████████████████████▌                                                          | 50/180 [06:59<43:04, 19.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C433A0318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4378BA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██████████████████████▉                                                          | 51/180 [07:30<49:26, 23.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C478DA1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F93C828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|███████████████████████▍                                                         | 52/180 [08:00<53:31, 25.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FB18EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C43268708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|███████████████████████▊                                                         | 53/180 [08:30<56:14, 26.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C44C028B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C45F49DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|████████████████████████▎                                                        | 54/180 [09:00<58:00, 27.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C431A1168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4733EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|████████████████████████▊                                                        | 55/180 [09:08<45:29, 21.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FB18828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C478DAD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|█████████████████████████▏                                                       | 56/180 [09:16<36:39, 17.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C44C02F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42C9A4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|█████████████████████████▋                                                       | 57/180 [09:24<30:24, 14.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42C9AAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A19D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|██████████████████████████                                                       | 58/180 [09:32<26:02, 12.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F4B21F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C44C02E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|██████████████████████████▌                                                      | 59/180 [09:40<23:03, 11.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4378B168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42C9A558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███████████████████████████                                                      | 60/180 [09:49<20:52, 10.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C410E3318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FDAD708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███████████████████████████▍                                                     | 61/180 [10:03<22:47, 11.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4282CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F93C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███████████████████████████▉                                                     | 62/180 [10:17<24:26, 12.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42C9A8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F93C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|████████████████████████████▎                                                    | 63/180 [10:31<25:14, 12.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A1CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4282CC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|████████████████████████████▊                                                    | 64/180 [10:45<25:27, 13.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47C63048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4280E0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|█████████████████████████████▎                                                   | 65/180 [10:59<25:53, 13.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F2F53A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F93C5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|█████████████████████████████▋                                                   | 66/180 [11:13<25:43, 13.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4378BB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47C63438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|██████████████████████████████▏                                                  | 67/180 [12:17<53:50, 28.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4113D168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42C9A8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|█████████████████████████████▊                                                 | 68/180 [13:19<1:12:23, 38.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C478DAB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FDAD4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|██████████████████████████████▎                                                | 69/180 [14:23<1:25:26, 46.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C478DA318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4282CA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|██████████████████████████████▋                                                | 70/180 [15:23<1:32:27, 50.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C41282318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C45F49B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███████████████████████████████▏                                               | 71/180 [16:25<1:37:46, 53.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42FB25E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C40FAACA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|███████████████████████████████▌                                               | 72/180 [17:26<1:41:05, 56.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3B616F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C435E90D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|████████████████████████████████                                               | 73/180 [17:32<1:12:59, 40.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47C63558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42FB2828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|█████████████████████████████████▎                                               | 74/180 [17:37<53:32, 30.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C431A1DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C478DAE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|█████████████████████████████████▊                                               | 75/180 [17:43<40:05, 22.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FDAD8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C44C02CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|██████████████████████████████████▏                                              | 76/180 [17:49<30:43, 17.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C414A1288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42FB29D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|██████████████████████████████████▋                                              | 77/180 [17:55<24:23, 14.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C45F49708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42C9A168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|███████████████████████████████████                                              | 78/180 [18:01<19:57, 11.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47F6BDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A10D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|███████████████████████████████████▌                                             | 79/180 [18:07<16:52, 10.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4339DE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C426FC0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████████████████████████████████████                                             | 80/180 [18:12<14:27,  8.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4282C1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C479F3D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████████████████████████████████████▍                                            | 81/180 [18:18<12:44,  7.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C414A1288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C426FC5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████████████████████████████████████▉                                            | 82/180 [18:23<11:31,  7.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C412829D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FB183A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|█████████████████████████████████████▎                                           | 83/180 [18:29<10:41,  6.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FB180D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F843A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|█████████████████████████████████████▊                                           | 84/180 [18:35<10:16,  6.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C45F49B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C478DA4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|██████████████████████████████████████▎                                          | 85/180 [18:42<10:48,  6.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4282C318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C431A1D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|██████████████████████████████████████▋                                          | 86/180 [18:49<10:48,  6.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47C63558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C429E63A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|███████████████████████████████████████▏                                         | 87/180 [18:57<11:00,  7.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C41399B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FDADE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|███████████████████████████████████████▌                                         | 88/180 [19:04<10:53,  7.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47AE09D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C45F49708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|████████████████████████████████████████                                         | 89/180 [19:13<11:40,  7.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C429E6E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4378B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|████████████████████████████████████████▌                                        | 90/180 [19:21<11:24,  7.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4378BC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4282C4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|████████████████████████████████████████▉                                        | 91/180 [19:27<10:54,  7.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FB18AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3E211438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████████████████████████████████████████▍                                       | 92/180 [19:35<10:55,  7.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47945DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4282C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████████████████████████████████████████▊                                       | 93/180 [19:42<10:40,  7.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F93C5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47AE0678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|██████████████████████████████████████████▎                                      | 94/180 [19:49<10:07,  7.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C478DA0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47AE0558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|██████████████████████████████████████████▊                                      | 95/180 [19:55<09:45,  6.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F93C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C429E6E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|███████████████████████████████████████████▏                                     | 96/180 [20:02<09:30,  6.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FB18558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47AE0AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|███████████████████████████████████████████▋                                     | 97/180 [20:11<10:24,  7.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A1C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C429E64C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|████████████████████████████████████████████                                     | 98/180 [20:20<10:46,  7.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F93C9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A10D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|████████████████████████████████████████████▌                                    | 99/180 [20:30<11:42,  8.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C411A93A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B6373A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|████████████████████████████████████████████▍                                   | 100/180 [20:39<11:40,  8.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4774F288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C414A1048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|████████████████████████████████████████████▉                                   | 101/180 [20:48<11:35,  8.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C429E60D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47AE0828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████████████████████████████████████████████▎                                  | 102/180 [20:57<11:29,  8.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A1CC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4774F438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████████████████████████████████████████████▊                                  | 103/180 [21:11<13:22, 10.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C413B9048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FDAD438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|██████████████████████████████████████████████▏                                 | 104/180 [21:23<13:58, 11.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42FB2DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C413B9438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|██████████████████████████████████████████████▋                                 | 105/180 [21:37<14:50, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A1C5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C413B9DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|███████████████████████████████████████████████                                 | 106/180 [21:51<15:23, 12.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C39E140D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3F9378B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|███████████████████████████████████████████████▌                                | 107/180 [22:04<15:25, 12.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F9374C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C413B99D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|████████████████████████████████████████████████                                | 108/180 [22:18<15:34, 12.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C45F491F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42EE5168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|████████████████████████████████████████████████▍                               | 109/180 [22:26<13:29, 11.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4774FE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42EE5048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|████████████████████████████████████████████████▉                               | 110/180 [22:32<11:40, 10.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C387A80D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B637828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|█████████████████████████████████████████████████▎                              | 111/180 [22:40<10:31,  9.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C411A9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B637D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|█████████████████████████████████████████████████▊                              | 112/180 [22:47<09:36,  8.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C437B34C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B637AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████████████████████████████████████████████████▏                             | 113/180 [22:53<08:55,  7.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C437B3288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B637A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████████████████████████████████████████████████▋                             | 114/180 [23:01<08:30,  7.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4B27E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B6378B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|███████████████████████████████████████████████████                             | 115/180 [23:08<08:12,  7.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4B27EDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B637558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|███████████████████████████████████████████████████▌                            | 116/180 [23:15<08:06,  7.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C479F34C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B637048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|████████████████████████████████████████████████████                            | 117/180 [23:23<07:55,  7.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4B5399D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B637AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|████████████████████████████████████████████████████▍                           | 118/180 [23:30<07:41,  7.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F2F5948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C437B3438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|████████████████████████████████████████████████████▉                           | 119/180 [23:38<07:45,  7.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F4B2048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B27EA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|█████████████████████████████████████████████████████▎                          | 120/180 [23:46<07:44,  7.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FEBFF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C45F49B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|█████████████████████████████████████████████████████▊                          | 121/180 [23:57<08:25,  8.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C429E60D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FEBFC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████████████████████████████████████████████████████▏                         | 122/180 [24:06<08:38,  8.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FDADB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B27E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████████████████████████████████████████████████████▋                         | 123/180 [24:15<08:23,  8.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C414A1558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42FB2DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|███████████████████████████████████████████████████████                         | 124/180 [24:25<08:27,  9.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3F4B2318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47AF0B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|███████████████████████████████████████████████████████▌                        | 125/180 [24:34<08:21,  9.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47A614C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C413B9798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|████████████████████████████████████████████████████████                        | 126/180 [24:44<08:36,  9.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4B539B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FDAD318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|████████████████████████████████████████████████████████▍                       | 127/180 [24:52<07:53,  8.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C46160E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4774FA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|████████████████████████████████████████████████████████▉                       | 128/180 [24:59<07:19,  8.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A1C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C4B27ECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|█████████████████████████████████████████████████████████▎                      | 129/180 [25:06<06:51,  8.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C479F3798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C437B3048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|█████████████████████████████████████████████████████████▊                      | 130/180 [25:14<06:33,  7.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C411A93A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42EE58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|██████████████████████████████████████████████████████████▏                     | 131/180 [25:21<06:13,  7.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C478DAC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42C9A3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|██████████████████████████████████████████████████████████▋                     | 132/180 [25:28<05:57,  7.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FF41828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47AE0CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████████████████████████████████████████████████████████                     | 133/180 [25:39<06:39,  8.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C47AE0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C389FF048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████████████████████████████████████████████████████████▌                    | 134/180 [25:50<07:05,  9.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C389FF678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C478DA8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|████████████████████████████████████████████████████████████                    | 135/180 [26:01<07:19,  9.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C38A1C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C389FFA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|████████████████████████████████████████████████████████████▍                   | 136/180 [26:12<07:24, 10.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C410E3948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C389FF678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|████████████████████████████████████████████████████████████▉                   | 137/180 [26:22<07:23, 10.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4282CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C387A8438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|█████████████████████████████████████████████████████████████▎                  | 138/180 [26:33<07:22, 10.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3FDAD318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C3FEBF828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|█████████████████████████████████████████████████████████████▊                  | 139/180 [27:08<12:09, 17.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C413B9F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C47AE0CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|██████████████████████████████████████████████████████████████▏                 | 140/180 [27:43<15:20, 23.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4282C4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C42FB2D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|██████████████████████████████████████████████████████████████▋                 | 141/180 [28:19<17:21, 26.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C42FB2CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C412DE5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|███████████████████████████████████████████████████████████████                 | 142/180 [28:54<18:27, 29.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C478DA798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C429E6AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|███████████████████████████████████████████████████████████████▌                | 143/180 [29:26<18:34, 30.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'zeros', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C4B27E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018C412DEDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████████████████████████████████████████████████████████████                | 144/180 [29:57<18:14, 30.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'hidden_layers': 7, 'hidden_neuron': 50, 'kernel_initializer': 'ones', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018C3E211708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_8/Sigmoid:0) = ] [[-nan][-nan][-nan]...] [y (Cast_5/x:0) = ] [0]\n\t [[node assert_greater_equal/Assert/AssertGuard/Assert\n (defined at D:\\anaconda\\lib\\site-packages\\keras\\utils\\metrics_utils.py:611)\n]] [Op:__inference_train_function_10525500]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node assert_greater_equal/Assert/AssertGuard/Assert:\nIn[0] assert_greater_equal/Assert/AssertGuard/Assert/assert_greater_equal/All:\t\nIn[1] assert_greater_equal/Assert/AssertGuard/Assert/data_0:\t\nIn[2] assert_greater_equal/Assert/AssertGuard/Assert/data_1:\t\nIn[3] assert_greater_equal/Assert/AssertGuard/Assert/data_2:\t\nIn[4] assert_greater_equal/Assert/AssertGuard/Assert/sequential/dense_8/Sigmoid:\t\nIn[5] assert_greater_equal/Assert/AssertGuard/Assert/data_4:\t\nIn[6] assert_greater_equal/Assert/AssertGuard/Assert/Cast_5/x:\n\nOperation defined at: (most recent call last)\n>>>   File \"D:\\anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"D:\\anaconda\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"D:\\anaconda\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"D:\\anaconda\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"D:\\anaconda\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n>>>     self.ctx_run(self.run)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3166, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-78-e7bdfdb9da50>\", line 5, in <module>\n>>>     experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\talos\\scan\\Scan.py\", line 196, in __init__\n>>>     scan_run(self)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\talos\\scan\\scan_run.py\", line 26, in scan_run\n>>>     self = scan_round(self)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\talos\\scan\\scan_round.py\", line 19, in scan_round\n>>>     self.model_history, self.round_model = ingest_model(self)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\talos\\model\\ingest_model.py\", line 10, in ingest_model\n>>>     self.round_params)\n>>> \n>>>   File \"<ipython-input-77-e3294614e259>\", line 64, in numerai_model\n>>>     restore_best_weights=True)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 817, in train_step\n>>>     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 460, in update_state\n>>>     metric_obj.update_state(y_t, y_p, sample_weight=mask)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 73, in decorated\n>>>     update_op = update_state_fn(*args, **kwargs)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\metrics.py\", line 177, in update_state_fn\n>>>     return ag_update_state(*args, **kwargs)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\metrics.py\", line 1410, in update_state\n>>>     sample_weight=sample_weight)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 611, in update_confusion_matrix_variables\n>>>     message='predictions must be >= 0'),\n>>> \n\nFunction call stack:\ntrain_function -> assert_greater_equal_Assert_AssertGuard_false_10525259\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-e7bdfdb9da50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumerai_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# start runtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscan_run\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mscan_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\talos\\scan\\scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# otherwise proceed with next permutation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscan_round\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\talos\\scan\\scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mingest_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\talos\\model\\ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                       self.round_params)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-e3294614e259>\u001b[0m in \u001b[0;36mnumerai_model\u001b[1;34m(x_train, y_train, x_val, y_val, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                                         \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                                         \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                                                       restore_best_weights=True)\n\u001b[0m\u001b[0;32m     65\u001b[0m                                     ] #,ta.live(),\n\u001b[0;32m     66\u001b[0m                         )\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_8/Sigmoid:0) = ] [[-nan][-nan][-nan]...] [y (Cast_5/x:0) = ] [0]\n\t [[node assert_greater_equal/Assert/AssertGuard/Assert\n (defined at D:\\anaconda\\lib\\site-packages\\keras\\utils\\metrics_utils.py:611)\n]] [Op:__inference_train_function_10525500]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node assert_greater_equal/Assert/AssertGuard/Assert:\nIn[0] assert_greater_equal/Assert/AssertGuard/Assert/assert_greater_equal/All:\t\nIn[1] assert_greater_equal/Assert/AssertGuard/Assert/data_0:\t\nIn[2] assert_greater_equal/Assert/AssertGuard/Assert/data_1:\t\nIn[3] assert_greater_equal/Assert/AssertGuard/Assert/data_2:\t\nIn[4] assert_greater_equal/Assert/AssertGuard/Assert/sequential/dense_8/Sigmoid:\t\nIn[5] assert_greater_equal/Assert/AssertGuard/Assert/data_4:\t\nIn[6] assert_greater_equal/Assert/AssertGuard/Assert/Cast_5/x:\n\nOperation defined at: (most recent call last)\n>>>   File \"D:\\anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"D:\\anaconda\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"D:\\anaconda\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"D:\\anaconda\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"D:\\anaconda\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n>>>     self.ctx_run(self.run)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3166, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-78-e7bdfdb9da50>\", line 5, in <module>\n>>>     experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\talos\\scan\\Scan.py\", line 196, in __init__\n>>>     scan_run(self)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\talos\\scan\\scan_run.py\", line 26, in scan_run\n>>>     self = scan_round(self)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\talos\\scan\\scan_round.py\", line 19, in scan_round\n>>>     self.model_history, self.round_model = ingest_model(self)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\talos\\model\\ingest_model.py\", line 10, in ingest_model\n>>>     self.round_params)\n>>> \n>>>   File \"<ipython-input-77-e3294614e259>\", line 64, in numerai_model\n>>>     restore_best_weights=True)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 817, in train_step\n>>>     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 460, in update_state\n>>>     metric_obj.update_state(y_t, y_p, sample_weight=mask)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 73, in decorated\n>>>     update_op = update_state_fn(*args, **kwargs)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\metrics.py\", line 177, in update_state_fn\n>>>     return ag_update_state(*args, **kwargs)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\metrics.py\", line 1410, in update_state\n>>>     sample_weight=sample_weight)\n>>> \n>>>   File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 611, in update_confusion_matrix_variables\n>>>     message='predictions must be >= 0'),\n>>> \n\nFunction call stack:\ntrain_function -> assert_greater_equal_Assert_AssertGuard_false_10525259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "34a75b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/M/Predykcja klasy M - podejscie z Focal-loss (nowe)/050522205253.csv')\n",
    "#binary cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6795e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>0.428949</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473674</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "      <td>0.426787</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.445202</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>0.428306</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464828</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>0.431504</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451393</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0.423689</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459189</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>584</td>\n",
       "      <td>0.464915</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468334</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>584</td>\n",
       "      <td>0.464928</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468348</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>582</td>\n",
       "      <td>0.465191</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468601</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>584</td>\n",
       "      <td>0.464846</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468264</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>585</td>\n",
       "      <td>0.464568</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467997</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss fbeta_score  precision  recall  val_loss  \\\n",
       "0              98  0.428949        [0.]        0.0     0.0  0.473674   \n",
       "1              59  0.426787        [0.]        0.0     0.0  0.445202   \n",
       "2              58  0.428306        [0.]        0.0     0.0  0.464828   \n",
       "3              58  0.431504        [0.]        0.0     0.0  0.451393   \n",
       "4              57  0.423689        [0.]        0.0     0.0  0.459189   \n",
       "..            ...       ...         ...        ...     ...       ...   \n",
       "139           584  0.464915        [0.]        0.0     0.0  0.468334   \n",
       "140           584  0.464928        [0.]        0.0     0.0  0.468348   \n",
       "141           582  0.465191        [0.]        0.0     0.0  0.468601   \n",
       "142           584  0.464846        [0.]        0.0     0.0  0.468264   \n",
       "143           585  0.464568        [0.]        0.0     0.0  0.467997   \n",
       "\n",
       "    val_fbeta_score  val_precision  val_recall activation_layer  ...  epochs  \\\n",
       "0              [0.]            0.0         0.0              elu  ...  100000   \n",
       "1              [0.]            0.0         0.0              elu  ...  100000   \n",
       "2              [0.]            0.0         0.0              elu  ...  100000   \n",
       "3              [0.]            0.0         0.0              elu  ...  100000   \n",
       "4              [0.]            0.0         0.0              elu  ...  100000   \n",
       "..              ...            ...         ...              ...  ...     ...   \n",
       "139            [0.]            0.0         0.0              elu  ...  100000   \n",
       "140            [0.]            0.0         0.0              elu  ...  100000   \n",
       "141            [0.]            0.0         0.0              elu  ...  100000   \n",
       "142            [0.]            0.0         0.0              elu  ...  100000   \n",
       "143            [0.]            0.0         0.0              elu  ...  100000   \n",
       "\n",
       "     first_neuron  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "0             110              5             50          orthogonal   \n",
       "1             110              5             50          orthogonal   \n",
       "2             110              5             50          orthogonal   \n",
       "3             110              5             50          orthogonal   \n",
       "4             110              5             50          orthogonal   \n",
       "..            ...            ...            ...                 ...   \n",
       "139           110              7             50               zeros   \n",
       "140           110              7             50               zeros   \n",
       "141           110              7             50               zeros   \n",
       "142           110              7             50               zeros   \n",
       "143           110              7             50               zeros   \n",
       "\n",
       "     kernel_regularizer_l1  kernel_regularizer_l2  last_activation      lr  \\\n",
       "0                   0.0001                 0.0001          sigmoid  0.0100   \n",
       "1                   0.0001                 0.0001          sigmoid  0.0100   \n",
       "2                   0.0001                 0.0001          sigmoid  0.0100   \n",
       "3                   0.0001                 0.0001          sigmoid  0.0100   \n",
       "4                   0.0001                 0.0001          sigmoid  0.0100   \n",
       "..                     ...                    ...              ...     ...   \n",
       "139                 0.0001                 0.0001          sigmoid  0.0001   \n",
       "140                 0.0001                 0.0001          sigmoid  0.0001   \n",
       "141                 0.0001                 0.0001          sigmoid  0.0001   \n",
       "142                 0.0001                 0.0001          sigmoid  0.0001   \n",
       "143                 0.0001                 0.0001          sigmoid  0.0001   \n",
       "\n",
       "    optimizer  \n",
       "0     rmsprop  \n",
       "1        adam  \n",
       "2    adadelta  \n",
       "3      adamax  \n",
       "4       nadam  \n",
       "..        ...  \n",
       "139      adam  \n",
       "140  adadelta  \n",
       "141    adamax  \n",
       "142     nadam  \n",
       "143   adagrad  \n",
       "\n",
       "[144 rows x 25 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ac45fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stare_wart=df['val_fbeta_score']\n",
    "nowe_wart=[]\n",
    "for x in stare_wart:\n",
    "    wart=x.replace('[','')\n",
    "    wart=wart.replace(']','')\n",
    "    wart=float(wart)\n",
    "    nowe_wart.append(wart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "645be9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['val_fbeta_score']=nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3298d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7913815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>59</td>\n",
       "      <td>0.425842</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>59</td>\n",
       "      <td>0.425605</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>59</td>\n",
       "      <td>0.425672</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>59</td>\n",
       "      <td>0.425684</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>59</td>\n",
       "      <td>0.425741</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>zeros</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>207</td>\n",
       "      <td>0.479393</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.581842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>164</td>\n",
       "      <td>0.492485</td>\n",
       "      <td>[0.1875]</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.582664</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>166</td>\n",
       "      <td>0.495091</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.593480</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>185</td>\n",
       "      <td>0.487273</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.607072</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>61</td>\n",
       "      <td>0.430082</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "41             59  0.425842          [0.]        0.0  0.000000  0.431468   \n",
       "126            59  0.425605          [0.]        0.0  0.000000  0.431472   \n",
       "128            59  0.425672          [0.]        0.0  0.000000  0.431479   \n",
       "39             59  0.425684          [0.]        0.0  0.000000  0.431491   \n",
       "40             59  0.425741          [0.]        0.0  0.000000  0.431499   \n",
       "..            ...       ...           ...        ...       ...       ...   \n",
       "107           207  0.479393  [0.17021276]        1.0  0.093023  0.581842   \n",
       "15            164  0.492485      [0.1875]        0.9  0.104651  0.582664   \n",
       "13            166  0.495091  [0.17021276]        1.0  0.093023  0.593480   \n",
       "103           185  0.487273  [0.17021276]        1.0  0.093023  0.607072   \n",
       "95             61  0.430082          [0.]        0.0  0.000000  0.930028   \n",
       "\n",
       "     val_fbeta_score  val_precision  val_recall activation_layer  ...  epochs  \\\n",
       "41          0.000000       0.000000    0.000000              elu  ...  100000   \n",
       "126         0.000000       0.000000    0.000000              elu  ...  100000   \n",
       "128         0.000000       0.000000    0.000000              elu  ...  100000   \n",
       "39          0.000000       0.000000    0.000000              elu  ...  100000   \n",
       "40          0.000000       0.000000    0.000000              elu  ...  100000   \n",
       "..               ...            ...         ...              ...  ...     ...   \n",
       "107         0.000000       0.000000    0.000000              elu  ...  100000   \n",
       "15          0.142857       0.333333    0.090909              elu  ...  100000   \n",
       "13          0.076923       0.250000    0.045455              elu  ...  100000   \n",
       "103         0.153846       0.500000    0.090909              elu  ...  100000   \n",
       "95          0.000000       0.000000    0.000000              elu  ...  100000   \n",
       "\n",
       "     first_neuron  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "41            110              5             50               zeros   \n",
       "126           110              7             50               zeros   \n",
       "128           110              7             50               zeros   \n",
       "39            110              5             50               zeros   \n",
       "40            110              5             50               zeros   \n",
       "..            ...            ...            ...                 ...   \n",
       "107           110              7             50          orthogonal   \n",
       "15            110              5             50          orthogonal   \n",
       "13            110              5             50          orthogonal   \n",
       "103           110              7             50          orthogonal   \n",
       "95            110              7             50          orthogonal   \n",
       "\n",
       "     kernel_regularizer_l1  kernel_regularizer_l2  last_activation      lr  \\\n",
       "41                  0.0001                 0.0001          sigmoid  0.0100   \n",
       "126                 0.0001                 0.0001          sigmoid  0.0100   \n",
       "128                 0.0001                 0.0001          sigmoid  0.0100   \n",
       "39                  0.0001                 0.0001          sigmoid  0.0100   \n",
       "40                  0.0001                 0.0001          sigmoid  0.0100   \n",
       "..                     ...                    ...              ...     ...   \n",
       "107                 0.0001                 0.0001          sigmoid  0.0001   \n",
       "15                  0.0001                 0.0001          sigmoid  0.0001   \n",
       "13                  0.0001                 0.0001          sigmoid  0.0001   \n",
       "103                 0.0001                 0.0001          sigmoid  0.0001   \n",
       "95                  0.0001                 0.0001          sigmoid  0.0100   \n",
       "\n",
       "    optimizer  \n",
       "41    adagrad  \n",
       "126   rmsprop  \n",
       "128  adadelta  \n",
       "39     adamax  \n",
       "40      nadam  \n",
       "..        ...  \n",
       "107   adagrad  \n",
       "15     adamax  \n",
       "13       adam  \n",
       "103      adam  \n",
       "95    adagrad  \n",
       "\n",
       "[144 rows x 25 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2745ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

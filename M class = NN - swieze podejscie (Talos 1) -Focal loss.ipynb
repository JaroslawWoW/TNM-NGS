{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-iraqi",
   "metadata": {},
   "source": [
    "## 1. Biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compressed-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import talos as ta\n",
    "from talos.model.early_stopper import early_stopper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext tensorboard\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from focal_loss import BinaryFocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32aa6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define our custom loss function.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        # Define epsilon so that the back-propagation will not result in NaN for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        # y_pred = y_pred + epsilon\n",
    "        # Clip the prediciton value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        # Calculate alpha_t\n",
    "        alpha_factor = K.ones_like(y_true) * alpha\n",
    "        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -K.log(p_t)\n",
    "        weight = alpha_t * K.pow((1 - p_t), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.mean(K.sum(loss, axis=1))\n",
    "        return loss\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "\n",
    "def categorical_focal_loss(alpha, gamma=2.):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "    When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "    loss.\n",
    "           m\n",
    "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "      categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Compute mean loss in mini_batch\n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "    return categorical_focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-upset",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "humanitarian-desperate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/Dane_do_uczenia_M.csv\", encoding=\"utf-8\")\n",
    "del train_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b3fcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000212232.1</th>\n",
       "      <th>ENSG00000238741.1</th>\n",
       "      <th>ENSG00000252481.1</th>\n",
       "      <th>ENSG00000239002.3</th>\n",
       "      <th>ENSG00000212443.1</th>\n",
       "      <th>ENSG00000274012.1</th>\n",
       "      <th>ENSG00000252010.1</th>\n",
       "      <th>ENSG00000202198.1</th>\n",
       "      <th>ENSG00000251791.1</th>\n",
       "      <th>ENSG00000202058.1</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000241475.1</th>\n",
       "      <th>ENSG00000274618.1</th>\n",
       "      <th>ENSG00000227293.1</th>\n",
       "      <th>ENSG00000253526.1</th>\n",
       "      <th>ENSG00000270654.1</th>\n",
       "      <th>ENSG00000271394.1</th>\n",
       "      <th>ENSG00000265423.1</th>\n",
       "      <th>ENSG00000253165.1</th>\n",
       "      <th>ENSG00000201901.1</th>\n",
       "      <th>scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.356617</td>\n",
       "      <td>31.768974</td>\n",
       "      <td>27.356617</td>\n",
       "      <td>5.294829</td>\n",
       "      <td>8.824715</td>\n",
       "      <td>6.645010e+02</td>\n",
       "      <td>2.647415</td>\n",
       "      <td>4.235863e+02</td>\n",
       "      <td>7.059772</td>\n",
       "      <td>16.766959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.695633</td>\n",
       "      <td>1.086954</td>\n",
       "      <td>6.521724</td>\n",
       "      <td>2.173908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.758692e+03</td>\n",
       "      <td>1.086954</td>\n",
       "      <td>5.434770e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.185177</td>\n",
       "      <td>77.002713</td>\n",
       "      <td>7.475992</td>\n",
       "      <td>4.485595</td>\n",
       "      <td>8.971190</td>\n",
       "      <td>2.775836e+03</td>\n",
       "      <td>2.242797</td>\n",
       "      <td>2.609121e+02</td>\n",
       "      <td>6.728392</td>\n",
       "      <td>10.466388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.747599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.157930</td>\n",
       "      <td>17.431612</td>\n",
       "      <td>6.536855</td>\n",
       "      <td>2.178952</td>\n",
       "      <td>2.905269</td>\n",
       "      <td>2.338741e+02</td>\n",
       "      <td>3.631586</td>\n",
       "      <td>6.827382e+01</td>\n",
       "      <td>1.452634</td>\n",
       "      <td>1.452634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29912.168049</td>\n",
       "      <td>21631.677176</td>\n",
       "      <td>9554.333460</td>\n",
       "      <td>20332.131551</td>\n",
       "      <td>5136.495208</td>\n",
       "      <td>1.255850e+06</td>\n",
       "      <td>19221.760289</td>\n",
       "      <td>1.926818e+06</td>\n",
       "      <td>5198.182500</td>\n",
       "      <td>2655.637935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>953.068666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.562431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269.367843</td>\n",
       "      <td>64.771657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.374585</td>\n",
       "      <td>M0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000212232.1  ENSG00000238741.1  ENSG00000252481.1  ENSG00000239002.3  \\\n",
       "0          27.356617          31.768974          27.356617           5.294829   \n",
       "1           8.695633           1.086954           6.521724           2.173908   \n",
       "2          20.185177          77.002713           7.475992           4.485595   \n",
       "3          18.157930          17.431612           6.536855           2.178952   \n",
       "4       29912.168049       21631.677176        9554.333460       20332.131551   \n",
       "\n",
       "   ENSG00000212443.1  ENSG00000274012.1  ENSG00000252010.1  ENSG00000202198.1  \\\n",
       "0           8.824715       6.645010e+02           2.647415       4.235863e+02   \n",
       "1           0.000000       1.758692e+03           1.086954       5.434770e+01   \n",
       "2           8.971190       2.775836e+03           2.242797       2.609121e+02   \n",
       "3           2.905269       2.338741e+02           3.631586       6.827382e+01   \n",
       "4        5136.495208       1.255850e+06       19221.760289       1.926818e+06   \n",
       "\n",
       "   ENSG00000251791.1  ENSG00000202058.1  ...  ENSG00000241475.1  \\\n",
       "0           7.059772          16.766959  ...           0.000000   \n",
       "1           0.000000           0.000000  ...           0.000000   \n",
       "2           6.728392          10.466388  ...           0.000000   \n",
       "3           1.452634           1.452634  ...           0.726317   \n",
       "4        5198.182500        2655.637935  ...           0.000000   \n",
       "\n",
       "   ENSG00000274618.1  ENSG00000227293.1  ENSG00000253526.1  ENSG00000270654.1  \\\n",
       "0           0.000000                0.0           0.000000                0.0   \n",
       "1           0.000000                0.0           0.000000                0.0   \n",
       "2           0.747599                0.0           0.747599                0.0   \n",
       "3           0.000000                0.0           0.000000                0.0   \n",
       "4         953.068666                0.0          20.562431                0.0   \n",
       "\n",
       "   ENSG00000271394.1  ENSG00000265423.1  ENSG00000253165.1  ENSG00000201901.1  \\\n",
       "0           0.000000           0.000000                0.0           0.000000   \n",
       "1           0.000000           0.000000                0.0           0.000000   \n",
       "2           0.747599           0.000000                0.0           0.000000   \n",
       "3           0.000000           0.000000                0.0           0.000000   \n",
       "4         269.367843          64.771657                0.0         123.374585   \n",
       "\n",
       "   scale  \n",
       "0     M0  \n",
       "1     M0  \n",
       "2     M0  \n",
       "3     M0  \n",
       "4     M0  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c392d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['scale'].loc[(train_df['scale'] == 'M0')] = 0\n",
    "train_df['scale'].loc[(train_df['scale'] == 'M1')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db4b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.loc[(train_df['scale']!='MX')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33410b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42,stratify=train_df['scale'])\n",
    "#test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42,stratify=test_df['scale'])\n",
    "\n",
    "\n",
    "train_label=train_df['scale']\n",
    "test_label=test_df['scale']\n",
    "#val_label=val_df['scale']\n",
    "del train_df['scale']\n",
    "del test_df['scale']\n",
    "#del val_df['scale']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef6a0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6644f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cedcb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1cca4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-precipitation",
   "metadata": {},
   "source": [
    "## 1.2 Standaryzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indonesian-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df=scaler.fit_transform(train_df)\n",
    "test_df=scaler.fit_transform(test_df)\n",
    "#val_df=scaler.fit_transform(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcc66d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23532639, -0.28536286, -0.29189695, ..., -0.20273415,\n",
       "        -0.13658188, -0.25716709],\n",
       "       [-0.23488914, -0.28412701, -0.28766847, ..., -0.20273415,\n",
       "        -0.13658188, -0.25716709],\n",
       "       [-0.23513615, -0.28194648, -0.28119849, ..., -0.1127296 ,\n",
       "        -0.13658188, -0.18291094],\n",
       "       ...,\n",
       "       [-0.23560858, -0.28411483, -0.29158105, ..., -0.20273415,\n",
       "        -0.07110588, -0.25716709],\n",
       "       [13.92769305,  3.11477926,  6.81130447, ..., -0.05473062,\n",
       "        -0.13658188,  1.2895196 ],\n",
       "       [-0.23495945, -0.28021459, -0.28164044, ..., -0.20273415,\n",
       "        -0.07405669, -0.25716709]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "humanitarian-sperm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 110)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2486bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=np.asarray(train_label).astype(np.int)\n",
    "test_label=np.asarray(test_label).astype(np.int)\n",
    "#val_label=np.asarray(val_label).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b23fc815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "263db0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "633de496",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_label.reshape(-1,1)\n",
    "test_label=test_label.reshape(-1,1)\n",
    "#val_label=val_label.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "750b76b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-proportion",
   "metadata": {},
   "source": [
    "# 2 Moduł TALOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-stick",
   "metadata": {},
   "source": [
    "## 2.1 Słownik parametrów do wypróbowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dobor Gammy i alfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ed670b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[110], \n",
    "     'hidden_neuron':[100],\n",
    "\n",
    "     'hidden_layers':[3],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [32],\n",
    "\n",
    "    'lr':[0.001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "#    'batc_normalization':[False,True],\n",
    "#    'dropout': [0,0.2,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "    #'optimizer': ['rmsprop','adam','adadelta','adamax','nadam','adagrad'],\n",
    "    #'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "    'kernel_initializer': ['orthogonal'],\n",
    "    #'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    "    'activation_layer':['relu'],\n",
    "    #'batc_normalization':[False,True]\n",
    "    'batc_normalization':[False],\n",
    "    'alpha':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "     \n",
    "\n",
    "    'gamma':[0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
    "    \n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aeff143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss=[binary_focal_loss(alpha=params['alpha'], gamma=params['gamma'])],\n",
    "                  optimizer=tf.keras.optimizers.Adamax(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=1.0, threshold=0.5),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-married",
   "metadata": {},
   "source": [
    "## 2.3 Przeprowadzam skan, używając parametrów i funkcji wyżej\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec12a0dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/110 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F4B798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624EDDD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256210890D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 175.\n",
      "Epoch 00225: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▋                                                                                 | 1/110 [00:12<22:38, 12.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025627C10F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A1CF4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A27BE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 158.\n",
      "Epoch 00208: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|█▍                                                                                | 2/110 [00:23<21:24, 11.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025622198798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627C10288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562638E708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 125.\n",
      "Epoch 00175: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|██▏                                                                               | 3/110 [00:34<19:43, 11.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A2250D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562638E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABDF828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 126.\n",
      "Epoch 00176: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|██▉                                                                               | 4/110 [00:44<18:51, 10.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A223A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256250C7D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256210895E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 129.\n",
      "Epoch 00179: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|███▋                                                                              | 5/110 [00:54<18:24, 10.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562638E948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627C104C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623AD8948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 95.\n",
      "Epoch 00145: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|████▍                                                                             | 6/110 [01:03<17:19,  9.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F4BCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620F66048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256210895E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 136.\n",
      "Epoch 00186: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|█████▏                                                                            | 7/110 [01:14<17:38, 10.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F101F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629F4BE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256210894C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 77.\n",
      "Epoch 00127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|█████▉                                                                            | 8/110 [01:22<16:25,  9.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F10AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624F438B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025624EDDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 00110: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▋                                                                           | 9/110 [01:30<15:10,  9.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025624EDD678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623AD8318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025621089C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|███████▎                                                                         | 10/110 [01:37<14:21,  8.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025625076948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256265FCE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4BAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 73.\n",
      "Epoch 00123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|████████                                                                         | 11/110 [01:45<13:56,  8.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623AD83A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623765DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4B9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 169.\n",
      "Epoch 00219: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|████████▊                                                                        | 12/110 [01:57<15:34,  9.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025621089438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624EDDDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4BF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 129.\n",
      "Epoch 00179: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█████████▌                                                                       | 13/110 [02:08<15:48,  9.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A223948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025621089C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562638E318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 132.\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|██████████▎                                                                      | 14/110 [02:18<15:53,  9.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A64B438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562AD701F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025625076948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 107.\n",
      "Epoch 00157: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|███████████                                                                      | 15/110 [02:27<15:26,  9.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025622198A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2B6798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562652B3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 157.\n",
      "Epoch 00207: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|███████████▊                                                                     | 16/110 [02:39<16:03, 10.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025627C10288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A47E798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A223168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 126.\n",
      "Epoch 00176: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|████████████▌                                                                    | 17/110 [02:49<15:50, 10.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256233975E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2235E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4B5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 132.\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█████████████▎                                                                   | 18/110 [02:59<15:40, 10.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A47E9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623AD8EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025622198B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 86.\n",
      "Epoch 00136: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█████████████▉                                                                   | 19/110 [03:08<14:39,  9.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A223EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A47EAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4BB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "Epoch 00118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|██████████████▋                                                                  | 20/110 [03:15<13:33,  9.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623C8CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256209EA168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A059048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 82.\n",
      "Epoch 00132: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|███████████████▍                                                                 | 21/110 [03:23<13:00,  8.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.2, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A1CF678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256236DA0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E8F4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 129.\n",
      "Epoch 00179: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▏                                                                | 22/110 [03:34<13:33,  9.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256236DA1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624EDDE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4BD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 124.\n",
      "Epoch 00174: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|████████████████▉                                                                | 23/110 [03:44<13:55,  9.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E5A798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A225828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623765B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 128.\n",
      "Epoch 00178: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|█████████████████▋                                                               | 24/110 [03:55<14:09,  9.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562AD7B4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256262995E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562652BDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 123.\n",
      "Epoch 00173: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██████████████████▍                                                              | 25/110 [04:05<14:11, 10.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F4BEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256236F4558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E5A798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 144.\n",
      "Epoch 00194: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|███████████████████▏                                                             | 26/110 [04:16<14:35, 10.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025621089DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A02F318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4B8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "Epoch 00150: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|███████████████████▉                                                             | 27/110 [04:25<13:54, 10.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256236DA0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623AD8948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4BCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 85.\n",
      "Epoch 00135: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|████████████████████▌                                                            | 28/110 [04:34<13:03,  9.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A3CE9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A225948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025622198288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 117.\n",
      "Epoch 00167: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|█████████████████████▎                                                           | 29/110 [04:44<12:59,  9.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E55678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629E5A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4B168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 120.\n",
      "Epoch 00170: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██████████████████████                                                           | 30/110 [04:53<12:54,  9.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256236DA828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A059678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F4B0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 126.\n",
      "Epoch 00176: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██████████████████████▊                                                          | 31/110 [05:04<12:54,  9.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A039438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623C8C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F10828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 65.\n",
      "Epoch 00115: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|███████████████████████▌                                                         | 32/110 [05:11<11:49,  9.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.3, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620926948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629F10948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E55168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 00110: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|████████████████████████▎                                                        | 33/110 [05:18<10:57,  8.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A225DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562650CDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E555E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 147.\n",
      "Epoch 00197: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|█████████████████████████                                                        | 34/110 [05:29<11:45,  9.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F18E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A225798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620926828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 157.\n",
      "Epoch 00207: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|█████████████████████████▊                                                       | 35/110 [05:41<12:24,  9.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562650CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3C99D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A6F1798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 145.\n",
      "Epoch 00195: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|██████████████████████████▌                                                      | 36/110 [05:52<12:38, 10.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E550D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2B60D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025625076318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 139.\n",
      "Epoch 00189: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███████████████████████████▏                                                     | 37/110 [06:02<12:39, 10.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A3C9AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A0399D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562AD7B798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 109.\n",
      "Epoch 00159: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███████████████████████████▉                                                     | 38/110 [06:12<12:06, 10.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F4B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A857A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256210B7048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 102.\n",
      "Epoch 00152: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|████████████████████████████▋                                                    | 39/110 [06:21<11:35,  9.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A223948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ABDF828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 122.\n",
      "Epoch 00172: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|█████████████████████████████▍                                                   | 40/110 [06:31<11:31,  9.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562AD7BF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A64B4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A2235E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 92.\n",
      "Epoch 00142: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|██████████████████████████████▏                                                  | 41/110 [06:40<10:56,  9.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623AD81F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629F10708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 73.\n",
      "Epoch 00123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|██████████████████████████████▉                                                  | 42/110 [06:47<10:12,  9.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A64B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256251A03A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562AD7B828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 115.\n",
      "Epoch 00165: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███████████████████████████████▋                                                 | 43/110 [06:57<10:15,  9.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.4, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E5A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025622198708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620A93A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 00121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████████████████████████████████▍                                                | 44/110 [07:05<09:37,  8.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256210B7828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2B60D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C93A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 162.\n",
      "Epoch 00212: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|█████████████████████████████████▏                                               | 45/110 [07:16<10:25,  9.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E55678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627C10DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623C8C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 114.\n",
      "Epoch 00164: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|█████████████████████████████████▊                                               | 46/110 [07:26<10:14,  9.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A2B6798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624EDDDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623C8CC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 130.\n",
      "Epoch 00180: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|██████████████████████████████████▌                                              | 47/110 [07:36<10:16,  9.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256265FC948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562AAAC288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623C8C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 122.\n",
      "Epoch 00172: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|███████████████████████████████████▎                                             | 48/110 [07:46<10:10,  9.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025622198288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A64B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620B49C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 121.\n",
      "Epoch 00171: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████████████████████████████████████                                             | 49/110 [07:56<10:01,  9.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620B49828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025625076288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A223CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 111.\n",
      "Epoch 00161: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████████████████████████████████████▊                                            | 50/110 [08:06<09:45,  9.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562652BB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A27BE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A6F1A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 104.\n",
      "Epoch 00154: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|█████████████████████████████████████▌                                           | 51/110 [08:15<09:25,  9.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562650C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ABDE3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E55678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|██████████████████████████████████████▎                                          | 52/110 [08:23<08:49,  9.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623C8C678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256210B70D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562652BB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 53.\n",
      "Epoch 00103: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|███████████████████████████████████████                                          | 53/110 [08:30<08:05,  8.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ABDE168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623C8C318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|███████████████████████████████████████▊                                         | 54/110 [08:38<07:44,  8.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.5, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256210B7EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626482C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620B7D168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 73.\n",
      "Epoch 00123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|████████████████████████████████████████▌                                        | 55/110 [08:46<07:28,  8.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025622198828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627C10558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 144.\n",
      "Epoch 00194: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████████████████████████████████████████▏                                       | 56/110 [08:56<08:04,  8.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256264829D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620926708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 144.\n",
      "Epoch 00194: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████████████████████████████████████████▉                                       | 57/110 [09:07<08:25,  9.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620B6A5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562652BE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 137.\n",
      "Epoch 00187: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|██████████████████████████████████████████▋                                      | 58/110 [09:18<08:32,  9.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256210B75E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562674E318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209C4168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 106.\n",
      "Epoch 00156: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|███████████████████████████████████████████▍                                     | 59/110 [09:27<08:14,  9.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562638E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256209260D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620B490D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 133.\n",
      "Epoch 00183: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|████████████████████████████████████████████▏                                    | 60/110 [09:38<08:16,  9.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623C8C678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562378FA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209268B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 101.\n",
      "Epoch 00151: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|████████████████████████████████████████████▉                                    | 61/110 [09:47<07:54,  9.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A04C168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562652B9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256265FCB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 146.\n",
      "Epoch 00196: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████████████████████████████████████████████▋                                   | 62/110 [09:58<08:03, 10.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025622198C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ABDF828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209263A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 85.\n",
      "Epoch 00135: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|██████████████████████████████████████████████▍                                  | 63/110 [10:06<07:28,  9.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F4B828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3C9318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620B7D558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 79.\n",
      "Epoch 00129: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|███████████████████████████████████████████████▏                                 | 64/110 [10:14<06:59,  9.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256265FCB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624F438B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256250761F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 00116: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|███████████████████████████████████████████████▊                                 | 65/110 [10:22<06:30,  8.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A223C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025621089F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABDECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "Epoch 00150: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|████████████████████████████████████████████████▌                                | 66/110 [10:31<06:25,  8.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620B6ACA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ABDE678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620B49D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 95.\n",
      "Epoch 00145: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|█████████████████████████████████████████████████▎                               | 67/110 [10:40<06:18,  8.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562AD7B288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629E550D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562378FE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 149.\n",
      "Epoch 00199: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████████████████████████████████████████████████                               | 68/110 [10:51<06:39,  9.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E5A8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025621089678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C90D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 116.\n",
      "Epoch 00166: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████████████████████████████████████████████████▊                              | 69/110 [11:01<06:32,  9.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626397A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620B6A708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3CE708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 140.\n",
      "Epoch 00190: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|███████████████████████████████████████████████████▌                             | 70/110 [11:11<06:36,  9.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562378F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624EDDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620B7DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 99.\n",
      "Epoch 00149: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|████████████████████████████████████████████████████▎                            | 71/110 [11:20<06:14,  9.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623765048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A857D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A598CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 105.\n",
      "Epoch 00155: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|█████████████████████████████████████████████████████                            | 72/110 [11:29<06:00,  9.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A598948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623CFF0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256210B7948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|█████████████████████████████████████████████████████▊                           | 73/110 [11:37<05:32,  8.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620926708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620CABEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A04C318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 113.\n",
      "Epoch 00163: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████████████████████████████▍                          | 74/110 [11:47<05:29,  9.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623CFF8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256265FC9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209D7AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 87.\n",
      "Epoch 00137: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|███████████████████████████████████████████████████████▏                         | 75/110 [11:55<05:13,  8.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562378FE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256209EADC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C95E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 122.\n",
      "Epoch 00172: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|███████████████████████████████████████████████████████▉                         | 76/110 [12:06<05:18,  9.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620CAB3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A0399D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623CFF048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 96.\n",
      "Epoch 00146: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|████████████████████████████████████████████████████████▋                        | 77/110 [12:15<05:05,  9.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209D7E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025622198288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C98B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "Epoch 00147: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|█████████████████████████████████████████████████████████▍                       | 78/110 [12:24<04:54,  9.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256233975E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3C9798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626397288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 138.\n",
      "Epoch 00188: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|██████████████████████████████████████████████████████████▏                      | 79/110 [12:35<05:01,  9.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209C41F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256233300D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623CFF5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 134.\n",
      "Epoch 00184: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|██████████████████████████████████████████████████████████▉                      | 80/110 [12:45<04:57,  9.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A3C9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623AD8F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623CFFCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 139.\n",
      "Epoch 00189: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████████████████████████████████████████████████████████▋                     | 81/110 [12:56<04:54, 10.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F108B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3C9AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256263979D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 116.\n",
      "Epoch 00166: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|████████████████████████████████████████████████████████████▍                    | 82/110 [13:05<04:40, 10.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A598828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256233308B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209D7C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 00121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|█████████████████████████████████████████████████████████████                    | 83/110 [13:13<04:11,  9.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562378F948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256209D73A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623CFF4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 84.\n",
      "Epoch 00134: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|█████████████████████████████████████████████████████████████▊                   | 84/110 [13:21<03:55,  9.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209EA948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256210B7948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|██████████████████████████████████████████████████████████████▌                  | 85/110 [13:29<03:36,  8.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209D7168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A6F1678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 00088: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████████████████████████████████████████████████████████████▎                 | 86/110 [13:36<03:11,  7.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209EA288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256250C7D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A04C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 00116: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|████████████████████████████████████████████████████████████████                 | 87/110 [13:43<03:00,  7.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A6F1A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A225C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209D7CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 87.\n",
      "Epoch 00137: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████████████████████████████████████████████████████████████▊                | 88/110 [13:52<02:56,  8.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626482288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562378FAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626397EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 130.\n",
      "Epoch 00180: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|█████████████████████████████████████████████████████████████████▌               | 89/110 [14:02<03:02,  8.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209C48B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562674E318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A6F1678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 133.\n",
      "Epoch 00183: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|██████████████████████████████████████████████████████████████████▎              | 90/110 [14:12<03:05,  9.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562378FA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A02F558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626482E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 131.\n",
      "Epoch 00181: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|███████████████████████████████████████████████████████████████████              | 91/110 [14:23<03:02,  9.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A3C94C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A64B5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209C4D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 128.\n",
      "Epoch 00178: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|███████████████████████████████████████████████████████████████████▋             | 92/110 [14:33<02:56,  9.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209D70D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3C9DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209C41F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 99.\n",
      "Epoch 00149: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████████████████████████████████████████████████████████████████▍            | 93/110 [14:42<02:43,  9.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256251A03A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A1CF288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209C45E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 61.\n",
      "Epoch 00111: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|█████████████████████████████████████████████████████████████████████▏           | 94/110 [14:50<02:22,  8.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A223E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620CABF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209D7288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 85.\n",
      "Epoch 00135: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|█████████████████████████████████████████████████████████████████████▉           | 95/110 [14:58<02:11,  8.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A1CF3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629E55DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C95E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 00093: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|██████████████████████████████████████████████████████████████████████▋          | 96/110 [15:05<01:53,  8.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F4BC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A857438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629ED59D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 89.\n",
      "Epoch 00139: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|███████████████████████████████████████████████████████████████████████▍         | 97/110 [15:13<01:47,  8.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256210B7AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256233975E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256264824C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 00093: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%|████████████████████████████████████████████████████████████████████████▏        | 98/110 [15:20<01:33,  7.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626482F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620926B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F10798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Epoch 00090: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|████████████████████████████████████████████████████████████████████████▉        | 99/110 [15:26<01:20,  7.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A4D5D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A857C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629ED53A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 107.\n",
      "Epoch 00157: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 91%|████████████████████████████████████████████████████████████████████████▋       | 100/110 [15:36<01:19,  7.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 0.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A04C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629E55AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3C9798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 91.\n",
      "Epoch 00141: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████████████████████████████████████████████████████████████████████▍      | 101/110 [15:44<01:13,  8.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620CAB3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256250765E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629ED5B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 78.\n",
      "Epoch 00128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|██████████████████████████████████████████████████████████████████████████▏     | 102/110 [15:52<01:05,  8.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A3CE9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A039438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A04C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 125.\n",
      "Epoch 00175: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|██████████████████████████████████████████████████████████████████████████▉     | 103/110 [16:03<01:01,  8.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623CF6CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3CE798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623CFFB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "Epoch 00126: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|███████████████████████████████████████████████████████████████████████████▋    | 104/110 [16:11<00:51,  8.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623330828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626397558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623CF6E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Epoch 00095: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|████████████████████████████████████████████████████████████████████████████▎   | 105/110 [16:17<00:39,  8.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A068EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620B494C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025623CF6168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 00121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|█████████████████████████████████████████████████████████████████████████████   | 106/110 [16:25<00:31,  7.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 3.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F4BB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624F438B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4D5438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|█████████████████████████████████████████████████████████████████████████████▊  | 107/110 [16:33<00:23,  7.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620B49708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2B6558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256263971F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████▌ | 108/110 [16:39<00:14,  7.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 4.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A1CF288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3C94C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4D5D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Epoch 00074: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 99%|███████████████████████████████████████████████████████████████████████████████▎| 109/110 [16:45<00:06,  6.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 1, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626482A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A8570D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4D5A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 61.\n",
      "Epoch 00111: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [16:53<00:00,  9.21s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b6201bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/M/Predykcja klasy M - podejscie z Focal-loss (nowe)/050522211619.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49812ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stare_wart=df['val_fbeta_score']\n",
    "nowe_wart=[]\n",
    "for x in stare_wart:\n",
    "    wart=x.replace('[','')\n",
    "    wart=wart.replace(']','')\n",
    "    wart=float(wart)\n",
    "    nowe_wart.append(wart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc9a39a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22222222,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.27777776,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267,\n",
       " 0.26829267]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05cb4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['val_fbeta_score']=nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c70d0a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a72ba34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['round_epochs', 'loss', 'fbeta_score', 'precision', 'recall',\n",
       "       'val_loss', 'val_fbeta_score', 'val_precision', 'val_recall',\n",
       "       'activation_layer', 'activity_regularizer', 'alpha',\n",
       "       'batc_normalization', 'batch_size', 'bias_regularizer', 'dropout',\n",
       "       'epochs', 'first_neuron', 'gamma', 'hidden_layers', 'hidden_neuron',\n",
       "       'kernel_initializer', 'kernel_regularizer_l1', 'kernel_regularizer_l2',\n",
       "       'last_activation', 'lr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15aea9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>gamma</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>294</td>\n",
       "      <td>-27.039412</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.926886</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>306</td>\n",
       "      <td>-27.038244</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.926428</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>282</td>\n",
       "      <td>-27.036730</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.924812</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>319</td>\n",
       "      <td>-27.035040</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.923359</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>348</td>\n",
       "      <td>-27.035099</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.922380</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>280</td>\n",
       "      <td>-27.033897</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.921867</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>299</td>\n",
       "      <td>-27.032303</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.920349</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>209</td>\n",
       "      <td>-13.509267</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.453563</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>252</td>\n",
       "      <td>-13.509299</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.453326</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>241</td>\n",
       "      <td>-13.507066</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.451137</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>248</td>\n",
       "      <td>-13.506582</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.450884</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>220</td>\n",
       "      <td>-13.505239</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.448430</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>236</td>\n",
       "      <td>-13.508199</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.447373</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>227</td>\n",
       "      <td>-13.506232</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.447058</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005234</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>54</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>55</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>72</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>53</td>\n",
       "      <td>0.007351</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014177</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>0.017023</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>65</td>\n",
       "      <td>0.040688</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>0.050724</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61</td>\n",
       "      <td>0.138936</td>\n",
       "      <td>[0.30824372]</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182578</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0.187771</td>\n",
       "      <td>[0.2142857]</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.236437</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs       loss   fbeta_score  precision    recall   val_loss  \\\n",
       "31           294 -27.039412  [0.26299694]   0.151408  1.000000 -26.926886   \n",
       "28           306 -27.038244  [0.26299694]   0.151408  1.000000 -26.926428   \n",
       "30           282 -27.036730  [0.26299694]   0.151408  1.000000 -26.924812   \n",
       "32           319 -27.035040  [0.26299694]   0.151408  1.000000 -26.923359   \n",
       "34           348 -27.035099  [0.26299694]   0.151408  1.000000 -26.922380   \n",
       "33           280 -27.033897  [0.26299694]   0.151408  1.000000 -26.921867   \n",
       "29           299 -27.032303  [0.26299694]   0.151408  1.000000 -26.920349   \n",
       "24           209 -13.509267  [0.26299694]   0.151408  1.000000 -13.453563   \n",
       "26           252 -13.509299  [0.26299694]   0.151408  1.000000 -13.453326   \n",
       "27           241 -13.507066  [0.26299694]   0.151408  1.000000 -13.451137   \n",
       "25           248 -13.506582  [0.26299694]   0.151408  1.000000 -13.450884   \n",
       "22           220 -13.505239  [0.26299694]   0.151408  1.000000 -13.448430   \n",
       "23           236 -13.508199  [0.26299694]   0.151408  1.000000 -13.447373   \n",
       "21           227 -13.506232  [0.26299694]   0.151408  1.000000 -13.447058   \n",
       "5             65   0.003965          [0.]   0.000000  0.000000   0.003973   \n",
       "13            53   0.005155  [0.26299694]   0.151408  1.000000   0.005117   \n",
       "20            53   0.005195  [0.26299694]   0.151408  1.000000   0.005191   \n",
       "19            53   0.005214  [0.26299694]   0.151408  1.000000   0.005192   \n",
       "6             53   0.005226          [0.]   0.000000  0.000000   0.005195   \n",
       "17            53   0.005234  [0.26299694]   0.151408  1.000000   0.005215   \n",
       "18            54   0.005311  [0.26299694]   0.151408  1.000000   0.005296   \n",
       "12            53   0.005329  [0.26299694]   0.151408  1.000000   0.005307   \n",
       "16            53   0.005447  [0.26299694]   0.151408  1.000000   0.005444   \n",
       "15            55   0.005610  [0.26299694]   0.151408  1.000000   0.005600   \n",
       "11            53   0.005771  [0.26299694]   0.151408  1.000000   0.005772   \n",
       "4             53   0.005927          [0.]   0.000000  0.000000   0.005927   \n",
       "14            72   0.007053  [0.26299694]   0.151408  1.000000   0.007078   \n",
       "10            53   0.007351  [0.26299694]   0.151408  1.000000   0.007345   \n",
       "3             53   0.008249          [0.]   0.000000  0.000000   0.008240   \n",
       "9             60   0.014147  [0.26299694]   0.151408  1.000000   0.014177   \n",
       "2             55   0.017023          [0.]   0.000000  0.000000   0.017136   \n",
       "8             65   0.040688  [0.26299694]   0.151408  1.000000   0.041006   \n",
       "1             77   0.050724          [0.]   0.000000  0.000000   0.056485   \n",
       "7             61   0.138936  [0.30824372]   0.182203  1.000000   0.182578   \n",
       "0             73   0.187771   [0.2142857]   0.461538  0.139535   0.236437   \n",
       "\n",
       "    val_fbeta_score  val_precision  val_recall activation_layer  ...  epochs  \\\n",
       "31         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "28         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "30         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "32         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "34         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "33         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "29         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "24         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "26         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "27         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "25         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "22         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "23         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "21         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "5          0.000000       0.000000    0.000000             relu  ...  100000   \n",
       "13         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "20         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "19         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "6          0.000000       0.000000    0.000000             relu  ...  100000   \n",
       "17         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "18         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "12         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "16         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "15         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "11         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "4          0.000000       0.000000    0.000000             relu  ...  100000   \n",
       "14         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "10         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "3          0.000000       0.000000    0.000000             relu  ...  100000   \n",
       "9          0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "2          0.000000       0.000000    0.000000             relu  ...  100000   \n",
       "8          0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "1          0.000000       0.000000    0.000000             relu  ...  100000   \n",
       "7          0.277778       0.163934    0.909091             relu  ...  100000   \n",
       "0          0.222222       0.600000    0.136364             relu  ...  100000   \n",
       "\n",
       "    first_neuron  gamma  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "31           110      6              3            100          orthogonal   \n",
       "28           110      0              3            100          orthogonal   \n",
       "30           110      4              3            100          orthogonal   \n",
       "32           110      8              3            100          orthogonal   \n",
       "34           110     12              3            100          orthogonal   \n",
       "33           110     10              3            100          orthogonal   \n",
       "29           110      2              3            100          orthogonal   \n",
       "24           110      6              3            100          orthogonal   \n",
       "26           110     10              3            100          orthogonal   \n",
       "27           110     12              3            100          orthogonal   \n",
       "25           110      8              3            100          orthogonal   \n",
       "22           110      2              3            100          orthogonal   \n",
       "23           110      4              3            100          orthogonal   \n",
       "21           110      0              3            100          orthogonal   \n",
       "5            110     10              3            100          orthogonal   \n",
       "13           110     12              3            100          orthogonal   \n",
       "20           110     12              3            100          orthogonal   \n",
       "19           110     10              3            100          orthogonal   \n",
       "6            110     12              3            100          orthogonal   \n",
       "17           110      6              3            100          orthogonal   \n",
       "18           110      8              3            100          orthogonal   \n",
       "12           110     10              3            100          orthogonal   \n",
       "16           110      4              3            100          orthogonal   \n",
       "15           110      2              3            100          orthogonal   \n",
       "11           110      8              3            100          orthogonal   \n",
       "4            110      8              3            100          orthogonal   \n",
       "14           110      0              3            100          orthogonal   \n",
       "10           110      6              3            100          orthogonal   \n",
       "3            110      6              3            100          orthogonal   \n",
       "9            110      4              3            100          orthogonal   \n",
       "2            110      4              3            100          orthogonal   \n",
       "8            110      2              3            100          orthogonal   \n",
       "1            110      2              3            100          orthogonal   \n",
       "7            110      0              3            100          orthogonal   \n",
       "0            110      0              3            100          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation    lr  \n",
       "31                 0.0001                 0.0001          sigmoid  0.01  \n",
       "28                 0.0001                 0.0001          sigmoid  0.01  \n",
       "30                 0.0001                 0.0001          sigmoid  0.01  \n",
       "32                 0.0001                 0.0001          sigmoid  0.01  \n",
       "34                 0.0001                 0.0001          sigmoid  0.01  \n",
       "33                 0.0001                 0.0001          sigmoid  0.01  \n",
       "29                 0.0001                 0.0001          sigmoid  0.01  \n",
       "24                 0.0001                 0.0001          sigmoid  0.01  \n",
       "26                 0.0001                 0.0001          sigmoid  0.01  \n",
       "27                 0.0001                 0.0001          sigmoid  0.01  \n",
       "25                 0.0001                 0.0001          sigmoid  0.01  \n",
       "22                 0.0001                 0.0001          sigmoid  0.01  \n",
       "23                 0.0001                 0.0001          sigmoid  0.01  \n",
       "21                 0.0001                 0.0001          sigmoid  0.01  \n",
       "5                  0.0001                 0.0001          sigmoid  0.01  \n",
       "13                 0.0001                 0.0001          sigmoid  0.01  \n",
       "20                 0.0001                 0.0001          sigmoid  0.01  \n",
       "19                 0.0001                 0.0001          sigmoid  0.01  \n",
       "6                  0.0001                 0.0001          sigmoid  0.01  \n",
       "17                 0.0001                 0.0001          sigmoid  0.01  \n",
       "18                 0.0001                 0.0001          sigmoid  0.01  \n",
       "12                 0.0001                 0.0001          sigmoid  0.01  \n",
       "16                 0.0001                 0.0001          sigmoid  0.01  \n",
       "15                 0.0001                 0.0001          sigmoid  0.01  \n",
       "11                 0.0001                 0.0001          sigmoid  0.01  \n",
       "4                  0.0001                 0.0001          sigmoid  0.01  \n",
       "14                 0.0001                 0.0001          sigmoid  0.01  \n",
       "10                 0.0001                 0.0001          sigmoid  0.01  \n",
       "3                  0.0001                 0.0001          sigmoid  0.01  \n",
       "9                  0.0001                 0.0001          sigmoid  0.01  \n",
       "2                  0.0001                 0.0001          sigmoid  0.01  \n",
       "8                  0.0001                 0.0001          sigmoid  0.01  \n",
       "1                  0.0001                 0.0001          sigmoid  0.01  \n",
       "7                  0.0001                 0.0001          sigmoid  0.01  \n",
       "0                  0.0001                 0.0001          sigmoid  0.01  \n",
       "\n",
       "[35 rows x 26 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a59f82db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df.reindex(columns=['val_fbeta_score','loss', 'fbeta_score',\n",
    "       'precision', 'recall', 'val_loss', \n",
    "      'val_precision',\n",
    "       'val_recall', 'gamma','alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5be1d39f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.039412</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.926886</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.038244</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.926428</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.036730</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.924812</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.035040</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.923359</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.035099</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.922380</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.033897</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.921867</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.032303</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.920349</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.509267</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.453563</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.509299</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.453326</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.507066</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.451137</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.506582</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.450884</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.505239</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.448430</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.508199</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.447373</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.506232</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.447058</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005234</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.007351</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014177</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017023</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.040688</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050724</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.138936</td>\n",
       "      <td>[0.30824372]</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182578</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.187771</td>\n",
       "      <td>[0.2142857]</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.236437</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_fbeta_score       loss   fbeta_score  precision    recall   val_loss  \\\n",
       "31         0.268293 -27.039412  [0.26299694]   0.151408  1.000000 -26.926886   \n",
       "28         0.268293 -27.038244  [0.26299694]   0.151408  1.000000 -26.926428   \n",
       "30         0.268293 -27.036730  [0.26299694]   0.151408  1.000000 -26.924812   \n",
       "32         0.268293 -27.035040  [0.26299694]   0.151408  1.000000 -26.923359   \n",
       "34         0.268293 -27.035099  [0.26299694]   0.151408  1.000000 -26.922380   \n",
       "33         0.268293 -27.033897  [0.26299694]   0.151408  1.000000 -26.921867   \n",
       "29         0.268293 -27.032303  [0.26299694]   0.151408  1.000000 -26.920349   \n",
       "24         0.268293 -13.509267  [0.26299694]   0.151408  1.000000 -13.453563   \n",
       "26         0.268293 -13.509299  [0.26299694]   0.151408  1.000000 -13.453326   \n",
       "27         0.268293 -13.507066  [0.26299694]   0.151408  1.000000 -13.451137   \n",
       "25         0.268293 -13.506582  [0.26299694]   0.151408  1.000000 -13.450884   \n",
       "22         0.268293 -13.505239  [0.26299694]   0.151408  1.000000 -13.448430   \n",
       "23         0.268293 -13.508199  [0.26299694]   0.151408  1.000000 -13.447373   \n",
       "21         0.268293 -13.506232  [0.26299694]   0.151408  1.000000 -13.447058   \n",
       "5          0.000000   0.003965          [0.]   0.000000  0.000000   0.003973   \n",
       "13         0.268293   0.005155  [0.26299694]   0.151408  1.000000   0.005117   \n",
       "20         0.268293   0.005195  [0.26299694]   0.151408  1.000000   0.005191   \n",
       "19         0.268293   0.005214  [0.26299694]   0.151408  1.000000   0.005192   \n",
       "6          0.000000   0.005226          [0.]   0.000000  0.000000   0.005195   \n",
       "17         0.268293   0.005234  [0.26299694]   0.151408  1.000000   0.005215   \n",
       "18         0.268293   0.005311  [0.26299694]   0.151408  1.000000   0.005296   \n",
       "12         0.268293   0.005329  [0.26299694]   0.151408  1.000000   0.005307   \n",
       "16         0.268293   0.005447  [0.26299694]   0.151408  1.000000   0.005444   \n",
       "15         0.268293   0.005610  [0.26299694]   0.151408  1.000000   0.005600   \n",
       "11         0.268293   0.005771  [0.26299694]   0.151408  1.000000   0.005772   \n",
       "4          0.000000   0.005927          [0.]   0.000000  0.000000   0.005927   \n",
       "14         0.268293   0.007053  [0.26299694]   0.151408  1.000000   0.007078   \n",
       "10         0.268293   0.007351  [0.26299694]   0.151408  1.000000   0.007345   \n",
       "3          0.000000   0.008249          [0.]   0.000000  0.000000   0.008240   \n",
       "9          0.268293   0.014147  [0.26299694]   0.151408  1.000000   0.014177   \n",
       "2          0.000000   0.017023          [0.]   0.000000  0.000000   0.017136   \n",
       "8          0.268293   0.040688  [0.26299694]   0.151408  1.000000   0.041006   \n",
       "1          0.000000   0.050724          [0.]   0.000000  0.000000   0.056485   \n",
       "7          0.277778   0.138936  [0.30824372]   0.182203  1.000000   0.182578   \n",
       "0          0.222222   0.187771   [0.2142857]   0.461538  0.139535   0.236437   \n",
       "\n",
       "    val_precision  val_recall  gamma  alpha  \n",
       "31       0.154930    1.000000      6    3.0  \n",
       "28       0.154930    1.000000      0    3.0  \n",
       "30       0.154930    1.000000      4    3.0  \n",
       "32       0.154930    1.000000      8    3.0  \n",
       "34       0.154930    1.000000     12    3.0  \n",
       "33       0.154930    1.000000     10    3.0  \n",
       "29       0.154930    1.000000      2    3.0  \n",
       "24       0.154930    1.000000      6    2.0  \n",
       "26       0.154930    1.000000     10    2.0  \n",
       "27       0.154930    1.000000     12    2.0  \n",
       "25       0.154930    1.000000      8    2.0  \n",
       "22       0.154930    1.000000      2    2.0  \n",
       "23       0.154930    1.000000      4    2.0  \n",
       "21       0.154930    1.000000      0    2.0  \n",
       "5        0.000000    0.000000     10    0.8  \n",
       "13       0.154930    1.000000     12    0.9  \n",
       "20       0.154930    1.000000     12    1.0  \n",
       "19       0.154930    1.000000     10    1.0  \n",
       "6        0.000000    0.000000     12    0.8  \n",
       "17       0.154930    1.000000      6    1.0  \n",
       "18       0.154930    1.000000      8    1.0  \n",
       "12       0.154930    1.000000     10    0.9  \n",
       "16       0.154930    1.000000      4    1.0  \n",
       "15       0.154930    1.000000      2    1.0  \n",
       "11       0.154930    1.000000      8    0.9  \n",
       "4        0.000000    0.000000      8    0.8  \n",
       "14       0.154930    1.000000      0    1.0  \n",
       "10       0.154930    1.000000      6    0.9  \n",
       "3        0.000000    0.000000      6    0.8  \n",
       "9        0.154930    1.000000      4    0.9  \n",
       "2        0.000000    0.000000      4    0.8  \n",
       "8        0.154930    1.000000      2    0.9  \n",
       "1        0.000000    0.000000      2    0.8  \n",
       "7        0.163934    0.909091      0    0.9  \n",
       "0        0.600000    0.136364      0    0.8  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2365cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['val_fbeta_score']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "91335ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.039412</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.926886</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.038244</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.926428</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.036730</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.924812</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.035040</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.923359</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.035099</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.922380</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.033897</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.921867</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-27.032303</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.920349</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.509267</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.453563</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.509299</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.453326</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.507066</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.451137</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.506582</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.450884</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.505239</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.448430</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.508199</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.447373</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>-13.506232</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-13.447058</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005234</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.007351</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014177</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.040688</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.138936</td>\n",
       "      <td>[0.30824372]</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182578</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.187771</td>\n",
       "      <td>[0.2142857]</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.236437</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_fbeta_score       loss   fbeta_score  precision    recall   val_loss  \\\n",
       "31         0.268293 -27.039412  [0.26299694]   0.151408  1.000000 -26.926886   \n",
       "28         0.268293 -27.038244  [0.26299694]   0.151408  1.000000 -26.926428   \n",
       "30         0.268293 -27.036730  [0.26299694]   0.151408  1.000000 -26.924812   \n",
       "32         0.268293 -27.035040  [0.26299694]   0.151408  1.000000 -26.923359   \n",
       "34         0.268293 -27.035099  [0.26299694]   0.151408  1.000000 -26.922380   \n",
       "33         0.268293 -27.033897  [0.26299694]   0.151408  1.000000 -26.921867   \n",
       "29         0.268293 -27.032303  [0.26299694]   0.151408  1.000000 -26.920349   \n",
       "24         0.268293 -13.509267  [0.26299694]   0.151408  1.000000 -13.453563   \n",
       "26         0.268293 -13.509299  [0.26299694]   0.151408  1.000000 -13.453326   \n",
       "27         0.268293 -13.507066  [0.26299694]   0.151408  1.000000 -13.451137   \n",
       "25         0.268293 -13.506582  [0.26299694]   0.151408  1.000000 -13.450884   \n",
       "22         0.268293 -13.505239  [0.26299694]   0.151408  1.000000 -13.448430   \n",
       "23         0.268293 -13.508199  [0.26299694]   0.151408  1.000000 -13.447373   \n",
       "21         0.268293 -13.506232  [0.26299694]   0.151408  1.000000 -13.447058   \n",
       "13         0.268293   0.005155  [0.26299694]   0.151408  1.000000   0.005117   \n",
       "20         0.268293   0.005195  [0.26299694]   0.151408  1.000000   0.005191   \n",
       "19         0.268293   0.005214  [0.26299694]   0.151408  1.000000   0.005192   \n",
       "17         0.268293   0.005234  [0.26299694]   0.151408  1.000000   0.005215   \n",
       "18         0.268293   0.005311  [0.26299694]   0.151408  1.000000   0.005296   \n",
       "12         0.268293   0.005329  [0.26299694]   0.151408  1.000000   0.005307   \n",
       "16         0.268293   0.005447  [0.26299694]   0.151408  1.000000   0.005444   \n",
       "15         0.268293   0.005610  [0.26299694]   0.151408  1.000000   0.005600   \n",
       "11         0.268293   0.005771  [0.26299694]   0.151408  1.000000   0.005772   \n",
       "14         0.268293   0.007053  [0.26299694]   0.151408  1.000000   0.007078   \n",
       "10         0.268293   0.007351  [0.26299694]   0.151408  1.000000   0.007345   \n",
       "9          0.268293   0.014147  [0.26299694]   0.151408  1.000000   0.014177   \n",
       "8          0.268293   0.040688  [0.26299694]   0.151408  1.000000   0.041006   \n",
       "7          0.277778   0.138936  [0.30824372]   0.182203  1.000000   0.182578   \n",
       "0          0.222222   0.187771   [0.2142857]   0.461538  0.139535   0.236437   \n",
       "\n",
       "    val_precision  val_recall  gamma  alpha  \n",
       "31       0.154930    1.000000      6    3.0  \n",
       "28       0.154930    1.000000      0    3.0  \n",
       "30       0.154930    1.000000      4    3.0  \n",
       "32       0.154930    1.000000      8    3.0  \n",
       "34       0.154930    1.000000     12    3.0  \n",
       "33       0.154930    1.000000     10    3.0  \n",
       "29       0.154930    1.000000      2    3.0  \n",
       "24       0.154930    1.000000      6    2.0  \n",
       "26       0.154930    1.000000     10    2.0  \n",
       "27       0.154930    1.000000     12    2.0  \n",
       "25       0.154930    1.000000      8    2.0  \n",
       "22       0.154930    1.000000      2    2.0  \n",
       "23       0.154930    1.000000      4    2.0  \n",
       "21       0.154930    1.000000      0    2.0  \n",
       "13       0.154930    1.000000     12    0.9  \n",
       "20       0.154930    1.000000     12    1.0  \n",
       "19       0.154930    1.000000     10    1.0  \n",
       "17       0.154930    1.000000      6    1.0  \n",
       "18       0.154930    1.000000      8    1.0  \n",
       "12       0.154930    1.000000     10    0.9  \n",
       "16       0.154930    1.000000      4    1.0  \n",
       "15       0.154930    1.000000      2    1.0  \n",
       "11       0.154930    1.000000      8    0.9  \n",
       "14       0.154930    1.000000      0    1.0  \n",
       "10       0.154930    1.000000      6    0.9  \n",
       "9        0.154930    1.000000      4    0.9  \n",
       "8        0.154930    1.000000      2    0.9  \n",
       "7        0.163934    0.909091      0    0.9  \n",
       "0        0.600000    0.136364      0    0.8  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590f787",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4d095f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/M/Predykcja klasy M - podejscie z Focal-loss (nowe)/050522215522.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a85030e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stare_wart=df['val_fbeta_score']\n",
    "nowe_wart=[]\n",
    "for x in stare_wart:\n",
    "    wart=x.replace('[','')\n",
    "    wart=wart.replace(']','')\n",
    "    wart=float(wart)\n",
    "    nowe_wart.append(wart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "85a91dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['val_fbeta_score']=nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ba3a186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b851d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.009852</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013962</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>[0.30442476]</td>\n",
       "      <td>0.179541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.039148</td>\n",
       "      <td>[0.30879712]</td>\n",
       "      <td>0.182590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044532</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.050561</td>\n",
       "      <td>[0.2882883]</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.057813</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.058559</td>\n",
       "      <td>[0.12903227]</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.062913</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.052713</td>\n",
       "      <td>[0.3099099]</td>\n",
       "      <td>0.183369</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065896</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.068684</td>\n",
       "      <td>[0.32203388]</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.084143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.080819</td>\n",
       "      <td>[0.13043478]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.091931</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>[0.185567]</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.096040</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.084575</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.099681</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.073628</td>\n",
       "      <td>[0.30852994]</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.103561</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.100749</td>\n",
       "      <td>[0.13043478]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.113805</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.096534</td>\n",
       "      <td>[0.2982456]</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.197674</td>\n",
       "      <td>0.116812</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.100924</td>\n",
       "      <td>[0.3113553]</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.120481</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.111448</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.127735</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.111559</td>\n",
       "      <td>[0.22000001]</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.139918</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.112993</td>\n",
       "      <td>[0.20833333]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.154853</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.139186</td>\n",
       "      <td>[0.38333333]</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.161941</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.138017</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.175342</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.140976</td>\n",
       "      <td>[0.31365314]</td>\n",
       "      <td>0.186404</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.190441</td>\n",
       "      <td>0.168224</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.154647</td>\n",
       "      <td>[0.17021276]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.191870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.155029</td>\n",
       "      <td>[0.26666665]</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.209560</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.151576</td>\n",
       "      <td>[0.35200003]</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.219989</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.197131</td>\n",
       "      <td>[0.42152467]</td>\n",
       "      <td>0.343066</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.248677</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.207913</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.259258</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.220536</td>\n",
       "      <td>[0.2962963]</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.266233</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.213429</td>\n",
       "      <td>[0.29126212]</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.276685</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     val_fbeta_score      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "109         0.268293  0.001101  [0.26299694]   0.151408  1.000000  0.001101   \n",
       "108         0.268293  0.001361  [0.26299694]   0.151408  1.000000  0.001361   \n",
       "107         0.268293  0.001398  [0.26299694]   0.151408  1.000000  0.001395   \n",
       "106         0.268293  0.001443  [0.26299694]   0.151408  1.000000  0.001444   \n",
       "103         0.268293  0.002017  [0.26299694]   0.151408  1.000000  0.002019   \n",
       "104         0.268293  0.002037  [0.26299694]   0.151408  1.000000  0.002028   \n",
       "102         0.268293  0.002070  [0.26299694]   0.151408  1.000000  0.002071   \n",
       "105         0.268293  0.002143  [0.26299694]   0.151408  1.000000  0.002133   \n",
       "100         0.268293  0.003228  [0.26299694]   0.151408  1.000000  0.003222   \n",
       "101         0.268293  0.003385  [0.26299694]   0.151408  1.000000  0.003366   \n",
       "99          0.268293  0.004976  [0.26299694]   0.151408  1.000000  0.004969   \n",
       "98          0.268293  0.005226  [0.26299694]   0.151408  1.000000  0.005266   \n",
       "97          0.268293  0.007050  [0.26299694]   0.151408  1.000000  0.007111   \n",
       "96          0.268293  0.009852  [0.26299694]   0.151408  1.000000  0.009934   \n",
       "95          0.268293  0.013850  [0.26299694]   0.151408  1.000000  0.013962   \n",
       "94          0.268293  0.019561  [0.26299694]   0.151408  1.000000  0.019712   \n",
       "93          0.295775  0.027910  [0.30442476]   0.179541  1.000000  0.029968   \n",
       "92          0.285714  0.039148  [0.30879712]   0.182590  1.000000  0.044532   \n",
       "81          0.296296  0.050561   [0.2882883]   0.640000  0.186047  0.057813   \n",
       "70          0.160000  0.058559  [0.12903227]   0.857143  0.069767  0.062913   \n",
       "91          0.285714  0.052713   [0.3099099]   0.183369  1.000000  0.065896   \n",
       "80          0.214286  0.068684  [0.32203388]   0.593750  0.220930  0.084143   \n",
       "47          0.083333  0.080819  [0.13043478]   1.000000  0.069767  0.091931   \n",
       "69          0.230769  0.077419    [0.185567]   0.818182  0.104651  0.096040   \n",
       "58          0.160000  0.084575  [0.15053762]   1.000000  0.081395  0.099681   \n",
       "90          0.283688  0.073628  [0.30852994]   0.182796  0.988372  0.103561   \n",
       "35          0.083333  0.100749  [0.13043478]   1.000000  0.069767  0.113805   \n",
       "79          0.275862  0.096534   [0.2982456]   0.607143  0.197674  0.116812   \n",
       "89          0.289855  0.100924   [0.3113553]   0.184783  0.988372  0.120481   \n",
       "46          0.160000  0.111448  [0.17021276]   1.000000  0.093023  0.127735   \n",
       "68          0.206897  0.111559  [0.22000001]   0.785714  0.127907  0.139918   \n",
       "57          0.142857  0.112993  [0.20833333]   1.000000  0.116279  0.154853   \n",
       "78          0.187500  0.139186  [0.38333333]   0.298701  0.534884  0.161941   \n",
       "34          0.148148  0.138017  [0.17021276]   1.000000  0.093023  0.175342   \n",
       "88          0.279070  0.140976  [0.31365314]   0.186404  0.988372  0.190441   \n",
       "45          0.153846  0.154647  [0.17021276]   1.000000  0.093023  0.191870   \n",
       "56          0.148148  0.155029  [0.26666665]   0.736842  0.162791  0.209560   \n",
       "67          0.193548  0.151576  [0.35200003]   0.564103  0.255814  0.219989   \n",
       "77          0.214286  0.197131  [0.42152467]   0.343066  0.546512  0.248677   \n",
       "44          0.153846  0.207913  [0.25742576]   0.866667  0.151163  0.259258   \n",
       "66          0.200000  0.220536   [0.2962963]   0.727273  0.186047  0.266233   \n",
       "55          0.142857  0.213429  [0.29126212]   0.882353  0.174419  0.276685   \n",
       "\n",
       "     val_precision  val_recall  gamma  alpha  \n",
       "109       0.154930    1.000000    5.0    1.0  \n",
       "108       0.154930    1.000000    4.5    1.0  \n",
       "107       0.154930    1.000000    4.0    1.0  \n",
       "106       0.154930    1.000000    3.5    1.0  \n",
       "103       0.154930    1.000000    2.0    1.0  \n",
       "104       0.154930    1.000000    2.5    1.0  \n",
       "102       0.154930    1.000000    1.5    1.0  \n",
       "105       0.154930    1.000000    3.0    1.0  \n",
       "100       0.154930    1.000000    0.5    1.0  \n",
       "101       0.154930    1.000000    1.0    1.0  \n",
       "99        0.154930    1.000000    0.0    1.0  \n",
       "98        0.154930    1.000000    5.0    0.9  \n",
       "97        0.154930    1.000000    4.5    0.9  \n",
       "96        0.154930    1.000000    4.0    0.9  \n",
       "95        0.154930    1.000000    3.5    0.9  \n",
       "94        0.154930    1.000000    3.0    0.9  \n",
       "93        0.175000    0.954545    2.5    0.9  \n",
       "92        0.169492    0.909091    2.0    0.9  \n",
       "81        0.800000    0.181818    2.0    0.8  \n",
       "70        0.666667    0.090909    2.0    0.7  \n",
       "91        0.169492    0.909091    1.5    0.9  \n",
       "80        0.500000    0.136364    1.5    0.8  \n",
       "47        0.500000    0.045455    1.5    0.5  \n",
       "69        0.750000    0.136364    1.5    0.7  \n",
       "58        0.666667    0.090909    1.5    0.6  \n",
       "90        0.168067    0.909091    1.0    0.9  \n",
       "35        0.500000    0.045455    1.0    0.4  \n",
       "79        0.571429    0.181818    1.0    0.8  \n",
       "89        0.172414    0.909091    0.5    0.9  \n",
       "46        0.666667    0.090909    1.0    0.5  \n",
       "68        0.428571    0.136364    1.0    0.7  \n",
       "57        0.333333    0.090909    1.0    0.6  \n",
       "78        0.300000    0.136364    0.5    0.8  \n",
       "34        0.400000    0.090909    0.5    0.4  \n",
       "88        0.168224    0.818182    0.0    0.9  \n",
       "45        0.500000    0.090909    0.5    0.5  \n",
       "56        0.400000    0.090909    0.5    0.6  \n",
       "67        0.333333    0.136364    0.5    0.7  \n",
       "77        0.500000    0.136364    0.0    0.8  \n",
       "44        0.500000    0.090909    0.0    0.5  \n",
       "66        0.375000    0.136364    0.0    0.7  \n",
       "55        0.333333    0.090909    0.0    0.6  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "62ef1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reindex(columns=['val_fbeta_score', 'fbeta_score',\n",
    "       'precision', 'recall', 'val_loss', 'loss',\n",
    "      'val_precision',\n",
    "       'val_recall', 'gamma','alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "613bca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['val_fbeta_score']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dc85fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['recall']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dfd030d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['precision']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "94417ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>[0.29126212]</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.276685</td>\n",
       "      <td>0.213429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>[0.2962963]</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.266233</td>\n",
       "      <td>0.220536</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>[0.25742576]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>0.259258</td>\n",
       "      <td>0.207913</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>[0.42152467]</td>\n",
       "      <td>0.343066</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.248677</td>\n",
       "      <td>0.197131</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.193548</td>\n",
       "      <td>[0.35200003]</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.219989</td>\n",
       "      <td>0.151576</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.148148</td>\n",
       "      <td>[0.26666665]</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.209560</td>\n",
       "      <td>0.155029</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.279070</td>\n",
       "      <td>[0.31365314]</td>\n",
       "      <td>0.186404</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.190441</td>\n",
       "      <td>0.140976</td>\n",
       "      <td>0.168224</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>[0.38333333]</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.161941</td>\n",
       "      <td>0.139186</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.206897</td>\n",
       "      <td>[0.22000001]</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.139918</td>\n",
       "      <td>0.111559</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.289855</td>\n",
       "      <td>[0.3113553]</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.120481</td>\n",
       "      <td>0.100924</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.275862</td>\n",
       "      <td>[0.2982456]</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.197674</td>\n",
       "      <td>0.116812</td>\n",
       "      <td>0.096534</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.283688</td>\n",
       "      <td>[0.30852994]</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.103561</td>\n",
       "      <td>0.073628</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>[0.185567]</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.096040</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>[0.32203388]</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.084143</td>\n",
       "      <td>0.068684</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.160000</td>\n",
       "      <td>[0.12903227]</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.062913</td>\n",
       "      <td>0.058559</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.296296</td>\n",
       "      <td>[0.2882883]</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.057813</td>\n",
       "      <td>0.050561</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_fbeta_score   fbeta_score  precision    recall  val_loss      loss  \\\n",
       "55         0.142857  [0.29126212]   0.882353  0.174419  0.276685  0.213429   \n",
       "66         0.200000   [0.2962963]   0.727273  0.186047  0.266233  0.220536   \n",
       "44         0.153846  [0.25742576]   0.866667  0.151163  0.259258  0.207913   \n",
       "77         0.214286  [0.42152467]   0.343066  0.546512  0.248677  0.197131   \n",
       "67         0.193548  [0.35200003]   0.564103  0.255814  0.219989  0.151576   \n",
       "56         0.148148  [0.26666665]   0.736842  0.162791  0.209560  0.155029   \n",
       "88         0.279070  [0.31365314]   0.186404  0.988372  0.190441  0.140976   \n",
       "78         0.187500  [0.38333333]   0.298701  0.534884  0.161941  0.139186   \n",
       "68         0.206897  [0.22000001]   0.785714  0.127907  0.139918  0.111559   \n",
       "89         0.289855   [0.3113553]   0.184783  0.988372  0.120481  0.100924   \n",
       "79         0.275862   [0.2982456]   0.607143  0.197674  0.116812  0.096534   \n",
       "90         0.283688  [0.30852994]   0.182796  0.988372  0.103561  0.073628   \n",
       "69         0.230769    [0.185567]   0.818182  0.104651  0.096040  0.077419   \n",
       "80         0.214286  [0.32203388]   0.593750  0.220930  0.084143  0.068684   \n",
       "70         0.160000  [0.12903227]   0.857143  0.069767  0.062913  0.058559   \n",
       "81         0.296296   [0.2882883]   0.640000  0.186047  0.057813  0.050561   \n",
       "\n",
       "    val_precision  val_recall  gamma  alpha  \n",
       "55       0.333333    0.090909    0.0    0.6  \n",
       "66       0.375000    0.136364    0.0    0.7  \n",
       "44       0.500000    0.090909    0.0    0.5  \n",
       "77       0.500000    0.136364    0.0    0.8  \n",
       "67       0.333333    0.136364    0.5    0.7  \n",
       "56       0.400000    0.090909    0.5    0.6  \n",
       "88       0.168224    0.818182    0.0    0.9  \n",
       "78       0.300000    0.136364    0.5    0.8  \n",
       "68       0.428571    0.136364    1.0    0.7  \n",
       "89       0.172414    0.909091    0.5    0.9  \n",
       "79       0.571429    0.181818    1.0    0.8  \n",
       "90       0.168067    0.909091    1.0    0.9  \n",
       "69       0.750000    0.136364    1.5    0.7  \n",
       "80       0.500000    0.136364    1.5    0.8  \n",
       "70       0.666667    0.090909    2.0    0.7  \n",
       "81       0.800000    0.181818    2.0    0.8  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18904521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## part 3\n",
    "alpha 0.9 - 0.6\n",
    "gamma!=0, powyzej 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4a343f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[110], \n",
    "     'hidden_neuron':[100],\n",
    "\n",
    "     'hidden_layers':[3],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [32],\n",
    "\n",
    "    'lr':[0.01],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "#    'batc_normalization':[False,True],\n",
    "#    'dropout': [0,0.2,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "    #'optimizer': ['rmsprop','adam','adadelta','adamax','nadam','adagrad'],\n",
    "    #'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "    'kernel_initializer': ['orthogonal'],\n",
    "    #'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    "    'activation_layer':['relu'],\n",
    "    #'batc_normalization':[False,True]\n",
    "    'batc_normalization':[False],\n",
    "    'alpha':[0.6,0.65,0.7,0.75,0.8,0.85,0.9],\n",
    "    'gamma':[1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2]\n",
    "    \n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "94a7a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss=[binary_focal_loss(alpha=params['alpha'], gamma=params['gamma'])],\n",
    "                  optimizer=tf.keras.optimizers.Adamax(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=1.0, threshold=0.5),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0e958e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/77 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E3CA3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562AD885E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562BF43DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Epoch 00080: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████                                                                      | 2/25 [1:41:19<19:25:10, 3039.61s/it]\n",
      "\n",
      "  1%|█                                                                                  | 1/77 [00:06<07:54,  6.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025624F430D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562BF43288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620F66798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|██▏                                                                                | 2/77 [00:11<07:23,  5.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626409DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620B49708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025625022708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Epoch 00078: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|███▏                                                                               | 3/77 [00:17<07:20,  5.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A3CE828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A04CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629FE0048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|████▎                                                                              | 4/77 [00:24<07:39,  6.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A6F1678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256221981F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629FE0168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|█████▍                                                                             | 5/77 [00:30<07:26,  6.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025624E96168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562216A4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256221DBAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▍                                                                            | 6/77 [00:36<07:15,  6.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.6, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256221DB4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025625022C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629ED8438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Epoch 00073: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|███████▌                                                                           | 7/77 [00:42<06:58,  5.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E3CAAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256261EA048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620E569D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|████████▌                                                                          | 8/77 [00:47<06:39,  5.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.8, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562216A4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ABDE558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629FE03A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█████████▋                                                                         | 9/77 [00:53<06:26,  5.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.9, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025624E96E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562216A828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620F595E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Epoch 00083: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|██████████▋                                                                       | 10/77 [00:59<06:29,  5.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.6, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626606D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256232E3798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Epoch 00073: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|███████████▋                                                                      | 11/77 [01:05<06:20,  5.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D9D8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627B0D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256211110D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Epoch 00091: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|████████████▊                                                                     | 12/77 [01:11<06:28,  5.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E55168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256237AD048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A04CE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Epoch 00073: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█████████████▊                                                                    | 13/77 [01:17<06:16,  5.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025624E96CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627A62C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9D168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 00075: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|██████████████▉                                                                   | 14/77 [01:22<06:07,  5.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626409288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620D9D318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209D79D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Epoch 00072: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|███████████████▉                                                                  | 15/77 [01:28<05:57,  5.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256261EA5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256262990D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626409E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Epoch 00069: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|█████████████████                                                                 | 16/77 [01:33<05:46,  5.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D9D948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562216A828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626409B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 00084: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██████████████████                                                                | 17/77 [01:40<05:48,  5.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.6, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E56A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256232E38B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4D5E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|███████████████████▏                                                              | 18/77 [01:45<05:44,  5.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256232EFB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562650C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256264098B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 00093: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|████████████████████▏                                                             | 19/77 [01:52<05:51,  6.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.8, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562633D8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626409558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9D828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Epoch 00092: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|█████████████████████▎                                                            | 20/77 [01:59<05:53,  6.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.9, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620F595E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629E5A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9D318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██████████████████████▎                                                           | 21/77 [02:04<05:34,  5.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.65, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620E561F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620F59828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9DAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|███████████████████████▍                                                          | 22/77 [02:09<05:18,  5.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562BF43708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562220C438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025625022E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 00086: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|████████████████████████▍                                                         | 23/77 [02:16<05:19,  5.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562674E948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629E55EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626409678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|█████████████████████████▌                                                        | 24/77 [02:22<05:13,  5.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E5A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A587DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620926828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 00086: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|██████████████████████████▌                                                       | 25/77 [02:28<05:12,  6.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025625022B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562781FAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626409678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███████████████████████████▋                                                      | 26/77 [02:33<04:58,  5.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A04C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256221DB4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629ED8CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|████████████████████████████▊                                                     | 27/77 [02:39<04:47,  5.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D9D678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620E561F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562216AB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|█████████████████████████████▊                                                    | 28/77 [02:44<04:33,  5.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.6, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A07F4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620926948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562674EA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 00063: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|██████████████████████████████▉                                                   | 29/77 [02:49<04:23,  5.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256237AD1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629FE05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Epoch 00073: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███████████████████████████████▉                                                  | 30/77 [02:55<04:20,  5.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.8, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209264C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562781F288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A07FCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Epoch 00085: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|█████████████████████████████████                                                 | 31/77 [03:01<04:25,  5.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.9, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A1CFB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A4D54C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025627801678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Epoch 00090: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|██████████████████████████████████                                                | 32/77 [03:08<04:29,  6.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.7, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D554C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562216A4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A07F948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Epoch 00067: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|███████████████████████████████████▏                                              | 33/77 [03:13<04:16,  5.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256232E35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256266C20D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256264095E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Epoch 00074: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████████████████████████████████████▏                                             | 34/77 [03:19<04:10,  5.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562216AE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256232EFB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E568B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Epoch 00073: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|█████████████████████████████████████▎                                            | 35/77 [03:25<04:03,  5.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256232E35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629F4B5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E56DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Epoch 00076: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|██████████████████████████████████████▎                                           | 36/77 [03:31<03:58,  5.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E565E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002561F685DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256264093A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 00084: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|███████████████████████████████████████▍                                          | 37/77 [03:37<03:57,  5.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629ED8558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627B0D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E560D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Epoch 00080: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|████████████████████████████████████████▍                                         | 38/77 [03:43<03:52,  5.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D9D168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256221DBF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626409CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Epoch 00071: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████████████████████████████████████████▌                                        | 39/77 [03:48<03:43,  5.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.6, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562220CD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620F66288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256232EFE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 00088: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|██████████████████████████████████████████▌                                       | 40/77 [03:55<03:43,  6.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626409DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A04C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E56168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Epoch 00083: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|███████████████████████████████████████████▋                                      | 41/77 [04:01<03:39,  6.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.8, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025621111E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626299048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025627BEDF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Epoch 00067: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|████████████████████████████████████████████▋                                     | 42/77 [04:07<03:26,  5.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.9, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F97678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025621111B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025627A62DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Epoch 00080: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████████████████████████████████████████████▊                                    | 43/77 [04:13<03:22,  5.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.75, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209264C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A059798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025627BEDC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|██████████████████████████████████████████████▊                                   | 44/77 [04:18<03:11,  5.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A04CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562650C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025627BED5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 00060: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|███████████████████████████████████████████████▉                                  | 45/77 [04:23<03:01,  5.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A059828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562216A4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620E563A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 62.\n",
      "Epoch 00112: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|████████████████████████████████████████████████▉                                 | 46/77 [04:31<03:13,  6.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F97D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A059798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620E56318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|██████████████████████████████████████████████████                                | 47/77 [04:36<02:59,  5.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D9D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620D555E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A07FA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Epoch 00072: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|███████████████████████████████████████████████████                               | 48/77 [04:42<02:50,  5.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025627B5B288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A07F558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|████████████████████████████████████████████████████▏                             | 49/77 [04:47<02:40,  5.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002561F6855E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025622198D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9D828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|█████████████████████████████████████████████████████▏                            | 50/77 [04:53<02:30,  5.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.6, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A07F3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A6F19D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209D7678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 00084: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|██████████████████████████████████████████████████████▎                           | 51/77 [04:59<02:29,  5.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256250223A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629FE00D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025627B5BEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Epoch 00072: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|███████████████████████████████████████████████████████▍                          | 52/77 [05:05<02:23,  5.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.8, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E564C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620D55708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9D8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|████████████████████████████████████████████████████████▍                         | 53/77 [05:10<02:15,  5.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.9, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629FE09D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025624E96798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620E56828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|█████████████████████████████████████████████████████████▌                        | 54/77 [05:15<02:05,  5.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.8, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562216A4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562220C438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A2CE9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 55/77 [05:20<01:58,  5.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ABA39D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A6F19D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F97438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 00102: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████████████████████████████████████████████████████████▋                      | 56/77 [05:27<02:04,  5.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ABA3B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3CE4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E55AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|████████████████████████████████████████████████████████████▋                     | 57/77 [05:33<01:55,  5.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620E7C9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629FE0438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Epoch 00071: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|█████████████████████████████████████████████████████████████▊                    | 58/77 [05:38<01:48,  5.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562220CD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A857E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA33A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Epoch 00090: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|██████████████████████████████████████████████████████████████▊                   | 59/77 [05:45<01:46,  5.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E550D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3CE4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 00060: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████████████████████████████████████████████████████████████▉                  | 60/77 [05:50<01:36,  5.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F4BE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256232E3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA31F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Epoch 00076: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|████████████████████████████████████████████████████████████████▉                 | 61/77 [05:56<01:31,  5.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.6, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629FE0438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626299048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|██████████████████████████████████████████████████████████████████                | 62/77 [06:01<01:24,  5.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256232E35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256266C2168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|███████████████████████████████████████████████████████████████████               | 63/77 [06:07<01:21,  5.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.8, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A223948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256278014C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025627BEDB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████▏             | 64/77 [06:13<01:13,  5.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.9, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562781F288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025618F968B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|█████████████████████████████████████████████████████████████████████▏            | 65/77 [06:18<01:06,  5.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F97048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562781F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562BF43438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 66/77 [06:23<00:59,  5.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A8D14C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620D555E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 00097: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|███████████████████████████████████████████████████████████████████████▎          | 67/77 [06:30<00:58,  5.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.1, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A4D58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256266C2DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A8D1708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████████▍         | 68/77 [06:36<00:52,  5.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256261EAAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A223A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620F66828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 00060: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████████████████████████████████████████████████████████████████████▍        | 69/77 [06:41<00:45,  5.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.3, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D9D558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620DFD3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562674EC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Epoch 00091: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▌       | 70/77 [06:48<00:41,  5.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.4, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256209264C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ABA3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025626368708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Epoch 00072: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▌      | 71/77 [06:53<00:35,  5.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.5, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562674EAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629E55EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562AD40DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|████████████████████████████████████████████████████████████████████████████▋     | 72/77 [06:58<00:28,  5.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.6, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256264095E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562781F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D9D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Epoch 00076: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████▋    | 73/77 [07:04<00:22,  5.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620926048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562359F558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A07F828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 00063: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|██████████████████████████████████████████████████████████████████████████████▊   | 74/77 [07:10<00:16,  5.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.8, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562633D948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620926708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256232EF708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▊  | 75/77 [07:15<00:10,  5.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.9, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A8D13A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623C7BF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620F66D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 00063: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 99%|████████████████████████████████████████████████████████████████████████████████▉ | 76/77 [07:20<00:05,  5.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.9, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 2, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.01}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620926948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A04C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A8D10D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [07:25<00:00,  5.79s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0f4377e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/M/Predykcja klasy M - podejscie z Focal-loss (nowe)/050522222938.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a22ef62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>gamma</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>0.108194</td>\n",
       "      <td>[0.13043478]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.124587</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>0.101885</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.120106</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>0.095572</td>\n",
       "      <td>[0.15053762]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.117651</td>\n",
       "      <td>[0.16000001]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98</td>\n",
       "      <td>0.088541</td>\n",
       "      <td>[0.06741573]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.106944</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>0.084127</td>\n",
       "      <td>[0.12903227]</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.096816</td>\n",
       "      <td>[0.16000001]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>76</td>\n",
       "      <td>0.052518</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>[0.26829267]</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>63</td>\n",
       "      <td>0.049284</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049682</td>\n",
       "      <td>[0.26829267]</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>55</td>\n",
       "      <td>0.046674</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047030</td>\n",
       "      <td>[0.26829267]</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>63</td>\n",
       "      <td>0.043638</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043978</td>\n",
       "      <td>[0.26829267]</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>57</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041182</td>\n",
       "      <td>[0.26829267]</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "0             80  0.108194  [0.13043478]   1.000000  0.069767  0.124587   \n",
       "1             68  0.101885  [0.15053762]   1.000000  0.081395  0.120106   \n",
       "2             78  0.095572  [0.15053762]   1.000000  0.081395  0.117651   \n",
       "3             98  0.088541  [0.06741573]   1.000000  0.034884  0.106944   \n",
       "4             81  0.084127  [0.12903227]   0.857143  0.069767  0.096816   \n",
       "..           ...       ...           ...        ...       ...       ...   \n",
       "72            76  0.052518  [0.26299694]   0.151408  1.000000  0.052903   \n",
       "73            63  0.049284  [0.26299694]   0.151408  1.000000  0.049682   \n",
       "74            55  0.046674  [0.26299694]   0.151408  1.000000  0.047030   \n",
       "75            63  0.043638  [0.26299694]   0.151408  1.000000  0.043978   \n",
       "76            57  0.040859  [0.26299694]   0.151408  1.000000  0.041182   \n",
       "\n",
       "   val_fbeta_score  val_precision  val_recall activation_layer  ...  epochs  \\\n",
       "0             [0.]       0.000000    0.000000             relu  ...  100000   \n",
       "1             [0.]       0.000000    0.000000             relu  ...  100000   \n",
       "2     [0.16000001]       0.666667    0.090909             relu  ...  100000   \n",
       "3             [0.]       0.000000    0.000000             relu  ...  100000   \n",
       "4     [0.16000001]       0.666667    0.090909             relu  ...  100000   \n",
       "..             ...            ...         ...              ...  ...     ...   \n",
       "72    [0.26829267]       0.154930    1.000000             relu  ...  100000   \n",
       "73    [0.26829267]       0.154930    1.000000             relu  ...  100000   \n",
       "74    [0.26829267]       0.154930    1.000000             relu  ...  100000   \n",
       "75    [0.26829267]       0.154930    1.000000             relu  ...  100000   \n",
       "76    [0.26829267]       0.154930    1.000000             relu  ...  100000   \n",
       "\n",
       "    first_neuron  gamma  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "0            110    1.0              3            100          orthogonal   \n",
       "1            110    1.1              3            100          orthogonal   \n",
       "2            110    1.2              3            100          orthogonal   \n",
       "3            110    1.3              3            100          orthogonal   \n",
       "4            110    1.4              3            100          orthogonal   \n",
       "..           ...    ...            ...            ...                 ...   \n",
       "72           110    1.6              3            100          orthogonal   \n",
       "73           110    1.7              3            100          orthogonal   \n",
       "74           110    1.8              3            100          orthogonal   \n",
       "75           110    1.9              3            100          orthogonal   \n",
       "76           110    2.0              3            100          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation    lr  \n",
       "0                  0.0001                 0.0001          sigmoid  0.01  \n",
       "1                  0.0001                 0.0001          sigmoid  0.01  \n",
       "2                  0.0001                 0.0001          sigmoid  0.01  \n",
       "3                  0.0001                 0.0001          sigmoid  0.01  \n",
       "4                  0.0001                 0.0001          sigmoid  0.01  \n",
       "..                    ...                    ...              ...   ...  \n",
       "72                 0.0001                 0.0001          sigmoid  0.01  \n",
       "73                 0.0001                 0.0001          sigmoid  0.01  \n",
       "74                 0.0001                 0.0001          sigmoid  0.01  \n",
       "75                 0.0001                 0.0001          sigmoid  0.01  \n",
       "76                 0.0001                 0.0001          sigmoid  0.01  \n",
       "\n",
       "[77 rows x 26 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "68a276b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reindex(columns=['val_fbeta_score', 'fbeta_score',\n",
    "       'precision', 'recall', 'val_loss', 'loss',\n",
    "      'val_precision',\n",
    "       'val_recall', 'gamma','alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7e15fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['val_fbeta_score']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6b084a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['recall']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "11f3c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['precision']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eb6c8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9463df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stare_wart=df['val_fbeta_score']\n",
    "nowe_wart=[]\n",
    "for x in stare_wart:\n",
    "    wart=x.replace('[','')\n",
    "    wart=wart.replace(']','')\n",
    "    wart=float(wart)\n",
    "    nowe_wart.append(wart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "70f0ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['val_fbeta_score']=nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3802b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['val_fbeta_score']>=0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "77ebb3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>[0.2201835]</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.113385</td>\n",
       "      <td>0.088619</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.281690</td>\n",
       "      <td>[0.3068592]</td>\n",
       "      <td>0.181624</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.100955</td>\n",
       "      <td>0.086858</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>[0.30601093]</td>\n",
       "      <td>0.181425</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.096065</td>\n",
       "      <td>0.081503</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.281690</td>\n",
       "      <td>[0.30852994]</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.095733</td>\n",
       "      <td>0.075617</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.206897</td>\n",
       "      <td>[0.17821783]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.090992</td>\n",
       "      <td>0.079640</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.289855</td>\n",
       "      <td>[0.31365314]</td>\n",
       "      <td>0.186404</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.088710</td>\n",
       "      <td>0.070545</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>[0.3041145]</td>\n",
       "      <td>0.179704</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.083474</td>\n",
       "      <td>0.073808</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>[0.18181817]</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.082003</td>\n",
       "      <td>0.069394</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>[0.3099631]</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.077891</td>\n",
       "      <td>0.066723</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>[0.20833331]</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.075508</td>\n",
       "      <td>0.064734</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.281690</td>\n",
       "      <td>[0.30465952]</td>\n",
       "      <td>0.180085</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.070117</td>\n",
       "      <td>0.059704</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.283688</td>\n",
       "      <td>[0.3043478]</td>\n",
       "      <td>0.180258</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.063545</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.289855</td>\n",
       "      <td>[0.3132075]</td>\n",
       "      <td>0.186937</td>\n",
       "      <td>0.965116</td>\n",
       "      <td>0.066672</td>\n",
       "      <td>0.055698</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>[0.3074141]</td>\n",
       "      <td>0.182013</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.058782</td>\n",
       "      <td>0.052117</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.281690</td>\n",
       "      <td>[0.30379745]</td>\n",
       "      <td>0.179872</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.056526</td>\n",
       "      <td>0.048453</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_fbeta_score   fbeta_score  precision    recall  val_loss      loss  \\\n",
       "45         0.222222   [0.2201835]   0.521739  0.139535  0.113385  0.088619   \n",
       "55         0.281690   [0.3068592]   0.181624  0.988372  0.100955  0.086858   \n",
       "56         0.285714  [0.30601093]   0.181425  0.976744  0.096065  0.081503   \n",
       "57         0.281690  [0.30852994]   0.182796  0.988372  0.095733  0.075617   \n",
       "47         0.206897  [0.17821783]   0.600000  0.104651  0.090992  0.079640   \n",
       "58         0.289855  [0.31365314]   0.186404  0.988372  0.088710  0.070545   \n",
       "66         0.277778   [0.3041145]   0.179704  0.988372  0.083474  0.073808   \n",
       "49         0.214286  [0.18181817]   0.692308  0.104651  0.082003  0.069394   \n",
       "59         0.285714   [0.3099631]   0.184211  0.976744  0.077891  0.066723   \n",
       "50         0.222222  [0.20833331]   0.258621  0.174419  0.075508  0.064734   \n",
       "69         0.281690  [0.30465952]   0.180085  0.988372  0.070117  0.059704   \n",
       "60         0.283688   [0.3043478]   0.180258  0.976744  0.068800  0.063545   \n",
       "62         0.289855   [0.3132075]   0.186937  0.965116  0.066672  0.055698   \n",
       "63         0.285714   [0.3074141]   0.182013  0.988372  0.058782  0.052117   \n",
       "64         0.281690  [0.30379745]   0.179872  0.976744  0.056526  0.048453   \n",
       "\n",
       "    val_precision  val_recall  gamma  alpha  \n",
       "45       0.600000    0.136364    1.1   0.80  \n",
       "55       0.166667    0.909091    1.0   0.85  \n",
       "56       0.169492    0.909091    1.1   0.85  \n",
       "57       0.166667    0.909091    1.2   0.85  \n",
       "47       0.428571    0.136364    1.3   0.80  \n",
       "58       0.172414    0.909091    1.3   0.85  \n",
       "66       0.163934    0.909091    1.0   0.90  \n",
       "49       0.500000    0.136364    1.5   0.80  \n",
       "59       0.169492    0.909091    1.4   0.85  \n",
       "50       0.600000    0.136364    1.6   0.80  \n",
       "69       0.166667    0.909091    1.3   0.90  \n",
       "60       0.168067    0.909091    1.5   0.85  \n",
       "62       0.172414    0.909091    1.7   0.85  \n",
       "63       0.169492    0.909091    1.8   0.85  \n",
       "64       0.166667    0.909091    1.9   0.85  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf259",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eb2d4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[50,110], \n",
    "     'hidden_neuron':[50,150],\n",
    "\n",
    "     'hidden_layers':[1,3,5],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [32],\n",
    "\n",
    "    #'lr':[0.01],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "#    'batc_normalization':[False,True],\n",
    "#    'dropout': [0,0.2,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "    'optimizer': ['rmsprop','adam','adadelta','adamax','nadam','adagrad'],\n",
    "    #'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "    'kernel_initializer': ['orthogonal'],\n",
    "    #'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    "    'activation_layer':['relu'],\n",
    "    #'batc_normalization':[False,True]\n",
    "    'batc_normalization':[False],\n",
    "    'alpha':[0.85],\n",
    "    'gamma':[1.7]\n",
    "    \n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "653d69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss=[binary_focal_loss(alpha=params['alpha'], gamma=params['gamma'])],\n",
    "                  optimizer=params['optimizer'],#tf.keras.optimizers.Adamax(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=1.0, threshold=0.5),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall')])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ab4ff57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                           | 0/72 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E6D0B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562E64DCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025630303B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 00093: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▏                                                                            | 1/16 [01:52<28:09, 112.64s/it]\n",
      "\n",
      "\n",
      "  1%|█▏                                                                                 | 1/72 [00:05<06:36,  5.59s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025626482A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ECE4F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256266C2708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Epoch 00096: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|██▎                                                                                | 2/72 [00:11<06:30,  5.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E6A6558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626409AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A2CE8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|███▍                                                                               | 3/72 [00:15<05:30,  4.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620F599D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025621089678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4D5288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 108.\n",
      "Epoch 00158: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  6%|████▌                                                                              | 4/72 [00:22<06:42,  5.92s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ABA3438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626299948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E75CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 00097: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|█████▊                                                                             | 5/72 [00:28<06:38,  5.95s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025625022F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562633D948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A8D1708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  8%|██████▉                                                                            | 6/72 [00:32<05:44,  5.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620E7CDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A8D1CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E6A6C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Epoch 00071: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|████████                                                                           | 7/72 [00:37<05:30,  5.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E6A6948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562E64DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025630303AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Epoch 00077: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 11%|█████████▏                                                                         | 8/72 [00:42<05:24,  5.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E3CA9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620E569D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025614BD4E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 12%|██████████▍                                                                        | 9/72 [00:46<04:57,  4.72s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F97948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623C7B1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629E56DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 00107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 14%|███████████▍                                                                      | 10/72 [00:52<05:16,  5.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256303030D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2AC9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E6A6678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Epoch 00080: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 15%|████████████▌                                                                     | 11/72 [00:57<05:17,  5.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623C7BAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A4D54C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025630303678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█████████████▋                                                                    | 12/72 [01:01<04:47,  4.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F97B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256232E3558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Epoch 00085: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 18%|██████████████▊                                                                   | 13/72 [01:07<05:04,  5.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629F97948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620E7CDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E75C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 19%|███████████████▉                                                                  | 14/72 [01:13<05:07,  5.30s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562781F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256209D7708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E664C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 21%|█████████████████                                                                 | 15/72 [01:17<04:47,  5.04s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ABA3288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562781F4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E6D0A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 85.\n",
      "Epoch 00135: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 22%|██████████████████▏                                                               | 16/72 [01:25<05:28,  5.86s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A857D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562E6D0B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E6648B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 24%|███████████████████▎                                                              | 17/72 [01:31<05:27,  5.96s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562AD40CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256261EA678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620D55828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|████████████████████▌                                                             | 18/72 [01:35<04:55,  5.48s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562674E798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A6F1948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E664E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 00065: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 26%|█████████████████████▋                                                            | 19/72 [01:41<04:54,  5.56s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025630303678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629FE01F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E664AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 00075: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 28%|██████████████████████▊                                                           | 20/72 [01:47<04:57,  5.72s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E6D03A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A72A9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A8570D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 29%|███████████████████████▉                                                          | 21/72 [01:52<04:37,  5.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629FE00D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620D55828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E6D0AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 82.\n",
      "Epoch 00132: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 31%|█████████████████████████                                                         | 22/72 [02:01<05:19,  6.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256232E3558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562AD401F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E2CBAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Epoch 00078: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 32%|██████████████████████████▏                                                       | 23/72 [02:07<05:19,  6.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ECE4A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620E7C0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620DFD288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███████████████████████████▎                                                      | 24/72 [02:12<04:47,  5.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E7A23A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620F59C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 35%|████████████████████████████▍                                                     | 25/72 [02:18<04:41,  5.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620DFD558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2CE708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ECE4B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 36%|█████████████████████████████▌                                                    | 26/72 [02:24<04:39,  6.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D9DAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A72A9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F97678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 38%|██████████████████████████████▊                                                   | 27/72 [02:29<04:18,  5.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A2CE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256266C2E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562EA10CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 59.\n",
      "Epoch 00109: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 39%|███████████████████████████████▉                                                  | 28/72 [02:37<04:34,  6.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623C7BC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025621089678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E682CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Epoch 00073: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|█████████████████████████████████                                                 | 29/72 [02:44<04:35,  6.42s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D55828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562D034C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562EA10EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 42%|██████████████████████████████████▏                                               | 30/72 [02:49<04:10,  5.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ECE4678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620E56828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 43%|███████████████████████████████████▎                                              | 31/72 [02:55<04:13,  6.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629E56DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ABA3C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562EA104C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 44%|████████████████████████████████████▍                                             | 32/72 [03:02<04:10,  6.27s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620F59558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629E550D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ECE4DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 46%|█████████████████████████████████████▌                                            | 33/72 [03:07<03:58,  6.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ABA3948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2AC9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562EA10948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 47%|██████████████████████████████████████▋                                           | 34/72 [03:16<04:17,  6.78s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620DFDCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629F97678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562FFD5828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 49%|███████████████████████████████████████▊                                          | 35/72 [03:23<04:20,  7.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 50, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620F599D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025629F4B168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ECE4168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████████████████████████████████████████                                         | 36/72 [03:29<03:58,  6.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256221981F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A4D58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E7A2948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Epoch 00078: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 51%|██████████████████████████████████████████▏                                       | 37/72 [03:34<03:36,  6.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256266C25E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627B0D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025621052048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 53%|███████████████████████████████████████████▎                                      | 38/72 [03:40<03:21,  5.91s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256278014C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025614BD6318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ABA3CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 54%|████████████████████████████████████████████▍                                     | 39/72 [03:44<02:56,  5.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E7A29D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A72A288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E6A6E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 117.\n",
      "Epoch 00167: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 56%|█████████████████████████████████████████████▌                                    | 40/72 [03:52<03:21,  6.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A6F1948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256209D7708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E664A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 00084: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 57%|██████████████████████████████████████████████▋                                   | 41/72 [03:58<03:09,  6.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562080A288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256209EA948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E682EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 58%|███████████████████████████████████████████████▊                                  | 42/72 [04:02<02:43,  5.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025621052B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627B5B708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E6D0E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 00070: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|████████████████████████████████████████████████▉                                 | 43/72 [04:07<02:33,  5.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E664C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025625022B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A3CE288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 73.\n",
      "Epoch 00123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 61%|██████████████████████████████████████████████████                                | 44/72 [04:14<02:43,  5.84s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E6D0948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A8D1288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A2CE3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 62%|███████████████████████████████████████████████████▎                              | 45/72 [04:18<02:23,  5.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E3CA4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2CE678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E682B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 120.\n",
      "Epoch 00170: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 64%|████████████████████████████████████████████████████▍                             | 46/72 [04:27<02:44,  6.34s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A2CE558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A3CEB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620DFDC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 00084: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 65%|█████████████████████████████████████████████████████▌                            | 47/72 [04:32<02:34,  6.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ECE45E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256209D7708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E2CB288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████████████████████████████████████████████████████▋                           | 48/72 [04:36<02:12,  5.53s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ABA3F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562ECE4708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A2AC9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Epoch 00072: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 68%|███████████████████████████████████████████████████████▊                          | 49/72 [04:42<02:07,  5.54s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D55168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A8D1288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ECE4948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 00086: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 69%|████████████████████████████████████████████████████████▉                         | 50/72 [04:48<02:05,  5.71s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A2AC8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A2CEC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A07F048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 71%|██████████████████████████████████████████████████████████                        | 51/72 [04:53<01:52,  5.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ABA3318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627C10288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025620DFD0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 73.\n",
      "Epoch 00123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 72%|███████████████████████████████████████████████████████████▏                      | 52/72 [05:00<01:59,  5.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025622198D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025620DFD438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E75C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 74%|████████████████████████████████████████████████████████████▎                     | 53/72 [05:06<01:56,  6.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025630303678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562A223A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562EA10EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 54/72 [05:11<01:41,  5.62s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000256266C20D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562E6D05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4AC948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 76%|██████████████████████████████████████████████████████████████▋                   | 55/72 [05:17<01:36,  5.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623450AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623456F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256208491F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Epoch 00074: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 78%|███████████████████████████████████████████████████████████████▊                  | 56/72 [05:23<01:33,  5.86s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562EA10A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562D034C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562ECE4AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 79%|████████████████████████████████████████████████████████████████▉                 | 57/72 [05:28<01:23,  5.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562EBC4AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562EA10E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256209260D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 83.\n",
      "Epoch 00133: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 81%|██████████████████████████████████████████████████████████████████                | 58/72 [05:37<01:31,  6.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ECE4828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562080ADC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256210529D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Epoch 00078: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 82%|███████████████████████████████████████████████████████████████████▏              | 59/72 [05:44<01:27,  6.72s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620D9D0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627BED558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025629F97828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 60/72 [05:49<01:14,  6.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562674E798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562E3CA0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4AC9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 61/72 [05:55<01:07,  6.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025623450AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562FE42558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E75C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Epoch 00090: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 86%|██████████████████████████████████████████████████████████████████████▌           | 62/72 [06:02<01:03,  6.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562AD405E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562633DD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562EB449D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 63/72 [06:07<00:54,  6.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562AD401F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562781F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562FD84CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "Epoch 00114: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 89%|████████████████████████████████████████████████████████████████████████▉         | 64/72 [06:15<00:52,  6.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562A4AC828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562FD848B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562EB44048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Epoch 00074: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████████        | 65/72 [06:22<00:47,  6.72s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562674E8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x00000256221980D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4ACEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 66/72 [06:27<00:37,  6.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562FD849D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627B5B708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000256305F3438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 93%|████████████████████████████████████████████████████████████████████████████▎     | 67/72 [06:34<00:31,  6.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E75CD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025627B0D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562AD405E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 00070: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▍    | 68/72 [06:41<00:26,  6.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025629FE00D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x000002562E87EC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4AC3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 96%|██████████████████████████████████████████████████████████████████████████████▌   | 69/72 [06:47<00:19,  6.41s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562ECE4048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626606D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562FD84288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 58.\n",
      "Epoch 00108: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 70/72 [06:56<00:14,  7.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002562E6A6048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025626299048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562A4AC558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 99%|████████████████████████████████████████████████████████████████████████████████▊ | 71/72 [07:04<00:07,  7.43s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'alpha': 0.85, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 110, 'gamma': 1.7, 'hidden_layers': 5, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025620DFDC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function binary_focal_loss.<locals>.binary_focal_loss_fixed at 0x0000025623AD8B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002562E8FBB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 72/72 [07:10<00:00,  5.98s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy M - podejscie z Focal-loss (nowe)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cf2982bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/M/Predykcja klasy M - podejscie z Focal-loss (nowe)/050522225017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "257e2e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>gamma</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>0.051822</td>\n",
       "      <td>[0.2993763]</td>\n",
       "      <td>0.182278</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.064037</td>\n",
       "      <td>[0.28571427]</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>0.051066</td>\n",
       "      <td>[0.30852994]</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.065722</td>\n",
       "      <td>[0.28571427]</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>0.144740</td>\n",
       "      <td>[0.24305557]</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.145879</td>\n",
       "      <td>[0.18348624]</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>[0.317757]</td>\n",
       "      <td>0.189310</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.059225</td>\n",
       "      <td>[0.30612242]</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>0.051204</td>\n",
       "      <td>[0.30711612]</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.059571</td>\n",
       "      <td>[0.37837836]</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>70</td>\n",
       "      <td>0.056006</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056523</td>\n",
       "      <td>[0.26829267]</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>51</td>\n",
       "      <td>0.935253</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936036</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>108</td>\n",
       "      <td>0.056058</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056570</td>\n",
       "      <td>[0.26829267]</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>66</td>\n",
       "      <td>0.057568</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058078</td>\n",
       "      <td>[0.26829267]</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>51</td>\n",
       "      <td>0.929338</td>\n",
       "      <td>[0.27633852]</td>\n",
       "      <td>0.162272</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.929752</td>\n",
       "      <td>[0.2797203]</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "0             93  0.051822   [0.2993763]   0.182278  0.837209  0.064037   \n",
       "1             96  0.051066  [0.30852994]   0.182796  0.988372  0.065722   \n",
       "2             51  0.144740  [0.24305557]   0.142857  0.813953  0.145879   \n",
       "3            158  0.054593    [0.317757]   0.189310  0.988372  0.059225   \n",
       "4             97  0.051204  [0.30711612]   0.183036  0.953488  0.059571   \n",
       "..           ...       ...           ...        ...       ...       ...   \n",
       "67            70  0.056006  [0.26299694]   0.151408  1.000000  0.056523   \n",
       "68            51  0.935253          [0.]   0.000000  0.000000  0.936036   \n",
       "69           108  0.056058  [0.26299694]   0.151408  1.000000  0.056570   \n",
       "70            66  0.057568  [0.26299694]   0.151408  1.000000  0.058078   \n",
       "71            51  0.929338  [0.27633852]   0.162272  0.930233  0.929752   \n",
       "\n",
       "   val_fbeta_score  val_precision  val_recall activation_layer  ...  epochs  \\\n",
       "0     [0.28571427]       0.169492    0.909091             relu  ...  100000   \n",
       "1     [0.28571427]       0.169492    0.909091             relu  ...  100000   \n",
       "2     [0.18348624]       0.114943    0.454545             relu  ...  100000   \n",
       "3     [0.30612242]       0.197368    0.681818             relu  ...  100000   \n",
       "4     [0.37837836]       0.466667    0.318182             relu  ...  100000   \n",
       "..             ...            ...         ...              ...  ...     ...   \n",
       "67    [0.26829267]       0.154930    1.000000             relu  ...  100000   \n",
       "68            [0.]       0.000000    0.000000             relu  ...  100000   \n",
       "69    [0.26829267]       0.154930    1.000000             relu  ...  100000   \n",
       "70    [0.26829267]       0.154930    1.000000             relu  ...  100000   \n",
       "71     [0.2797203]       0.165289    0.909091             relu  ...  100000   \n",
       "\n",
       "    first_neuron  gamma  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "0             50    1.7              1             50          orthogonal   \n",
       "1             50    1.7              1             50          orthogonal   \n",
       "2             50    1.7              1             50          orthogonal   \n",
       "3             50    1.7              1             50          orthogonal   \n",
       "4             50    1.7              1             50          orthogonal   \n",
       "..           ...    ...            ...            ...                 ...   \n",
       "67           110    1.7              5            150          orthogonal   \n",
       "68           110    1.7              5            150          orthogonal   \n",
       "69           110    1.7              5            150          orthogonal   \n",
       "70           110    1.7              5            150          orthogonal   \n",
       "71           110    1.7              5            150          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation  optimizer  \n",
       "0                  0.0001                 0.0001          sigmoid    rmsprop  \n",
       "1                  0.0001                 0.0001          sigmoid       adam  \n",
       "2                  0.0001                 0.0001          sigmoid   adadelta  \n",
       "3                  0.0001                 0.0001          sigmoid     adamax  \n",
       "4                  0.0001                 0.0001          sigmoid      nadam  \n",
       "..                    ...                    ...              ...        ...  \n",
       "67                 0.0001                 0.0001          sigmoid       adam  \n",
       "68                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "69                 0.0001                 0.0001          sigmoid     adamax  \n",
       "70                 0.0001                 0.0001          sigmoid      nadam  \n",
       "71                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "\n",
       "[72 rows x 26 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9e33e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "stare_wart=df['val_fbeta_score']\n",
    "nowe_wart=[]\n",
    "for x in stare_wart:\n",
    "    wart=x.replace('[','')\n",
    "    wart=wart.replace(']','')\n",
    "    wart=float(wart)\n",
    "    nowe_wart.append(wart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "057c0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['val_fbeta_score']=nowe_wart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ed856cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ef00e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>gamma</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>51</td>\n",
       "      <td>0.935253</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>51</td>\n",
       "      <td>0.929338</td>\n",
       "      <td>[0.27633852]</td>\n",
       "      <td>0.162272</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.929752</td>\n",
       "      <td>0.279720</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>51</td>\n",
       "      <td>0.807648</td>\n",
       "      <td>[0.04347826]</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.808121</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>51</td>\n",
       "      <td>0.803775</td>\n",
       "      <td>[0.28832117]</td>\n",
       "      <td>0.170996</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.804499</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.159292</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>51</td>\n",
       "      <td>0.607745</td>\n",
       "      <td>[0.28169015]</td>\n",
       "      <td>0.165975</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.608829</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>108</td>\n",
       "      <td>0.056058</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056570</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>70</td>\n",
       "      <td>0.056006</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056523</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>62</td>\n",
       "      <td>0.055865</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056383</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>98</td>\n",
       "      <td>0.055828</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056350</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>90</td>\n",
       "      <td>0.054943</td>\n",
       "      <td>[0.26299694]</td>\n",
       "      <td>0.151408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055462</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "68            51  0.935253          [0.]   0.000000  0.000000  0.936036   \n",
       "71            51  0.929338  [0.27633852]   0.162272  0.930233  0.929752   \n",
       "32            51  0.807648  [0.04347826]   0.057692  0.034884  0.808121   \n",
       "35            51  0.803775  [0.28832117]   0.170996  0.918605  0.804499   \n",
       "59            51  0.607745  [0.28169015]   0.165975  0.930233  0.608829   \n",
       "..           ...       ...           ...        ...       ...       ...   \n",
       "69           108  0.056058  [0.26299694]   0.151408  1.000000  0.056570   \n",
       "67            70  0.056006  [0.26299694]   0.151408  1.000000  0.056523   \n",
       "31            62  0.055865  [0.26299694]   0.151408  1.000000  0.056383   \n",
       "33            98  0.055828  [0.26299694]   0.151408  1.000000  0.056350   \n",
       "61            90  0.054943  [0.26299694]   0.151408  1.000000  0.055462   \n",
       "\n",
       "    val_fbeta_score  val_precision  val_recall activation_layer  ...  epochs  \\\n",
       "68         0.000000       0.000000    0.000000             relu  ...  100000   \n",
       "71         0.279720       0.165289    0.909091             relu  ...  100000   \n",
       "32         0.102564       0.117647    0.090909             relu  ...  100000   \n",
       "35         0.266667       0.159292    0.818182             relu  ...  100000   \n",
       "59         0.281690       0.166667    0.909091             relu  ...  100000   \n",
       "..              ...            ...         ...              ...  ...     ...   \n",
       "69         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "67         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "31         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "33         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "61         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "\n",
       "    first_neuron  gamma  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "68           110    1.7              5            150          orthogonal   \n",
       "71           110    1.7              5            150          orthogonal   \n",
       "32            50    1.7              5            150          orthogonal   \n",
       "35            50    1.7              5            150          orthogonal   \n",
       "59           110    1.7              3            150          orthogonal   \n",
       "..           ...    ...            ...            ...                 ...   \n",
       "69           110    1.7              5            150          orthogonal   \n",
       "67           110    1.7              5            150          orthogonal   \n",
       "31            50    1.7              5            150          orthogonal   \n",
       "33            50    1.7              5            150          orthogonal   \n",
       "61           110    1.7              5             50          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation  optimizer  \n",
       "68                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "71                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "32                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "35                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "59                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "..                    ...                    ...              ...        ...  \n",
       "69                 0.0001                 0.0001          sigmoid     adamax  \n",
       "67                 0.0001                 0.0001          sigmoid       adam  \n",
       "31                 0.0001                 0.0001          sigmoid       adam  \n",
       "33                 0.0001                 0.0001          sigmoid     adamax  \n",
       "61                 0.0001                 0.0001          sigmoid       adam  \n",
       "\n",
       "[72 rows x 26 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6954f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['recall']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9dbb7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['precision']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "db1f50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['val_fbeta_score']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "33c51911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>gamma</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>51</td>\n",
       "      <td>0.929338</td>\n",
       "      <td>[0.27633852]</td>\n",
       "      <td>0.162272</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.929752</td>\n",
       "      <td>0.279720</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>51</td>\n",
       "      <td>0.807648</td>\n",
       "      <td>[0.04347826]</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.808121</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>51</td>\n",
       "      <td>0.803775</td>\n",
       "      <td>[0.28832117]</td>\n",
       "      <td>0.170996</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.804499</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.159292</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>51</td>\n",
       "      <td>0.607745</td>\n",
       "      <td>[0.28169015]</td>\n",
       "      <td>0.165975</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.608829</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>51</td>\n",
       "      <td>0.608901</td>\n",
       "      <td>[0.26966295]</td>\n",
       "      <td>0.156425</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.608227</td>\n",
       "      <td>0.281879</td>\n",
       "      <td>0.165354</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51</td>\n",
       "      <td>0.482894</td>\n",
       "      <td>[0.262697]</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>0.483054</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>51</td>\n",
       "      <td>0.480928</td>\n",
       "      <td>[0.28368795]</td>\n",
       "      <td>0.167364</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.481366</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.163793</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>51</td>\n",
       "      <td>0.346391</td>\n",
       "      <td>[0.23433243]</td>\n",
       "      <td>0.153025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.347060</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>51</td>\n",
       "      <td>0.288262</td>\n",
       "      <td>[0.08633094]</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.289086</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>0.287249</td>\n",
       "      <td>[0.26153845]</td>\n",
       "      <td>0.150709</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.287565</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>51</td>\n",
       "      <td>0.282478</td>\n",
       "      <td>[0.28990826]</td>\n",
       "      <td>0.172113</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.282130</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>51</td>\n",
       "      <td>0.277774</td>\n",
       "      <td>[0.269103]</td>\n",
       "      <td>0.156977</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.278439</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>51</td>\n",
       "      <td>0.275367</td>\n",
       "      <td>[0.]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274968</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>51</td>\n",
       "      <td>0.272048</td>\n",
       "      <td>[0.28264758]</td>\n",
       "      <td>0.167019</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.273899</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>51</td>\n",
       "      <td>0.214009</td>\n",
       "      <td>[0.05755395]</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.216458</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>51</td>\n",
       "      <td>0.211046</td>\n",
       "      <td>[0.04958678]</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.212499</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>0.205121</td>\n",
       "      <td>[0.2420382]</td>\n",
       "      <td>0.267606</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.206303</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51</td>\n",
       "      <td>0.161227</td>\n",
       "      <td>[0.07894737]</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.164248</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>51</td>\n",
       "      <td>0.160221</td>\n",
       "      <td>[0.05128205]</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.160731</td>\n",
       "      <td>0.239316</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>0.144740</td>\n",
       "      <td>[0.24305557]</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.145879</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51</td>\n",
       "      <td>0.138901</td>\n",
       "      <td>[0.27241382]</td>\n",
       "      <td>0.159919</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.141533</td>\n",
       "      <td>0.279720</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>107</td>\n",
       "      <td>0.058215</td>\n",
       "      <td>[0.3130755]</td>\n",
       "      <td>0.185996</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.291971</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>135</td>\n",
       "      <td>0.058856</td>\n",
       "      <td>[0.310219]</td>\n",
       "      <td>0.183983</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.077710</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>123</td>\n",
       "      <td>0.058243</td>\n",
       "      <td>[0.31460673]</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.074944</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>62</td>\n",
       "      <td>0.056594</td>\n",
       "      <td>[0.3071298]</td>\n",
       "      <td>0.182213</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.072302</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>86</td>\n",
       "      <td>0.053012</td>\n",
       "      <td>[0.30601093]</td>\n",
       "      <td>0.181425</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.071025</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>85</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>[0.29787236]</td>\n",
       "      <td>0.182292</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.070507</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>114</td>\n",
       "      <td>0.058909</td>\n",
       "      <td>[0.31481484]</td>\n",
       "      <td>0.187225</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.069456</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.176991</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>71</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>[0.3076923]</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.067845</td>\n",
       "      <td>0.279720</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>80</td>\n",
       "      <td>0.051099</td>\n",
       "      <td>[0.31365314]</td>\n",
       "      <td>0.186404</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.067451</td>\n",
       "      <td>0.300752</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>79</td>\n",
       "      <td>0.052764</td>\n",
       "      <td>[0.3130755]</td>\n",
       "      <td>0.185996</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.067188</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>78</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>[0.30645162]</td>\n",
       "      <td>0.185366</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.067138</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>0.051066</td>\n",
       "      <td>[0.30852994]</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.065722</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>73</td>\n",
       "      <td>0.053866</td>\n",
       "      <td>[0.3113553]</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.065676</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>70</td>\n",
       "      <td>0.053063</td>\n",
       "      <td>[0.30852994]</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.065666</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.162602</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>123</td>\n",
       "      <td>0.050042</td>\n",
       "      <td>[0.33684212]</td>\n",
       "      <td>0.205656</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.065242</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>84</td>\n",
       "      <td>0.051377</td>\n",
       "      <td>[0.3074074]</td>\n",
       "      <td>0.182819</td>\n",
       "      <td>0.965116</td>\n",
       "      <td>0.065140</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>79</td>\n",
       "      <td>0.053022</td>\n",
       "      <td>[0.30852994]</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.065133</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>82</td>\n",
       "      <td>0.051287</td>\n",
       "      <td>[0.30965394]</td>\n",
       "      <td>0.183585</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.064740</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>167</td>\n",
       "      <td>0.055416</td>\n",
       "      <td>[0.31499052]</td>\n",
       "      <td>0.188209</td>\n",
       "      <td>0.965116</td>\n",
       "      <td>0.064701</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>68</td>\n",
       "      <td>0.055283</td>\n",
       "      <td>[0.30465952]</td>\n",
       "      <td>0.180085</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.064496</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>72</td>\n",
       "      <td>0.054244</td>\n",
       "      <td>[0.3016158]</td>\n",
       "      <td>0.178344</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.064161</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>0.051822</td>\n",
       "      <td>[0.2993763]</td>\n",
       "      <td>0.182278</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.064037</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>133</td>\n",
       "      <td>0.056128</td>\n",
       "      <td>[0.31078613]</td>\n",
       "      <td>0.184382</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.063238</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.176991</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>109</td>\n",
       "      <td>0.057931</td>\n",
       "      <td>[0.310219]</td>\n",
       "      <td>0.183983</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.063149</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>74</td>\n",
       "      <td>0.053538</td>\n",
       "      <td>[0.30852994]</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.062948</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>132</td>\n",
       "      <td>0.057899</td>\n",
       "      <td>[0.3074141]</td>\n",
       "      <td>0.182013</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.062538</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>0.053746</td>\n",
       "      <td>[0.3082569]</td>\n",
       "      <td>0.183007</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.062390</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>0.051429</td>\n",
       "      <td>[0.31078613]</td>\n",
       "      <td>0.184382</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.062123</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>65</td>\n",
       "      <td>0.055866</td>\n",
       "      <td>[0.29893237]</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.061553</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>61</td>\n",
       "      <td>0.060581</td>\n",
       "      <td>[0.23255813]</td>\n",
       "      <td>0.145349</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.061096</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>61</td>\n",
       "      <td>0.059805</td>\n",
       "      <td>[0.23193917]</td>\n",
       "      <td>0.138636</td>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.060317</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>74</td>\n",
       "      <td>0.053564</td>\n",
       "      <td>[0.3125]</td>\n",
       "      <td>0.185590</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.060310</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>79</td>\n",
       "      <td>0.053252</td>\n",
       "      <td>[0.30909094]</td>\n",
       "      <td>0.183190</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>170</td>\n",
       "      <td>0.054145</td>\n",
       "      <td>[0.34070793]</td>\n",
       "      <td>0.210383</td>\n",
       "      <td>0.895349</td>\n",
       "      <td>0.060108</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>78</td>\n",
       "      <td>0.053894</td>\n",
       "      <td>[0.3113553]</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.060024</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>0.051204</td>\n",
       "      <td>[0.30711612]</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.059571</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>66</td>\n",
       "      <td>0.055270</td>\n",
       "      <td>[0.30601093]</td>\n",
       "      <td>0.181425</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.059556</td>\n",
       "      <td>0.293706</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>[0.317757]</td>\n",
       "      <td>0.189310</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.059225</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>75</td>\n",
       "      <td>0.053079</td>\n",
       "      <td>[0.31078613]</td>\n",
       "      <td>0.184382</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.059083</td>\n",
       "      <td>0.291971</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>relu</td>\n",
       "      <td>...</td>\n",
       "      <td>100000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss   fbeta_score  precision    recall  val_loss  \\\n",
       "71            51  0.929338  [0.27633852]   0.162272  0.930233  0.929752   \n",
       "32            51  0.807648  [0.04347826]   0.057692  0.034884  0.808121   \n",
       "35            51  0.803775  [0.28832117]   0.170996  0.918605  0.804499   \n",
       "59            51  0.607745  [0.28169015]   0.165975  0.930233  0.608829   \n",
       "56            51  0.608901  [0.26966295]   0.156425  0.976744  0.608227   \n",
       "20            51  0.482894    [0.262697]   0.154639  0.872093  0.483054   \n",
       "23            51  0.480928  [0.28368795]   0.167364  0.930233  0.481366   \n",
       "65            51  0.346391  [0.23433243]   0.153025  0.500000  0.347060   \n",
       "44            51  0.288262  [0.08633094]   0.113208  0.069767  0.289086   \n",
       "50            51  0.287249  [0.26153845]   0.150709  0.988372  0.287565   \n",
       "47            51  0.282478  [0.28990826]   0.172113  0.918605  0.282130   \n",
       "53            51  0.277774    [0.269103]   0.156977  0.941860  0.278439   \n",
       "26            51  0.275367          [0.]   0.000000  0.000000  0.274968   \n",
       "29            51  0.272048  [0.28264758]   0.167019  0.918605  0.273899   \n",
       "38            51  0.214009  [0.05755395]   0.075472  0.046512  0.216458   \n",
       "41            51  0.211046  [0.04958678]   0.085714  0.034884  0.212499   \n",
       "17            51  0.205121   [0.2420382]   0.267606  0.220930  0.206303   \n",
       "8             51  0.161227  [0.07894737]   0.090909  0.069767  0.164248   \n",
       "11            51  0.160221  [0.05128205]   0.057143  0.046512  0.160731   \n",
       "2             51  0.144740  [0.24305557]   0.142857  0.813953  0.145879   \n",
       "5             51  0.138901  [0.27241382]   0.159919  0.918605  0.141533   \n",
       "9            107  0.058215   [0.3130755]   0.185996  0.988372  0.082700   \n",
       "15           135  0.058856    [0.310219]   0.183983  0.988372  0.077710   \n",
       "51           123  0.058243  [0.31460673]   0.187500  0.976744  0.074944   \n",
       "54            62  0.056594   [0.3071298]   0.182213  0.976744  0.072302   \n",
       "49            86  0.053012  [0.30601093]   0.181425  0.976744  0.071025   \n",
       "12            85  0.053974  [0.29787236]   0.182292  0.813953  0.070507   \n",
       "63           114  0.058909  [0.31481484]   0.187225  0.988372  0.069456   \n",
       "6             71  0.052425   [0.3076923]   0.182609  0.976744  0.067845   \n",
       "10            80  0.051099  [0.31365314]   0.186404  0.988372  0.067451   \n",
       "13            79  0.052764   [0.3130755]   0.185996  0.988372  0.067188   \n",
       "36            78  0.052765  [0.30645162]   0.185366  0.883721  0.067138   \n",
       "1             96  0.051066  [0.30852994]   0.182796  0.988372  0.065722   \n",
       "28            73  0.053866   [0.3113553]   0.184783  0.988372  0.065676   \n",
       "42            70  0.053063  [0.30852994]   0.182796  0.988372  0.065666   \n",
       "43           123  0.050042  [0.33684212]   0.205656  0.930233  0.065242   \n",
       "40            84  0.051377   [0.3074074]   0.182819  0.965116  0.065140   \n",
       "25            79  0.053022  [0.30852994]   0.182796  0.988372  0.065133   \n",
       "37            82  0.051287  [0.30965394]   0.183585  0.988372  0.064740   \n",
       "39           167  0.055416  [0.31499052]   0.188209  0.965116  0.064701   \n",
       "60            68  0.055283  [0.30465952]   0.180085  0.988372  0.064496   \n",
       "48            72  0.054244   [0.3016158]   0.178344  0.976744  0.064161   \n",
       "0             93  0.051822   [0.2993763]   0.182278  0.837209  0.064037   \n",
       "57           133  0.056128  [0.31078613]   0.184382  0.988372  0.063238   \n",
       "27           109  0.057931    [0.310219]   0.183983  0.988372  0.063149   \n",
       "64            74  0.053538  [0.30852994]   0.182796  0.988372  0.062948   \n",
       "21           132  0.057899   [0.3074141]   0.182013  0.988372  0.062538   \n",
       "22            78  0.053746   [0.3082569]   0.183007  0.976744  0.062390   \n",
       "7             77  0.051429  [0.31078613]   0.184382  0.988372  0.062123   \n",
       "18            65  0.055866  [0.29893237]   0.176471  0.976744  0.061553   \n",
       "66            61  0.060581  [0.23255813]   0.145349  0.581395  0.061096   \n",
       "30            61  0.059805  [0.23193917]   0.138636  0.709302  0.060317   \n",
       "55            74  0.053564      [0.3125]   0.185590  0.988372  0.060310   \n",
       "52            79  0.053252  [0.30909094]   0.183190  0.988372  0.060119   \n",
       "45           170  0.054145  [0.34070793]   0.210383  0.895349  0.060108   \n",
       "58            78  0.053894   [0.3113553]   0.184783  0.988372  0.060024   \n",
       "4             97  0.051204  [0.30711612]   0.183036  0.953488  0.059571   \n",
       "24            66  0.055270  [0.30601093]   0.181425  0.976744  0.059556   \n",
       "3            158  0.054593    [0.317757]   0.189310  0.988372  0.059225   \n",
       "19            75  0.053079  [0.31078613]   0.184382  0.988372  0.059083   \n",
       "\n",
       "    val_fbeta_score  val_precision  val_recall activation_layer  ...  epochs  \\\n",
       "71         0.279720       0.165289    0.909091             relu  ...  100000   \n",
       "32         0.102564       0.117647    0.090909             relu  ...  100000   \n",
       "35         0.266667       0.159292    0.818182             relu  ...  100000   \n",
       "59         0.281690       0.166667    0.909091             relu  ...  100000   \n",
       "56         0.281879       0.165354    0.954545             relu  ...  100000   \n",
       "20         0.203390       0.162162    0.272727             relu  ...  100000   \n",
       "23         0.275362       0.163793    0.863636             relu  ...  100000   \n",
       "65         0.179775       0.119403    0.363636             relu  ...  100000   \n",
       "44         0.057143       0.076923    0.045455             relu  ...  100000   \n",
       "50         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "47         0.308824       0.184211    0.954545             relu  ...  100000   \n",
       "53         0.270270       0.158730    0.909091             relu  ...  100000   \n",
       "26         0.083333       0.500000    0.045455             relu  ...  100000   \n",
       "29         0.283333       0.173469    0.772727             relu  ...  100000   \n",
       "38         0.052632       0.062500    0.045455             relu  ...  100000   \n",
       "41         0.206897       0.428571    0.136364             relu  ...  100000   \n",
       "17         0.150000       0.166667    0.136364             relu  ...  100000   \n",
       "8          0.108108       0.133333    0.090909             relu  ...  100000   \n",
       "11         0.239316       0.147368    0.636364             relu  ...  100000   \n",
       "2          0.183486       0.114943    0.454545             relu  ...  100000   \n",
       "5          0.279720       0.165289    0.909091             relu  ...  100000   \n",
       "9          0.291971       0.173913    0.909091             relu  ...  100000   \n",
       "15         0.289855       0.172414    0.909091             relu  ...  100000   \n",
       "51         0.298507       0.178571    0.909091             relu  ...  100000   \n",
       "54         0.283688       0.168067    0.909091             relu  ...  100000   \n",
       "49         0.283688       0.168067    0.909091             relu  ...  100000   \n",
       "12         0.283688       0.168067    0.909091             relu  ...  100000   \n",
       "63         0.296296       0.176991    0.909091             relu  ...  100000   \n",
       "6          0.279720       0.165289    0.909091             relu  ...  100000   \n",
       "10         0.300752       0.180180    0.909091             relu  ...  100000   \n",
       "13         0.298507       0.178571    0.909091             relu  ...  100000   \n",
       "36         0.294118       0.175439    0.909091             relu  ...  100000   \n",
       "1          0.285714       0.169492    0.909091             relu  ...  100000   \n",
       "28         0.303030       0.181818    0.909091             relu  ...  100000   \n",
       "42         0.275862       0.162602    0.909091             relu  ...  100000   \n",
       "43         0.410256       0.470588    0.363636             relu  ...  100000   \n",
       "40         0.285714       0.169492    0.909091             relu  ...  100000   \n",
       "25         0.281690       0.166667    0.909091             relu  ...  100000   \n",
       "37         0.281690       0.166667    0.909091             relu  ...  100000   \n",
       "39         0.307692       0.202899    0.636364             relu  ...  100000   \n",
       "60         0.281690       0.166667    0.909091             relu  ...  100000   \n",
       "48         0.281690       0.166667    0.909091             relu  ...  100000   \n",
       "0          0.285714       0.169492    0.909091             relu  ...  100000   \n",
       "57         0.296296       0.176991    0.909091             relu  ...  100000   \n",
       "27         0.285714       0.169492    0.909091             relu  ...  100000   \n",
       "64         0.289855       0.172414    0.909091             relu  ...  100000   \n",
       "21         0.289855       0.172414    0.909091             relu  ...  100000   \n",
       "22         0.285714       0.169492    0.909091             relu  ...  100000   \n",
       "7          0.283688       0.168067    0.909091             relu  ...  100000   \n",
       "18         0.283688       0.168067    0.909091             relu  ...  100000   \n",
       "66         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "30         0.268293       0.154930    1.000000             relu  ...  100000   \n",
       "55         0.294118       0.175439    0.909091             relu  ...  100000   \n",
       "52         0.283688       0.168067    0.909091             relu  ...  100000   \n",
       "45         0.298507       0.178571    0.909091             relu  ...  100000   \n",
       "58         0.281690       0.166667    0.909091             relu  ...  100000   \n",
       "4          0.378378       0.466667    0.318182             relu  ...  100000   \n",
       "24         0.293706       0.173554    0.954545             relu  ...  100000   \n",
       "3          0.306122       0.197368    0.681818             relu  ...  100000   \n",
       "19         0.291971       0.173913    0.909091             relu  ...  100000   \n",
       "\n",
       "    first_neuron  gamma  hidden_layers  hidden_neuron  kernel_initializer  \\\n",
       "71           110    1.7              5            150          orthogonal   \n",
       "32            50    1.7              5            150          orthogonal   \n",
       "35            50    1.7              5            150          orthogonal   \n",
       "59           110    1.7              3            150          orthogonal   \n",
       "56           110    1.7              3            150          orthogonal   \n",
       "20            50    1.7              3            150          orthogonal   \n",
       "23            50    1.7              3            150          orthogonal   \n",
       "65           110    1.7              5             50          orthogonal   \n",
       "44           110    1.7              1            150          orthogonal   \n",
       "50           110    1.7              3             50          orthogonal   \n",
       "47           110    1.7              1            150          orthogonal   \n",
       "53           110    1.7              3             50          orthogonal   \n",
       "26            50    1.7              5             50          orthogonal   \n",
       "29            50    1.7              5             50          orthogonal   \n",
       "38           110    1.7              1             50          orthogonal   \n",
       "41           110    1.7              1             50          orthogonal   \n",
       "17            50    1.7              3             50          orthogonal   \n",
       "8             50    1.7              1            150          orthogonal   \n",
       "11            50    1.7              1            150          orthogonal   \n",
       "2             50    1.7              1             50          orthogonal   \n",
       "5             50    1.7              1             50          orthogonal   \n",
       "9             50    1.7              1            150          orthogonal   \n",
       "15            50    1.7              3             50          orthogonal   \n",
       "51           110    1.7              3             50          orthogonal   \n",
       "54           110    1.7              3            150          orthogonal   \n",
       "49           110    1.7              3             50          orthogonal   \n",
       "12            50    1.7              3             50          orthogonal   \n",
       "63           110    1.7              5             50          orthogonal   \n",
       "6             50    1.7              1            150          orthogonal   \n",
       "10            50    1.7              1            150          orthogonal   \n",
       "13            50    1.7              3             50          orthogonal   \n",
       "36           110    1.7              1             50          orthogonal   \n",
       "1             50    1.7              1             50          orthogonal   \n",
       "28            50    1.7              5             50          orthogonal   \n",
       "42           110    1.7              1            150          orthogonal   \n",
       "43           110    1.7              1            150          orthogonal   \n",
       "40           110    1.7              1             50          orthogonal   \n",
       "25            50    1.7              5             50          orthogonal   \n",
       "37           110    1.7              1             50          orthogonal   \n",
       "39           110    1.7              1             50          orthogonal   \n",
       "60           110    1.7              5             50          orthogonal   \n",
       "48           110    1.7              3             50          orthogonal   \n",
       "0             50    1.7              1             50          orthogonal   \n",
       "57           110    1.7              3            150          orthogonal   \n",
       "27            50    1.7              5             50          orthogonal   \n",
       "64           110    1.7              5             50          orthogonal   \n",
       "21            50    1.7              3            150          orthogonal   \n",
       "22            50    1.7              3            150          orthogonal   \n",
       "7             50    1.7              1            150          orthogonal   \n",
       "18            50    1.7              3            150          orthogonal   \n",
       "66           110    1.7              5            150          orthogonal   \n",
       "30            50    1.7              5            150          orthogonal   \n",
       "55           110    1.7              3            150          orthogonal   \n",
       "52           110    1.7              3             50          orthogonal   \n",
       "45           110    1.7              1            150          orthogonal   \n",
       "58           110    1.7              3            150          orthogonal   \n",
       "4             50    1.7              1             50          orthogonal   \n",
       "24            50    1.7              5             50          orthogonal   \n",
       "3             50    1.7              1             50          orthogonal   \n",
       "19            50    1.7              3            150          orthogonal   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2  last_activation  optimizer  \n",
       "71                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "32                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "35                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "59                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "56                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "20                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "23                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "65                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "44                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "50                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "47                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "53                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "26                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "29                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "38                 0.0001                 0.0001          sigmoid   adadelta  \n",
       "41                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "17                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "8                  0.0001                 0.0001          sigmoid   adadelta  \n",
       "11                 0.0001                 0.0001          sigmoid    adagrad  \n",
       "2                  0.0001                 0.0001          sigmoid   adadelta  \n",
       "5                  0.0001                 0.0001          sigmoid    adagrad  \n",
       "9                  0.0001                 0.0001          sigmoid     adamax  \n",
       "15                 0.0001                 0.0001          sigmoid     adamax  \n",
       "51                 0.0001                 0.0001          sigmoid     adamax  \n",
       "54                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "49                 0.0001                 0.0001          sigmoid       adam  \n",
       "12                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "63                 0.0001                 0.0001          sigmoid     adamax  \n",
       "6                  0.0001                 0.0001          sigmoid    rmsprop  \n",
       "10                 0.0001                 0.0001          sigmoid      nadam  \n",
       "13                 0.0001                 0.0001          sigmoid       adam  \n",
       "36                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "1                  0.0001                 0.0001          sigmoid       adam  \n",
       "28                 0.0001                 0.0001          sigmoid      nadam  \n",
       "42                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "43                 0.0001                 0.0001          sigmoid       adam  \n",
       "40                 0.0001                 0.0001          sigmoid      nadam  \n",
       "25                 0.0001                 0.0001          sigmoid       adam  \n",
       "37                 0.0001                 0.0001          sigmoid       adam  \n",
       "39                 0.0001                 0.0001          sigmoid     adamax  \n",
       "60                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "48                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "0                  0.0001                 0.0001          sigmoid    rmsprop  \n",
       "57                 0.0001                 0.0001          sigmoid     adamax  \n",
       "27                 0.0001                 0.0001          sigmoid     adamax  \n",
       "64                 0.0001                 0.0001          sigmoid      nadam  \n",
       "21                 0.0001                 0.0001          sigmoid     adamax  \n",
       "22                 0.0001                 0.0001          sigmoid      nadam  \n",
       "7                  0.0001                 0.0001          sigmoid       adam  \n",
       "18                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "66                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "30                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "55                 0.0001                 0.0001          sigmoid       adam  \n",
       "52                 0.0001                 0.0001          sigmoid      nadam  \n",
       "45                 0.0001                 0.0001          sigmoid     adamax  \n",
       "58                 0.0001                 0.0001          sigmoid      nadam  \n",
       "4                  0.0001                 0.0001          sigmoid      nadam  \n",
       "24                 0.0001                 0.0001          sigmoid    rmsprop  \n",
       "3                  0.0001                 0.0001          sigmoid     adamax  \n",
       "19                 0.0001                 0.0001          sigmoid       adam  \n",
       "\n",
       "[60 rows x 26 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea637ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

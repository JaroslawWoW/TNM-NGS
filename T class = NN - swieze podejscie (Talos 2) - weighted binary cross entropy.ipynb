{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6343fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import talos as ta\n",
    "from talos.model.early_stopper import early_stopper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext tensorboard\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11bc838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/Dane_do_uczenia_T.csv\", encoding=\"utf-8\")\n",
    "del train_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46afdada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000168779.18</th>\n",
       "      <th>ENSG00000043355.9</th>\n",
       "      <th>ENSG00000139800.8</th>\n",
       "      <th>ENSG00000095752.5</th>\n",
       "      <th>ENSG00000114270.14</th>\n",
       "      <th>ENSG00000143320.7</th>\n",
       "      <th>ENSG00000104112.7</th>\n",
       "      <th>ENSG00000168703.5</th>\n",
       "      <th>ENSG00000116176.6</th>\n",
       "      <th>ENSG00000188523.7</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000104755.13</th>\n",
       "      <th>ENSG00000250256.1</th>\n",
       "      <th>ENSG00000259447.1</th>\n",
       "      <th>ENSG00000235864.1</th>\n",
       "      <th>ENSG00000227718.1</th>\n",
       "      <th>ENSG00000229370.1</th>\n",
       "      <th>ENSG00000254434.1</th>\n",
       "      <th>ENSG00000233146.1</th>\n",
       "      <th>ENSG00000251969.3</th>\n",
       "      <th>scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.418540</td>\n",
       "      <td>2.139513</td>\n",
       "      <td>2.139513</td>\n",
       "      <td>9.984396</td>\n",
       "      <td>139.068370</td>\n",
       "      <td>134.076172</td>\n",
       "      <td>11.410738</td>\n",
       "      <td>67.038086</td>\n",
       "      <td>173.300584</td>\n",
       "      <td>16.402936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.426342</td>\n",
       "      <td>0.713171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.254904</td>\n",
       "      <td>0.850981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850981</td>\n",
       "      <td>45.101982</td>\n",
       "      <td>22.125501</td>\n",
       "      <td>84.247098</td>\n",
       "      <td>1.701962</td>\n",
       "      <td>11.913731</td>\n",
       "      <td>5.956866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.850981</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.931528</td>\n",
       "      <td>0.643843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.562078</td>\n",
       "      <td>452.621281</td>\n",
       "      <td>237.577884</td>\n",
       "      <td>6.438425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.096063</td>\n",
       "      <td>3.219213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.643843</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.870129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>123.558325</td>\n",
       "      <td>773.544728</td>\n",
       "      <td>262.778974</td>\n",
       "      <td>15.662323</td>\n",
       "      <td>2.610387</td>\n",
       "      <td>70.480453</td>\n",
       "      <td>1.740258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870129</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.718041</td>\n",
       "      <td>2.718041</td>\n",
       "      <td>2.718041</td>\n",
       "      <td>39.411591</td>\n",
       "      <td>145.415179</td>\n",
       "      <td>88.336324</td>\n",
       "      <td>19.026285</td>\n",
       "      <td>1167.398495</td>\n",
       "      <td>16.308244</td>\n",
       "      <td>16.308244</td>\n",
       "      <td>...</td>\n",
       "      <td>1.359020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.359020</td>\n",
       "      <td>1.35902</td>\n",
       "      <td>1.35902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000168779.18  ENSG00000043355.9  ENSG00000139800.8  \\\n",
       "0            6.418540           2.139513           2.139513   \n",
       "1            4.254904           0.850981           0.000000   \n",
       "2            1.931528           0.643843           0.000000   \n",
       "3            0.870129           0.000000           0.000000   \n",
       "4            2.718041           2.718041           2.718041   \n",
       "\n",
       "   ENSG00000095752.5  ENSG00000114270.14  ENSG00000143320.7  \\\n",
       "0           9.984396          139.068370         134.076172   \n",
       "1           0.850981           45.101982          22.125501   \n",
       "2          40.562078          452.621281         237.577884   \n",
       "3         123.558325          773.544728         262.778974   \n",
       "4          39.411591          145.415179          88.336324   \n",
       "\n",
       "   ENSG00000104112.7  ENSG00000168703.5  ENSG00000116176.6  ENSG00000188523.7  \\\n",
       "0          11.410738          67.038086         173.300584          16.402936   \n",
       "1          84.247098           1.701962          11.913731           5.956866   \n",
       "2           6.438425           0.000000          16.096063           3.219213   \n",
       "3          15.662323           2.610387          70.480453           1.740258   \n",
       "4          19.026285        1167.398495          16.308244          16.308244   \n",
       "\n",
       "   ...  ENSG00000104755.13  ENSG00000250256.1  ENSG00000259447.1  \\\n",
       "0  ...            0.000000                0.0           0.000000   \n",
       "1  ...            0.850981                0.0           0.850981   \n",
       "2  ...            0.000000                0.0           0.643843   \n",
       "3  ...            0.000000                0.0           0.000000   \n",
       "4  ...            1.359020                0.0           1.359020   \n",
       "\n",
       "   ENSG00000235864.1  ENSG00000227718.1  ENSG00000229370.1  ENSG00000254434.1  \\\n",
       "0            0.00000            0.00000                0.0           1.426342   \n",
       "1            0.00000            0.00000                0.0           0.000000   \n",
       "2            0.00000            0.00000                0.0           0.000000   \n",
       "3            0.00000            0.00000                0.0           0.000000   \n",
       "4            1.35902            1.35902                0.0           0.000000   \n",
       "\n",
       "   ENSG00000233146.1  ENSG00000251969.3  scale  \n",
       "0           0.713171           0.000000     T1  \n",
       "1           0.000000           0.000000     T1  \n",
       "2           0.000000           0.000000     T1  \n",
       "3           0.000000           0.870129     T1  \n",
       "4           0.000000           0.000000     T1  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc5ff62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T1    496\n",
       "T3    327\n",
       "T2    132\n",
       "Name: scale, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['scale'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12749254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['scale'].loc[(train_df['scale'] == 'T1')] = 0\n",
    "train_df['scale'].loc[(train_df['scale'] == 'T2')] = 1\n",
    "train_df['scale'].loc[(train_df['scale'] == 'T3')] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70af0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42,stratify=train_df['scale'])\n",
    "#test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42,stratify=test_df['scale'])\n",
    "\n",
    "\n",
    "train_label=train_df['scale']\n",
    "test_label=test_df['scale']\n",
    "#val_label=val_df['scale']\n",
    "del train_df['scale']\n",
    "del test_df['scale']\n",
    "#del val_df['scale']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a11fa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96bf7b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43830d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d496a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c166a",
   "metadata": {},
   "source": [
    "## 1.2 Standaryzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eafe28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df=scaler.fit_transform(train_df)\n",
    "test_df=scaler.fit_transform(test_df)\n",
    "#val_df=scaler.fit_transform(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24251a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03296043, -0.17404131, -0.22961697, ..., -0.18492773,\n",
       "        -0.20587262, -0.13905207],\n",
       "       [-0.09799013, -0.20838135, -0.22961697, ..., -0.18492773,\n",
       "        -0.20587262, -0.13905207],\n",
       "       [-0.08505218, -0.13459551,  0.03327218, ..., -0.12028205,\n",
       "        -0.11752968, -0.13905207],\n",
       "       ...,\n",
       "       [-0.19643114, -0.1944861 , -0.22961697, ..., -0.18492773,\n",
       "        -0.20587262, -0.13905207],\n",
       "       [-0.17516001, -0.22062863, -0.22961697, ..., -0.18492773,\n",
       "        -0.20587262, -0.13905207],\n",
       "       [-0.00979766, -0.22062863, -0.22961697, ..., -0.18492773,\n",
       "        -0.20587262, -0.13905207]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a14023e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 105)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "510b4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=np.asarray(train_label).astype(np.int)\n",
    "test_label=np.asarray(test_label).astype(np.int)\n",
    "#val_label=np.asarray(val_label).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e2e8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_label)\n",
    "train_label = encoder.transform(train_label)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "\n",
    "train_label = np_utils.to_categorical(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3440fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_label)\n",
    "test_label = encoder.transform(test_label)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "\n",
    "test_label = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26659d2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d98957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2422e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67700d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb32e8a",
   "metadata": {},
   "source": [
    "# 2 Moduł TALOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e057c",
   "metadata": {},
   "source": [
    "## 2.1 Słownik parametrów do wypróbowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d970ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dobor Gammy i alfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "734e1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55,110,220], \n",
    "     'hidden_neuron':[50,100,150],\n",
    "\n",
    "     'hidden_layers':[3,5,7,9,12],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['softmax'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [32],\n",
    "\n",
    "    'lr':[0.001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "#    'batc_normalization':[False,True],\n",
    "#    'dropout': [0,0.2,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "    'optimizer': ['rmsprop','adam','adadelta','adamax','nadam','adagrad'],\n",
    "#    'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "    'kernel_initializer': ['orthogonal'],\n",
    "   # 'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    "  #  'activation_layer':['relu'],\n",
    "    'batc_normalization':[False,True]\n",
    " #   'batc_normalization':[False],\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e94d6",
   "metadata": {},
   "source": [
    "## 2.22 Weights for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a569ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weights for each class so that we can balance the data\n",
    "weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                            classes=np.unique(train_label),\n",
    "                                            y=train_label.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d6a8e",
   "metadata": {},
   "source": [
    "weights = array([0.64147775, 2.42539683, 0.97201018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d5003a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(3, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=params['optimizer'],\n",
    "                  #tf.keras.optimizers.Adamax(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=3,beta=1.0)])\n",
    "    \n",
    "  \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        class_weight={0 : 0.64147775,\n",
    "                                      1 : 2.42539683,\n",
    "                                      2 : 0.97201018},\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1883c",
   "metadata": {},
   "source": [
    "## 2.3 Przeprowadzam skan, używając parametrów i funkcji wyżej\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d5b15",
   "metadata": {},
   "source": [
    "##  3. Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78cd5c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBD481DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████▏   | 514/540 [2:27:35<08:33, 19.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 9, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFF54708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC45A6828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████▍   | 515/540 [2:27:56<08:22, 20.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 9, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED12EC1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED288AB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████▌   | 516/540 [2:28:11<07:22, 18.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 9, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC18F2798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC8665168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 234.\n",
      "Epoch 00284: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████▋   | 517/540 [2:29:21<13:03, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 9, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFCBFEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3E8D798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 221.\n",
      "Epoch 00271: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████▊   | 518/540 [2:30:26<15:51, 43.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 9, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED282E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC858D0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████▉   | 519/540 [2:30:43<12:24, 35.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 9, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECD9FDE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECFCBFA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Epoch 00071: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████   | 520/540 [2:31:05<10:27, 31.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 9, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC858D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED282E1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 233.\n",
      "Epoch 00283: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████▎  | 521/540 [2:32:28<14:49, 46.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 9, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBAD3AF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECFCBF9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████▍  | 522/540 [2:32:46<11:25, 38.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC43A0438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED9976678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "Epoch 00114: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████▌  | 523/540 [2:33:15<10:03, 35.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6EFC828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED72D6678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 00088: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████▋  | 524/540 [2:33:39<08:29, 31.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3C28F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED0115318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████▊  | 525/540 [2:33:55<06:46, 27.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED12ECDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6D56798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 146.\n",
      "Epoch 00196: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████▉  | 526/540 [2:34:32<07:01, 30.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED2D26678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECDB41C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████  | 527/540 [2:34:57<06:13, 28.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB18BC0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECA7B9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████▎ | 528/540 [2:35:13<04:57, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED25D05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED41A1708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 184.\n",
      "Epoch 00234: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████▍ | 529/540 [2:36:09<06:15, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED148E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED25AF558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 328.\n",
      "Epoch 00378: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████▌ | 530/540 [2:37:31<08:03, 48.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECBA0CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED41A11F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████▋ | 531/540 [2:37:50<05:56, 39.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6EFC828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECA797C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 104.\n",
      "Epoch 00154: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|████████████████████████████████████████████████████████████████████████████▊ | 532/540 [2:38:28<05:14, 39.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC43A0168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC407BF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 268.\n",
      "Epoch 00318: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|████████████████████████████████████████████████████████████████████████████▉ | 533/540 [2:39:53<06:10, 52.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED143E4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED12754C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████████████████████████████████████████▏| 534/540 [2:40:11<04:13, 42.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'rmsprop'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED9976678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECFD58948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 232.\n",
      "Epoch 00282: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████████████████████████████████████████▎| 535/540 [2:41:35<04:34, 54.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED25D0828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED280D9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 320.\n",
      "Epoch 00370: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████████████████████████████████████████▍| 536/540 [2:43:24<04:45, 71.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adadelta'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC58FBCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC6FCF1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████████████████████████████████████████▌| 537/540 [2:43:48<02:50, 56.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adamax'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED67D9048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2EB2EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 101.\n",
      "Epoch 00151: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████▋| 538/540 [2:44:36<01:48, 54.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'nadam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECDB6EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED01A4EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 255.\n",
      "Epoch 00305: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████▊| 539/540 [2:46:29<01:11, 71.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 220, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001, 'optimizer': 'adagrad'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED01A40D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC8609828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 540/540 [2:46:51<00:00, 18.54s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "028ef827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/050922120301.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de6b1f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>0.237748</td>\n",
       "      <td>[0.95345914 0.89285713 0.92730844]</td>\n",
       "      <td>1.584663</td>\n",
       "      <td>[0.73732716 0.35714287 0.45871562]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>0.222430</td>\n",
       "      <td>[0.9723618  0.93636364 0.9609375 ]</td>\n",
       "      <td>1.654442</td>\n",
       "      <td>[0.69483566 0.25925925 0.45217392]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>0.861435</td>\n",
       "      <td>[0.00493827 0.24157955 0.        ]</td>\n",
       "      <td>0.881551</td>\n",
       "      <td>[0.01960784 0.24186045 0.        ]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>0.383558</td>\n",
       "      <td>[0.8894536 0.7673469 0.8548387]</td>\n",
       "      <td>0.975225</td>\n",
       "      <td>[0.75490195 0.35714287 0.59016395]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>0.212719</td>\n",
       "      <td>[0.9708492 0.8849558 0.9551657]</td>\n",
       "      <td>1.558193</td>\n",
       "      <td>[0.7272727  0.36065575 0.47524753]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>370</td>\n",
       "      <td>0.610296</td>\n",
       "      <td>[0.9236739  0.90990996 0.90431523]</td>\n",
       "      <td>1.302908</td>\n",
       "      <td>[0.6934673  0.28571427 0.49253735]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>53</td>\n",
       "      <td>3.010354</td>\n",
       "      <td>[0.40305346 0.21203437 0.34351146]</td>\n",
       "      <td>3.128910</td>\n",
       "      <td>[0.5116279 0.25      0.3508772]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>151</td>\n",
       "      <td>2.033046</td>\n",
       "      <td>[0.9321382  0.87826085 0.9168278 ]</td>\n",
       "      <td>2.672759</td>\n",
       "      <td>[0.7853881  0.27272728 0.5546218 ]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>305</td>\n",
       "      <td>0.709694</td>\n",
       "      <td>[0.9081501  0.84210527 0.8728652 ]</td>\n",
       "      <td>1.335972</td>\n",
       "      <td>[0.6703297  0.26666668 0.51428574]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>51</td>\n",
       "      <td>2.884065</td>\n",
       "      <td>[0.4977645 0.2793296 0.4248497]</td>\n",
       "      <td>2.965478</td>\n",
       "      <td>[0.5368421  0.22535212 0.36363637]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "0              56  0.237748  [0.95345914 0.89285713 0.92730844]  1.584663   \n",
       "1              55  0.222430  [0.9723618  0.93636364 0.9609375 ]  1.654442   \n",
       "2              51  0.861435  [0.00493827 0.24157955 0.        ]  0.881551   \n",
       "3              62  0.383558     [0.8894536 0.7673469 0.8548387]  0.975225   \n",
       "4              56  0.212719     [0.9708492 0.8849558 0.9551657]  1.558193   \n",
       "..            ...       ...                                 ...       ...   \n",
       "535           370  0.610296  [0.9236739  0.90990996 0.90431523]  1.302908   \n",
       "536            53  3.010354  [0.40305346 0.21203437 0.34351146]  3.128910   \n",
       "537           151  2.033046  [0.9321382  0.87826085 0.9168278 ]  2.672759   \n",
       "538           305  0.709694  [0.9081501  0.84210527 0.8728652 ]  1.335972   \n",
       "539            51  2.884065     [0.4977645 0.2793296 0.4248497]  2.965478   \n",
       "\n",
       "                        val_fbeta_score  activity_regularizer  \\\n",
       "0    [0.73732716 0.35714287 0.45871562]                0.0001   \n",
       "1    [0.69483566 0.25925925 0.45217392]                0.0001   \n",
       "2    [0.01960784 0.24186045 0.        ]                0.0001   \n",
       "3    [0.75490195 0.35714287 0.59016395]                0.0001   \n",
       "4    [0.7272727  0.36065575 0.47524753]                0.0001   \n",
       "..                                  ...                   ...   \n",
       "535  [0.6934673  0.28571427 0.49253735]                0.0001   \n",
       "536     [0.5116279 0.25      0.3508772]                0.0001   \n",
       "537  [0.7853881  0.27272728 0.5546218 ]                0.0001   \n",
       "538  [0.6703297  0.26666668 0.51428574]                0.0001   \n",
       "539  [0.5368421  0.22535212 0.36363637]                0.0001   \n",
       "\n",
       "     batc_normalization  batch_size  bias_regularizer  dropout  epochs  \\\n",
       "0                 False          32            0.0001        0  100000   \n",
       "1                 False          32            0.0001        0  100000   \n",
       "2                 False          32            0.0001        0  100000   \n",
       "3                 False          32            0.0001        0  100000   \n",
       "4                 False          32            0.0001        0  100000   \n",
       "..                  ...         ...               ...      ...     ...   \n",
       "535                True          32            0.0001        0  100000   \n",
       "536                True          32            0.0001        0  100000   \n",
       "537                True          32            0.0001        0  100000   \n",
       "538                True          32            0.0001        0  100000   \n",
       "539                True          32            0.0001        0  100000   \n",
       "\n",
       "     first_neuron  hidden_layers  hidden_neuron kernel_initializer  \\\n",
       "0              55              3             50         orthogonal   \n",
       "1              55              3             50         orthogonal   \n",
       "2              55              3             50         orthogonal   \n",
       "3              55              3             50         orthogonal   \n",
       "4              55              3             50         orthogonal   \n",
       "..            ...            ...            ...                ...   \n",
       "535           220             12            150         orthogonal   \n",
       "536           220             12            150         orthogonal   \n",
       "537           220             12            150         orthogonal   \n",
       "538           220             12            150         orthogonal   \n",
       "539           220             12            150         orthogonal   \n",
       "\n",
       "     kernel_regularizer_l1  kernel_regularizer_l2 last_activation     lr  \\\n",
       "0                   0.0001                 0.0001         softmax  0.001   \n",
       "1                   0.0001                 0.0001         softmax  0.001   \n",
       "2                   0.0001                 0.0001         softmax  0.001   \n",
       "3                   0.0001                 0.0001         softmax  0.001   \n",
       "4                   0.0001                 0.0001         softmax  0.001   \n",
       "..                     ...                    ...             ...    ...   \n",
       "535                 0.0001                 0.0001         softmax  0.001   \n",
       "536                 0.0001                 0.0001         softmax  0.001   \n",
       "537                 0.0001                 0.0001         softmax  0.001   \n",
       "538                 0.0001                 0.0001         softmax  0.001   \n",
       "539                 0.0001                 0.0001         softmax  0.001   \n",
       "\n",
       "    optimizer  \n",
       "0     rmsprop  \n",
       "1        adam  \n",
       "2    adadelta  \n",
       "3      adamax  \n",
       "4       nadam  \n",
       "..        ...  \n",
       "535      adam  \n",
       "536  adadelta  \n",
       "537    adamax  \n",
       "538     nadam  \n",
       "539   adagrad  \n",
       "\n",
       "[540 rows x 20 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da430680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03efbcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['round_epochs', 'loss', 'fbeta_score', 'val_loss', 'val_fbeta_score',\n",
       "       'activity_regularizer', 'batc_normalization', 'batch_size',\n",
       "       'bias_regularizer', 'dropout', 'epochs', 'first_neuron',\n",
       "       'hidden_layers', 'hidden_neuron', 'kernel_initializer',\n",
       "       'kernel_regularizer_l1', 'kernel_regularizer_l2', 'last_activation',\n",
       "       'lr', 'optimizer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "460f8c24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>83</td>\n",
       "      <td>0.640290</td>\n",
       "      <td>[0.6403756  0.         0.17877094]</td>\n",
       "      <td>0.638750</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>67</td>\n",
       "      <td>0.641137</td>\n",
       "      <td>[0.42917252 0.16603774 0.31636366]</td>\n",
       "      <td>0.638828</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>86</td>\n",
       "      <td>0.639362</td>\n",
       "      <td>[0.18661258 0.20654045 0.2290749 ]</td>\n",
       "      <td>0.639512</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>107</td>\n",
       "      <td>0.639984</td>\n",
       "      <td>[0.6838932 0.        0.       ]</td>\n",
       "      <td>0.639708</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>76</td>\n",
       "      <td>0.640387</td>\n",
       "      <td>[0.13129103 0.12075472 0.46401986]</td>\n",
       "      <td>0.640078</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>51</td>\n",
       "      <td>2.752526</td>\n",
       "      <td>[0.5534407  0.27906978 0.45109782]</td>\n",
       "      <td>2.812747</td>\n",
       "      <td>[0.5842697  0.26966292 0.38260868]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>51</td>\n",
       "      <td>2.762955</td>\n",
       "      <td>[0.40364188 0.23163842 0.3339806 ]</td>\n",
       "      <td>2.856422</td>\n",
       "      <td>[0.23611109 0.16494846 0.35460994]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>51</td>\n",
       "      <td>2.907009</td>\n",
       "      <td>[0.39502332 0.18994415 0.31878555]</td>\n",
       "      <td>2.956668</td>\n",
       "      <td>[0.25       0.22222221 0.40000004]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>110</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>51</td>\n",
       "      <td>2.884065</td>\n",
       "      <td>[0.4977645 0.2793296 0.4248497]</td>\n",
       "      <td>2.965478</td>\n",
       "      <td>[0.5368421  0.22535212 0.36363637]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>53</td>\n",
       "      <td>3.010354</td>\n",
       "      <td>[0.40305346 0.21203437 0.34351146]</td>\n",
       "      <td>3.128910</td>\n",
       "      <td>[0.5116279 0.25      0.3508772]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>220</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adadelta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "79             83  0.640290  [0.6403756  0.         0.17877094]  0.638750   \n",
       "162            67  0.641137  [0.42917252 0.16603774 0.31636366]  0.638828   \n",
       "163            86  0.639362  [0.18661258 0.20654045 0.2290749 ]  0.639512   \n",
       "169           107  0.639984     [0.6838932 0.        0.       ]  0.639708   \n",
       "259            76  0.640387  [0.13129103 0.12075472 0.46401986]  0.640078   \n",
       "..            ...       ...                                 ...       ...   \n",
       "449            51  2.752526  [0.5534407  0.27906978 0.45109782]  2.812747   \n",
       "356            51  2.762955  [0.40364188 0.23163842 0.3339806 ]  2.856422   \n",
       "446            51  2.907009  [0.39502332 0.18994415 0.31878555]  2.956668   \n",
       "539            51  2.884065     [0.4977645 0.2793296 0.4248497]  2.965478   \n",
       "536            53  3.010354  [0.40305346 0.21203437 0.34351146]  3.128910   \n",
       "\n",
       "                        val_fbeta_score  activity_regularizer  \\\n",
       "79      [0.6827586 0.        0.       ]                0.0001   \n",
       "162     [0.6827586 0.        0.       ]                0.0001   \n",
       "163  [0.         0.24770641 0.        ]                0.0001   \n",
       "169     [0.6827586 0.        0.       ]                0.0001   \n",
       "259     [0.6827586 0.        0.       ]                0.0001   \n",
       "..                                  ...                   ...   \n",
       "449  [0.5842697  0.26966292 0.38260868]                0.0001   \n",
       "356  [0.23611109 0.16494846 0.35460994]                0.0001   \n",
       "446  [0.25       0.22222221 0.40000004]                0.0001   \n",
       "539  [0.5368421  0.22535212 0.36363637]                0.0001   \n",
       "536     [0.5116279 0.25      0.3508772]                0.0001   \n",
       "\n",
       "     batc_normalization  batch_size  bias_regularizer  dropout  epochs  \\\n",
       "79                False          32            0.0001        0  100000   \n",
       "162               False          32            0.0001        0  100000   \n",
       "163               False          32            0.0001        0  100000   \n",
       "169               False          32            0.0001        0  100000   \n",
       "259               False          32            0.0001        0  100000   \n",
       "..                  ...         ...               ...      ...     ...   \n",
       "449                True          32            0.0001        0  100000   \n",
       "356                True          32            0.0001        0  100000   \n",
       "446                True          32            0.0001        0  100000   \n",
       "539                True          32            0.0001        0  100000   \n",
       "536                True          32            0.0001        0  100000   \n",
       "\n",
       "     first_neuron  hidden_layers  hidden_neuron kernel_initializer  \\\n",
       "79             55             12            100         orthogonal   \n",
       "162           110             12             50         orthogonal   \n",
       "163           110             12             50         orthogonal   \n",
       "169           110             12            100         orthogonal   \n",
       "259           220             12            100         orthogonal   \n",
       "..            ...            ...            ...                ...   \n",
       "449           110             12            150         orthogonal   \n",
       "356            55             12            150         orthogonal   \n",
       "446           110             12            150         orthogonal   \n",
       "539           220             12            150         orthogonal   \n",
       "536           220             12            150         orthogonal   \n",
       "\n",
       "     kernel_regularizer_l1  kernel_regularizer_l2 last_activation     lr  \\\n",
       "79                  0.0001                 0.0001         softmax  0.001   \n",
       "162                 0.0001                 0.0001         softmax  0.001   \n",
       "163                 0.0001                 0.0001         softmax  0.001   \n",
       "169                 0.0001                 0.0001         softmax  0.001   \n",
       "259                 0.0001                 0.0001         softmax  0.001   \n",
       "..                     ...                    ...             ...    ...   \n",
       "449                 0.0001                 0.0001         softmax  0.001   \n",
       "356                 0.0001                 0.0001         softmax  0.001   \n",
       "446                 0.0001                 0.0001         softmax  0.001   \n",
       "539                 0.0001                 0.0001         softmax  0.001   \n",
       "536                 0.0001                 0.0001         softmax  0.001   \n",
       "\n",
       "    optimizer  \n",
       "79       adam  \n",
       "162   rmsprop  \n",
       "163      adam  \n",
       "169      adam  \n",
       "259      adam  \n",
       "..        ...  \n",
       "449   adagrad  \n",
       "356  adadelta  \n",
       "446  adadelta  \n",
       "539   adagrad  \n",
       "536  adadelta  \n",
       "\n",
       "[540 rows x 20 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48b694c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of first_neuron')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'first_neuron'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b74d011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_neuron')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_neuron'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57ea1ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_layers')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_layers'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "473f441e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of optimizer')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'optimizer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cc096ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batc_normalization')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batc_normalization'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce773f",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63abab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(3, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adagrad(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=3,beta=1.0)])\n",
    "    \n",
    "  \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        class_weight={0 : 0.64147775,\n",
    "                                      1 : 2.42539683,\n",
    "                                      2 : 0.97201018},\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cec7da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55,330,440], \n",
    "     'hidden_neuron':[50,100,150],\n",
    "\n",
    "     'hidden_layers':[1,3,6,12],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['softmax'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [32],\n",
    "\n",
    "    'lr':[0.001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "#    'batc_normalization':[False,True],\n",
    "#    'dropout': [0,0.2,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "  #  'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "'kernel_initializer': ['orthogonal'],\n",
    "   \n",
    "    'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    "  #  'activation_layer':['relu'],\n",
    "    'batc_normalization':[False,True]\n",
    " #   'batc_normalization':[False],\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f622789c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 2083.\n",
      "Epoch 02133: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████▏                                                   | 108/360 [1:35:13<11:32:47, 164.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBC3A30D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB85D0F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 149.\n",
      "Epoch 00199: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████▋                                                    | 109/360 [1:35:25<8:18:41, 119.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6B5E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED27E1D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 171.\n",
      "Epoch 00221: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████▏                                                    | 110/360 [1:35:40<6:06:06, 87.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB6FEC048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE11BB318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 161.\n",
      "Epoch 00211: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████▍                                                    | 111/360 [1:35:54<4:32:52, 65.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED01A4B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB981F1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████▋                                                    | 112/360 [1:36:01<3:19:14, 48.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB85D05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB45F8318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "Epoch 00150: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████▊                                                    | 113/360 [1:36:17<2:37:55, 38.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB1866EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB6EC84C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 125.\n",
      "Epoch 00175: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████                                                    | 114/360 [1:36:38<2:16:27, 33.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECA797DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC8609D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████▎                                                   | 115/360 [1:36:48<1:47:23, 26.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED7299E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED54515E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████▍                                                   | 116/360 [1:37:00<1:29:07, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB737AC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED54510D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████▋                                                   | 117/360 [1:37:13<1:17:53, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED143E5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE22E68B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|████████████████████████▉                                                   | 118/360 [1:37:28<1:12:26, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB981F828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3A57708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████████████████                                                   | 119/360 [1:37:45<1:11:00, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC8609F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE264DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████████████████▎                                                  | 120/360 [1:38:06<1:15:04, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFD97168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3F81A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 126.\n",
      "Epoch 00176: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|█████████████████████████▌                                                  | 121/360 [1:38:24<1:14:00, 18.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB6FEC438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB58B4CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 166.\n",
      "Epoch 00216: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|█████████████████████████▊                                                  | 122/360 [1:38:47<1:19:00, 19.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED99251F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED539E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 93.\n",
      "Epoch 00143: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|█████████████████████████▉                                                  | 123/360 [1:39:04<1:15:08, 19.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECA8C0DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2C88048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 00110: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|██████████████████████████▏                                                 | 124/360 [1:39:19<1:09:41, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECA8FEE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBADA84C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 126.\n",
      "Epoch 00176: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████████████████▍                                                 | 125/360 [1:39:42<1:16:09, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE0E470D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBADA89D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 140.\n",
      "Epoch 00190: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████████████████▌                                                 | 126/360 [1:40:12<1:27:33, 22.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB6EC8798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED27E1AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████████████████▊                                                 | 127/360 [1:40:23<1:13:58, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB6FEC0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE18413A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 160.\n",
      "Epoch 00210: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████████████████                                                 | 128/360 [1:40:58<1:31:58, 23.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB737A168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1841A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████████████████▏                                                | 129/360 [1:41:13<1:21:17, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED70E6558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1831948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████████████████▍                                                | 130/360 [1:41:30<1:16:52, 20.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3AC0318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE21CE8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████████████████▋                                                | 131/360 [1:41:50<1:15:54, 19.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC01F2B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED96A34C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███████████████████████████▊                                                | 132/360 [1:42:13<1:19:24, 20.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC6FCF678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBD6040D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Epoch 00092: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|████████████████████████████                                                | 133/360 [1:42:25<1:08:39, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED7299828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECFD584C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 103.\n",
      "Epoch 00153: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|████████████████████████████▎                                               | 134/360 [1:42:42<1:07:44, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBC3A3048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB6B5F5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 120.\n",
      "Epoch 00170: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████▌                                               | 135/360 [1:43:03<1:09:58, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB30F60D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED9475DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 126.\n",
      "Epoch 00176: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████▋                                               | 136/360 [1:43:23<1:11:11, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBAE9B678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBAEE1318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 130.\n",
      "Epoch 00180: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████▉                                               | 137/360 [1:43:44<1:13:52, 19.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED40E4438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1B57048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 79.\n",
      "Epoch 00129: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████▏                                              | 138/360 [1:44:03<1:12:07, 19.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFECE048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB1907AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████▎                                              | 139/360 [1:44:13<1:01:00, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3CB1948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBADA8AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|██████████████████████████████▎                                               | 140/360 [1:44:24<54:52, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB737A558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3CB1C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|██████████████████████████████▌                                               | 141/360 [1:44:37<52:41, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFECE5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE0F55318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|██████████████████████████████▊                                               | 142/360 [1:44:52<52:43, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED72991F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1FDD948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 00058: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████▉                                               | 143/360 [1:45:09<55:53, 15.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB981F1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE26E8CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████▍                                             | 144/360 [1:45:29<1:00:35, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE11BB558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC85EA4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 186.\n",
      "Epoch 00236: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████▍                                              | 145/360 [1:45:42<55:58, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB477BF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC29CC678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 247.\n",
      "Epoch 00297: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████▋                                              | 146/360 [1:45:58<55:55, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC8609DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6B5E168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 315.\n",
      "Epoch 00365: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████▊                                              | 147/360 [1:46:18<59:45, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3DAA3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE24FCA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 390.\n",
      "Epoch 00440: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████▏                                            | 148/360 [1:46:42<1:07:01, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE24FC048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3AC0C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 341.\n",
      "Epoch 00391: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████▍                                            | 149/360 [1:47:06<1:12:11, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBD8EDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3CB1F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 369.\n",
      "Epoch 00419: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|███████████████████████████████▋                                            | 150/360 [1:47:37<1:23:37, 23.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3DA68B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3CB1168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 94.\n",
      "Epoch 00144: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|███████████████████████████████▉                                            | 151/360 [1:47:48<1:09:17, 19.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC0390E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB477B1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 124.\n",
      "Epoch 00174: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████                                            | 152/360 [1:48:03<1:03:44, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED9925048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3DAA948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 525.\n",
      "Epoch 00575: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████▎                                           | 153/360 [1:49:04<1:47:11, 31.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED65F23A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED98CB3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 142.\n",
      "Epoch 00192: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████▌                                           | 154/360 [1:49:21<1:32:25, 26.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB6D4FA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE61DAAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 228.\n",
      "Epoch 00278: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████▋                                           | 155/360 [1:49:51<1:35:49, 28.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFECE3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3DAA4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1747.\n",
      "Epoch 01797: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████▌                                          | 156/360 [1:54:48<6:09:03, 108.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3DAA048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED40E44C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 200.\n",
      "Epoch 00250: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████████████████▏                                          | 157/360 [1:55:05<4:34:43, 81.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EAB0D14C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2CD8438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 249.\n",
      "Epoch 00299: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████████████████▎                                          | 158/360 [1:55:27<3:33:04, 63.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC8609EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBAEE13A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 228.\n",
      "Epoch 00278: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████████████████▌                                          | 159/360 [1:55:48<2:49:32, 50.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED2AE0558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED94754C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 250.\n",
      "Epoch 00300: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████████████████▊                                          | 160/360 [1:56:10<2:19:56, 41.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBD604678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB737AB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 305.\n",
      "Epoch 00355: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████▉                                          | 161/360 [1:56:39<2:06:13, 38.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6B5E798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBD604558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 323.\n",
      "Epoch 00373: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████████████████████▏                                         | 162/360 [1:57:15<2:03:55, 37.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC85EA318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFC098B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 113.\n",
      "Epoch 00163: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████████████████████▍                                         | 163/360 [1:57:29<1:40:22, 30.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB85D0558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFD97F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 101.\n",
      "Epoch 00151: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|██████████████████████████████████▌                                         | 164/360 [1:57:45<1:25:36, 26.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC718A828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED98CB798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 512.\n",
      "Epoch 00562: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|██████████████████████████████████▊                                         | 165/360 [1:58:56<2:09:09, 39.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFC094C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB45F8A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 202.\n",
      "Epoch 00252: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████████████████████                                         | 166/360 [1:59:21<1:53:55, 35.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC6FCFB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE18418B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 221.\n",
      "Epoch 00271: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████████████████████▎                                        | 167/360 [1:59:57<1:53:30, 35.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBC3A30D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECFECEF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 2713.\n",
      "Epoch 02763: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████                                        | 168/360 [2:08:10<9:12:33, 172.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED2AE0048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED40E4A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 218.\n",
      "Epoch 00268: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████▏                                       | 169/360 [2:08:27<6:41:16, 126.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1841CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE61DAC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 238.\n",
      "Epoch 00288: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████▉                                        | 170/360 [2:08:47<4:58:05, 94.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE61DA048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBADA8948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 247.\n",
      "Epoch 00297: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████████████                                        | 171/360 [2:09:08<3:47:46, 72.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED9475EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED143E4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 333.\n",
      "Epoch 00383: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████████████▎                                       | 172/360 [2:09:34<3:02:31, 58.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3AC0708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3DAA4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 343.\n",
      "Epoch 00393: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████████████▌                                       | 173/360 [2:10:04<2:35:12, 49.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBADA8AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED0094288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 350.\n",
      "Epoch 00400: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████████████▋                                       | 174/360 [2:10:40<2:21:21, 45.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECDC07558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3DAA288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 137.\n",
      "Epoch 00187: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████████████████████▉                                       | 175/360 [2:10:54<1:51:56, 36.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED13CC798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE61DA438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 111.\n",
      "Epoch 00161: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████████████▏                                      | 176/360 [2:11:10<1:32:16, 30.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED9925048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB6D4F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 522.\n",
      "Epoch 00572: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████████████▎                                      | 177/360 [2:12:16<2:05:15, 41.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED68B6C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECDC07288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 175.\n",
      "Epoch 00225: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████████████▌                                      | 178/360 [2:12:37<1:45:46, 34.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE11BB558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2999828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 210.\n",
      "Epoch 00260: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████▊                                      | 179/360 [2:13:07<1:41:15, 33.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3DAAB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFC094C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1954.\n",
      "Epoch 02004: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████▌                                     | 180/360 [2:18:34<6:04:16, 121.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC85EA318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED539E828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 207.\n",
      "Epoch 00257: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████▏                                     | 181/360 [2:18:47<4:25:45, 89.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6BA7948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB45F80D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 172.\n",
      "Epoch 00222: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|██████████████████████████████████████▍                                     | 182/360 [2:19:00<3:16:00, 66.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3DA6948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC85EA318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 124.\n",
      "Epoch 00174: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|██████████████████████████████████████▋                                     | 183/360 [2:19:10<2:25:38, 49.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB6D4F558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3AC0288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 177.\n",
      "Epoch 00227: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|██████████████████████████████████████▊                                     | 184/360 [2:19:25<1:54:38, 39.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3CB1798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2999B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 131.\n",
      "Epoch 00181: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████                                     | 185/360 [2:19:39<1:31:46, 31.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFF54F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2CD89D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|███████████████████████████████████████▎                                    | 186/360 [2:19:46<1:09:55, 24.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC39AA8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED7299C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████████████████████▌                                     | 187/360 [2:19:54<55:42, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3CB1D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED7333CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 165.\n",
      "Epoch 00215: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|███████████████████████████████████████▋                                    | 188/360 [2:20:26<1:06:31, 23.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC718A678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3C1CDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|███████████████████████████████████████▉                                    | 189/360 [2:20:43<1:00:46, 21.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB6D4FD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE2059708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████▏                                    | 190/360 [2:20:59<55:18, 19.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB19074C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE5D49DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████▍                                    | 191/360 [2:21:16<53:06, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6CEFA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2671048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████▌                                    | 192/360 [2:21:38<55:09, 19.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED7333168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED68B6E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 114.\n",
      "Epoch 00164: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████████████████████▊                                    | 193/360 [2:21:56<54:02, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE24953A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE18411F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 178.\n",
      "Epoch 00228: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████████████████████████                                    | 194/360 [2:22:22<58:53, 21.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED2CD8DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027E9B7BB0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 116.\n",
      "Epoch 00166: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████████████████████▏                                  | 195/360 [2:22:46<1:00:59, 22.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED4349F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED7299E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 214.\n",
      "Epoch 00264: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████████████████████▍                                  | 196/360 [2:23:27<1:16:07, 27.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1B57EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3DA6F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████▌                                  | 197/360 [2:23:39<1:02:18, 22.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6D56B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC29CCAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 182.\n",
      "Epoch 00232: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████▊                                  | 198/360 [2:24:18<1:14:57, 27.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE2495438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB7155828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|██████████████████████████████████████████                                  | 199/360 [2:24:31<1:02:28, 23.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB58B40D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED539E168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████████████████████████▎                                  | 200/360 [2:24:45<54:41, 20.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3CB1678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB7155828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████████████████████████▌                                  | 201/360 [2:25:02<52:13, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3DAA288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE20208B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████████████████████████▊                                  | 202/360 [2:25:19<49:14, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFC1AE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE21B4708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████████████████████████▉                                  | 203/360 [2:25:37<48:50, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3AC0B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED93E2A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████▏                                 | 204/360 [2:26:02<53:07, 20.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC718A828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB981F048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "Epoch 00147: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████▍                                 | 205/360 [2:26:21<51:25, 19.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE147D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBAEE13A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 138.\n",
      "Epoch 00188: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████▋                                 | 206/360 [2:26:36<47:29, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE5D49678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3DA3A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "Epoch 00131: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████▊                                 | 207/360 [2:26:48<42:08, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3DA38B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3797828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 130.\n",
      "Epoch 00180: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████████████████████████████████████████████                                 | 208/360 [2:27:04<41:32, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED4349C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE2460828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 104.\n",
      "Epoch 00154: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████████████████████████████████████████████▎                                | 209/360 [2:27:20<40:49, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE61DA438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE147D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 00058: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████████████████████████████████████████████▌                                | 210/360 [2:27:28<34:53, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB7155318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1362558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 166.\n",
      "Epoch 00216: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████████████████████████████████████████████▋                                | 211/360 [2:27:52<41:49, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB58B40D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB477B4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████████████████████████████████████████████▉                                | 212/360 [2:28:02<36:22, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBC3A39D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED40E48B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|██████████████████████████████████████████████▏                               | 213/360 [2:28:14<34:24, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6D130D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6F5B948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|██████████████████████████████████████████████▎                               | 214/360 [2:28:26<32:52, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBADA8EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1EB68B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████▌                               | 215/360 [2:28:41<33:08, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFECEF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED66864C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████▊                               | 216/360 [2:28:58<35:25, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED2999318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE5D49A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 215.\n",
      "Epoch 00265: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████                               | 217/360 [2:29:10<33:18, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE5D49708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB97C7558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 284.\n",
      "Epoch 00334: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|███████████████████████████████████████████████▏                              | 218/360 [2:29:25<33:46, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB981FB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE147D3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 295.\n",
      "Epoch 00345: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|███████████████████████████████████████████████▍                              | 219/360 [2:29:41<34:34, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC718A5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6B5E798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 413.\n",
      "Epoch 00463: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|███████████████████████████████████████████████▋                              | 220/360 [2:30:02<38:55, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC862D558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE37975E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 352.\n",
      "Epoch 00402: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|███████████████████████████████████████████████▉                              | 221/360 [2:30:23<41:30, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1362048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFDB5168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 389.\n",
      "Epoch 00439: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████                              | 222/360 [2:30:50<47:49, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFDB5C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1B57288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 00107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████▎                             | 223/360 [2:30:58<38:24, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE5D49678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED68B6168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 129.\n",
      "Epoch 00179: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████▌                             | 224/360 [2:31:11<35:27, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC39AA8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE2495318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 455.\n",
      "Epoch 00505: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████▊                             | 225/360 [2:31:56<54:51, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBAE9B678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED9475A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 153.\n",
      "Epoch 00203: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|████████████████████████████████████████████████▉                             | 226/360 [2:32:11<48:24, 21.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE5D49DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3BAD9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 151.\n",
      "Epoch 00201: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████████████████▏                            | 227/360 [2:32:31<46:47, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE147D9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1362E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 963.\n",
      "Epoch 01013: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|████████████████████████████████████████████████▏                           | 228/360 [2:34:54<2:07:07, 57.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB97C7558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBC3A3048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 219.\n",
      "Epoch 00269: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████▎                           | 229/360 [2:35:10<1:38:33, 45.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE2495048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBAE9B828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 249.\n",
      "Epoch 00299: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████▌                           | 230/360 [2:35:28<1:20:08, 36.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6EF1A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE37974C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 271.\n",
      "Epoch 00321: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████▊                           | 231/360 [2:35:48<1:08:50, 32.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFC1AD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB981F438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 368.\n",
      "Epoch 00418: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████▉                           | 232/360 [2:36:13<1:03:36, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE2495678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1362828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 354.\n",
      "Epoch 00404: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████▏                          | 233/360 [2:36:40<1:01:31, 29.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED0094168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB981FC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 292.\n",
      "Epoch 00342: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████▍                          | 234/360 [2:37:08<1:00:33, 28.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6EF1AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3CB1F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 129.\n",
      "Epoch 00179: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████████████████████████████████████████████████▉                           | 235/360 [2:37:22<50:16, 24.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB737A9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFC1AD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 525.\n",
      "Epoch 00575: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|█████████████████████████████████████████████████▊                          | 236/360 [2:38:05<1:02:11, 30.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE37973A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6D13EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 462.\n",
      "Epoch 00512: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████████████████████████                          | 237/360 [2:39:00<1:16:56, 37.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED9925EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3DAAA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 211.\n",
      "Epoch 00261: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████████████████████████▏                         | 238/360 [2:39:22<1:06:24, 32.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECDA1C048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE147DF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 280.\n",
      "Epoch 00330: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████████████████████████▍                         | 239/360 [2:39:57<1:07:23, 33.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE2495438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB6B5F318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 2326.\n",
      "Epoch 02376: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████                         | 240/360 [2:45:55<4:21:29, 130.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE2495B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFC1A678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 255.\n",
      "Epoch 00305: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████▉                         | 241/360 [2:46:13<3:12:15, 96.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED01A48B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE147D708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 256.\n",
      "Epoch 00306: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|███████████████████████████████████████████████████                         | 242/360 [2:46:32<2:24:47, 73.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED270AD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFDB5F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 239.\n",
      "Epoch 00289: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████▎                        | 243/360 [2:46:52<1:51:59, 57.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC29CC1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3F81E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 358.\n",
      "Epoch 00408: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████▌                        | 244/360 [2:47:17<1:32:23, 47.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC0112EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE2495F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 345.\n",
      "Epoch 00395: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████▋                        | 245/360 [2:47:46<1:20:44, 42.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED2CD8168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3BADA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 342.\n",
      "Epoch 00392: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████▉                        | 246/360 [2:48:20<1:15:23, 39.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED2CD8DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3DA6D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 527.\n",
      "Epoch 00577: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████▏                       | 247/360 [2:48:58<1:14:05, 39.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3CB1798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE147D9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 422.\n",
      "Epoch 00472: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████▎                       | 248/360 [2:49:37<1:13:07, 39.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB477B4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECDC070D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 457.\n",
      "Epoch 00507: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████▌                       | 249/360 [2:50:34<1:22:06, 44.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED539E3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2999948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 213.\n",
      "Epoch 00263: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████▊                       | 250/360 [2:50:57<1:09:38, 37.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3BADE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED70C0DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 309.\n",
      "Epoch 00359: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████▉                       | 251/360 [2:51:36<1:09:37, 38.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED13CCF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBACFC318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 4586.\n",
      "Epoch 04636: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████▌                      | 252/360 [3:03:10<7:03:17, 235.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB981FC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE5D490D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 209.\n",
      "Epoch 00259: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████▋                      | 253/360 [3:03:24<5:00:50, 168.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB1907B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED98CB798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 155.\n",
      "Epoch 00205: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████▉                      | 254/360 [3:03:36<3:34:51, 121.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3F819D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECFECECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 142.\n",
      "Epoch 00192: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████▊                      | 255/360 [3:03:47<2:34:51, 88.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC6C01048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC0112678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████                      | 256/360 [3:03:53<1:50:22, 63.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC01128B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6D13A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 201.\n",
      "Epoch 00251: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████▎                     | 257/360 [3:04:10<1:25:34, 49.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFECE0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3CB1F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████▍                     | 258/360 [3:04:17<1:02:47, 36.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFDB5D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB7155318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|████████████████████████████████████████████████████████                      | 259/360 [3:04:25<47:23, 28.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE5D49288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDB9D24C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|████████████████████████████████████████████████████████▎                     | 260/360 [3:04:33<36:56, 22.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED70C0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3C16678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|████████████████████████████████████████████████████████▌                     | 261/360 [3:04:43<30:22, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC8609EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE211EDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████▊                     | 262/360 [3:04:54<26:42, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB85D05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE83FE4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████▉                     | 263/360 [3:05:07<24:55, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3C16AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE133F828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|█████████████████████████████████████████████████████████▏                    | 264/360 [3:05:24<25:04, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBAEE1558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB1591318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 91.\n",
      "Epoch 00141: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████▍                    | 265/360 [3:05:34<22:28, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6B5EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECDC07948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 98.\n",
      "Epoch 00148: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████▋                    | 266/360 [3:05:46<20:59, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFC1A4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EEB68ADC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 87.\n",
      "Epoch 00137: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████▊                    | 267/360 [3:05:57<19:51, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB18661F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3797048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 133.\n",
      "Epoch 00183: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|██████████████████████████████████████████████████████████                    | 268/360 [3:06:13<20:53, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED65F20D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED70C0DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 65.\n",
      "Epoch 00115: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████▎                   | 269/360 [3:06:25<19:49, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED70C0558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE37979D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 88.\n",
      "Epoch 00138: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████▌                   | 270/360 [3:06:41<20:53, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED2CD80D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB45F8678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████▋                   | 271/360 [3:06:49<18:25, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EEB68AD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC3D07CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 125.\n",
      "Epoch 00175: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████████████████████████████████▉                   | 272/360 [3:07:11<22:11, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED11F5F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3BADC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████████████████████████████▏                  | 273/360 [3:07:22<20:02, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBD604048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED71D9708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████████████████████████████▎                  | 274/360 [3:07:34<19:13, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBE9E54C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE25D9DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████████████████████████████▌                  | 275/360 [3:07:48<19:23, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED7299AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE11A1048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████████████████████████████████████████████████████████▊                  | 276/360 [3:08:06<20:35, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED5398B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB7155D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 96.\n",
      "Epoch 00146: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████████████████████████████                  | 277/360 [3:08:17<19:01, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE5D499D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBAEE1558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 106.\n",
      "Epoch 00156: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████████████████████████████▏                 | 278/360 [3:08:30<18:22, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB477B288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED68B6168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 160.\n",
      "Epoch 00210: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████████████████████████████████▍                 | 279/360 [3:08:47<19:42, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED3F3C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB32299D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████████████████████████████████▋                 | 280/360 [3:08:54<16:17, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB32291F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3BADE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 142.\n",
      "Epoch 00192: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████████████████████████████████▉                 | 281/360 [3:09:13<18:41, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3BAD3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED40E4438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 86.\n",
      "Epoch 00136: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████                 | 282/360 [3:09:29<19:15, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBD5F89D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB6C0E828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 148.\n",
      "Epoch 00198: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|█████████████████████████████████████████████████████████████▎                | 283/360 [3:09:50<21:35, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3BAD288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE24FC798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 133.\n",
      "Epoch 00183: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|█████████████████████████████████████████████████████████████▌                | 284/360 [3:10:13<23:41, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFC09678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBD5F84C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|█████████████████████████████████████████████████████████████▊                | 285/360 [3:10:25<20:48, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE5D499D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE20028B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|█████████████████████████████████████████████████████████████▉                | 286/360 [3:10:38<19:05, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB981FA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1EB5708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████▏               | 287/360 [3:10:53<18:30, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6D13EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE123AA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████▍               | 288/360 [3:11:10<19:02, 15.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1B59F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC03F4948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 222.\n",
      "Epoch 00272: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████▌               | 289/360 [3:11:23<17:34, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC3DAAE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6BA71F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 257.\n",
      "Epoch 00307: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|██████████████████████████████████████████████████████████████▊               | 290/360 [3:11:36<16:59, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED01A43A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6EF1C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 253.\n",
      "Epoch 00303: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|███████████████████████████████████████████████████████████████               | 291/360 [3:11:51<16:37, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFECE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE5D49318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 308.\n",
      "Epoch 00358: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|███████████████████████████████████████████████████████████████▎              | 292/360 [3:12:08<17:14, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6BA7678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB32293A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 331.\n",
      "Epoch 00381: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|███████████████████████████████████████████████████████████████▍              | 293/360 [3:12:28<18:35, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED143E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED11F55E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 352.\n",
      "Epoch 00402: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████▋              | 294/360 [3:12:53<21:10, 19.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB3229C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED9475AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 122.\n",
      "Epoch 00172: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████▉              | 295/360 [3:13:04<18:02, 16.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE5D49828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED70C0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 72.\n",
      "Epoch 00122: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████████████████████████████████████████████████████████████▏             | 296/360 [3:13:13<15:33, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECDC07288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED2999288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████████████████████████████████████████████████████████████▎             | 297/360 [3:13:26<14:43, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ECFECE048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3C17948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 153.\n",
      "Epoch 00203: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████▌             | 298/360 [3:13:41<14:55, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC0112828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFC09948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 149.\n",
      "Epoch 00199: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████▊             | 299/360 [3:14:01<16:19, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE4CAD678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB85D0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 2597.\n",
      "Epoch 02647: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|██████████████████████████████████████████████████████████████▌            | 300/360 [3:20:03<1:59:49, 119.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC85EA948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED70C0AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 268.\n",
      "Epoch 00318: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████████████████████████████▌            | 301/360 [3:20:22<1:28:02, 89.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1B57438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED70C0E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 242.\n",
      "Epoch 00292: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████████████████████████████▊            | 302/360 [3:20:40<1:05:55, 68.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED11F55E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3BAD8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 194.\n",
      "Epoch 00244: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████▋            | 303/360 [3:20:58<50:21, 53.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6BA7A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE25D94C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 349.\n",
      "Epoch 00399: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████▊            | 304/360 [3:21:22<41:18, 44.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE37973A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED11F5288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 317.\n",
      "Epoch 00367: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████            | 305/360 [3:21:47<35:24, 38.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED13CC798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1362558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 295.\n",
      "Epoch 00345: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████▎           | 306/360 [3:22:18<32:30, 36.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED11F5C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBACFC438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 125.\n",
      "Epoch 00175: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████▌           | 307/360 [3:22:31<25:47, 29.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1B57798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED11F5DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 105.\n",
      "Epoch 00155: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████████████████████████████████▋           | 308/360 [3:22:45<21:33, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB58B4558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBD5F8DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 407.\n",
      "Epoch 00457: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████████████████████████████████▉           | 309/360 [3:23:38<28:04, 33.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED68B6318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3C175E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 132.\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████▏          | 310/360 [3:23:56<23:48, 28.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED11F55E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB3229CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 233.\n",
      "Epoch 00283: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████▍          | 311/360 [3:24:28<24:21, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBEA7E0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECFECE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1376.\n",
      "Epoch 01426: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|█████████████████████████████████████████████████████████████████▊          | 312/360 [3:28:04<1:08:30, 85.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE147DC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ECFECE0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 250.\n",
      "Epoch 00300: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|███████████████████████████████████████████████████████████████████▊          | 313/360 [3:28:22<51:09, 65.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBADA8F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBEAACD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 236.\n",
      "Epoch 00286: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████          | 314/360 [3:28:40<39:10, 51.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB3229168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED11F5A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 263.\n",
      "Epoch 00313: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████▎         | 315/360 [3:29:01<31:33, 42.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFC094C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6EF1318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 327.\n",
      "Epoch 00377: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████▍         | 316/360 [3:29:25<26:47, 36.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED40E4708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE2495678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 377.\n",
      "Epoch 00427: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████▋         | 317/360 [3:29:55<24:48, 34.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3AC0708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB3229E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 359.\n",
      "Epoch 00409: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████▉         | 318/360 [3:30:29<24:14, 34.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6EF1318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED11F55E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 155.\n",
      "Epoch 00205: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████         | 319/360 [3:30:45<19:43, 28.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBACFC9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB32293A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 527.\n",
      "Epoch 00577: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████▎        | 320/360 [3:31:31<22:42, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB3229288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE25D9D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 492.\n",
      "Epoch 00542: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████▌        | 321/360 [3:32:30<26:55, 41.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6EF19D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3C17A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 196.\n",
      "Epoch 00246: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████▊        | 322/360 [3:32:52<22:32, 35.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EEB68A9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE4CAD708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 242.\n",
      "Epoch 00292: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████▉        | 323/360 [3:33:25<21:29, 34.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EC0112438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB85D0678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1700.\n",
      "Epoch 01750: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████▌       | 324/360 [3:37:51<1:02:33, 104.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EDFD97EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFDB5B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 208.\n",
      "Epoch 00258: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████▍       | 325/360 [3:38:05<44:57, 77.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBD604E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3797708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 192.\n",
      "Epoch 00242: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████▋       | 326/360 [3:38:18<32:52, 58.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE24FC1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE25D9D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 132.\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████▊       | 327/360 [3:38:29<24:06, 43.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED11F5A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB3229F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 138.\n",
      "Epoch 00188: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|███████████████████████████████████████████████████████████████████████       | 328/360 [3:38:42<18:25, 34.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6D56B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EBD5F8678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 125.\n",
      "Epoch 00175: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|███████████████████████████████████████████████████████████████████████▎      | 329/360 [3:38:55<14:33, 28.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBD5F8828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1B13E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 192.\n",
      "Epoch 00242: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████▌      | 330/360 [3:39:15<12:54, 25.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6B5E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB6C0EEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████▋      | 331/360 [3:39:23<09:49, 20.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED270A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3AFDA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████▉      | 332/360 [3:39:31<07:49, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED68B65E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6991F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|████████████████████████████████████████████████████████████████████████▏     | 333/360 [3:39:41<06:34, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED68B6C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE39144C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████████████████████████████████████▎     | 334/360 [3:39:52<05:55, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3AFD048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EEB7E6B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████████████████████████████████████▌     | 335/360 [3:40:05<05:36, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6B5E168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE5ED2E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████████████████████████████████████▊     | 336/360 [3:40:22<05:43, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1B57438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB3025708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 166.\n",
      "Epoch 00216: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████     | 337/360 [3:40:37<05:35, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED13CCDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE12698B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 151.\n",
      "Epoch 00201: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████▏    | 338/360 [3:40:52<05:22, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1B133A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1047828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 147.\n",
      "Epoch 00197: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████▍    | 339/360 [3:41:07<05:13, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3C17E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3914828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 146.\n",
      "Epoch 00196: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████▋    | 340/360 [3:41:24<05:06, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3914EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB85D0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████▉    | 341/360 [3:41:36<04:33, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1B598B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE12698B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 116.\n",
      "Epoch 00166: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████    | 342/360 [3:41:54<04:41, 15.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB3025708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE24950D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████▎   | 343/360 [3:42:03<03:49, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6BA71F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EC0112558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 119.\n",
      "Epoch 00169: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████▌   | 344/360 [3:42:24<04:12, 15.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED11F5D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE3C17828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████▊   | 345/360 [3:42:35<03:37, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBE9E5E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE25A2DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████▉   | 346/360 [3:42:48<03:13, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE169D048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE82834C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████▏  | 347/360 [3:43:02<03:01, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 330, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED655FD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE223F828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████▍  | 348/360 [3:43:19<02:59, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE25A2D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1047828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 136.\n",
      "Epoch 00186: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████▌  | 349/360 [3:43:33<02:41, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE3BAD318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE25A2828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "Epoch 00150: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████▊  | 350/360 [3:43:46<02:20, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 1, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBEAAC828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6D10F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 131.\n",
      "Epoch 00181: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████  | 351/360 [3:44:01<02:09, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED72769D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE835F438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 103.\n",
      "Epoch 00153: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████▎ | 352/360 [3:44:15<01:54, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED7276DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EB596E708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████▍ | 353/360 [3:44:26<01:33, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 3, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB596E798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED7276F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 96.\n",
      "Epoch 00146: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████▋ | 354/360 [3:44:44<01:27, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE2460438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EDFC1A0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|████████████████████████████████████████████████████████████████████████████▉ | 355/360 [3:44:53<01:04, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE26D0798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1362708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 158.\n",
      "Epoch 00208: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████████████████████████████████████████▏| 356/360 [3:45:19<01:07, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 6, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EE1047828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE2460708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████████████████████████████████████████▎| 357/360 [3:45:30<00:45, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EBD5F8558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027ED6BEB708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████████████████████████████████████████▌| 358/360 [3:45:43<00:29, 14.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 100, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027EB3025828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE1AA3DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████▊| 359/360 [3:45:58<00:14, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 32, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 440, 'hidden_layers': 12, 'hidden_neuron': 150, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'softmax', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "adding layer 7\n",
      "adding layer 8\n",
      "adding layer 9\n",
      "adding layer 10\n",
      "adding layer 11\n",
      "adding layer 12\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000027ED6D13EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000027EE265D048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 360/360 [3:46:15<00:00, 37.71s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba58ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/050922154138.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d6867b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b79386c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>249</td>\n",
       "      <td>0.646124</td>\n",
       "      <td>[0.74712646 0.3800905  0.55835235]</td>\n",
       "      <td>0.647863</td>\n",
       "      <td>[0.7699531  0.36923078 0.4230769 ]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308</td>\n",
       "      <td>0.650155</td>\n",
       "      <td>[0.74460846 0.41747576 0.5986395 ]</td>\n",
       "      <td>0.653004</td>\n",
       "      <td>[0.7714285 0.4       0.5737705]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>272</td>\n",
       "      <td>0.657727</td>\n",
       "      <td>[0.73388046 0.3688525  0.5475638 ]</td>\n",
       "      <td>0.655555</td>\n",
       "      <td>[0.77073175 0.49230772 0.5       ]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332</td>\n",
       "      <td>0.655803</td>\n",
       "      <td>[0.75514877 0.43564358 0.57964605]</td>\n",
       "      <td>0.659696</td>\n",
       "      <td>[0.7339449 0.4       0.4736842]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>275</td>\n",
       "      <td>0.622488</td>\n",
       "      <td>[0.6734398  0.42902207 0.6168582 ]</td>\n",
       "      <td>0.662324</td>\n",
       "      <td>[0.6808511  0.3466667  0.53781515]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>51</td>\n",
       "      <td>2.921837</td>\n",
       "      <td>[0.5067064  0.27728614 0.45945945]</td>\n",
       "      <td>3.009521</td>\n",
       "      <td>[0.5957447  0.24999997 0.3584906 ]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>330</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>51</td>\n",
       "      <td>2.943347</td>\n",
       "      <td>[0.4793893  0.30978262 0.3920792 ]</td>\n",
       "      <td>3.021042</td>\n",
       "      <td>[0.58947366 0.26229507 0.38167936]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>330</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>51</td>\n",
       "      <td>2.972579</td>\n",
       "      <td>[0.5427286 0.3163842 0.4418146]</td>\n",
       "      <td>3.036532</td>\n",
       "      <td>[0.5888889  0.31818178 0.3508772 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>440</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>51</td>\n",
       "      <td>2.989156</td>\n",
       "      <td>[0.49386504 0.2776204  0.44741875]</td>\n",
       "      <td>3.053918</td>\n",
       "      <td>[0.6387435  0.30985916 0.41666666]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>440</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>51</td>\n",
       "      <td>2.986738</td>\n",
       "      <td>[0.5288754  0.29461753 0.42553192]</td>\n",
       "      <td>3.070676</td>\n",
       "      <td>[0.57777774 0.3157895  0.34920633]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>440</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "0             249  0.646124  [0.74712646 0.3800905  0.55835235]  0.647863   \n",
       "1             308  0.650155  [0.74460846 0.41747576 0.5986395 ]  0.653004   \n",
       "288           272  0.657727  [0.73388046 0.3688525  0.5475638 ]  0.655555   \n",
       "2             332  0.655803  [0.75514877 0.43564358 0.57964605]  0.659696   \n",
       "36            275  0.622488  [0.6734398  0.42902207 0.6168582 ]  0.662324   \n",
       "..            ...       ...                                 ...       ...   \n",
       "59             51  2.921837  [0.5067064  0.27728614 0.45945945]  3.009521   \n",
       "275            51  2.943347  [0.4793893  0.30978262 0.3920792 ]  3.021042   \n",
       "215            51  2.972579     [0.5427286 0.3163842 0.4418146]  3.036532   \n",
       "287            51  2.989156  [0.49386504 0.2776204  0.44741875]  3.053918   \n",
       "71             51  2.986738  [0.5288754  0.29461753 0.42553192]  3.070676   \n",
       "\n",
       "                        val_fbeta_score activation_layer  \\\n",
       "0    [0.7699531  0.36923078 0.4230769 ]          sigmoid   \n",
       "1       [0.7714285 0.4       0.5737705]          sigmoid   \n",
       "288  [0.77073175 0.49230772 0.5       ]             relu   \n",
       "2       [0.7339449 0.4       0.4736842]          sigmoid   \n",
       "36   [0.6808511  0.3466667  0.53781515]          sigmoid   \n",
       "..                                  ...              ...   \n",
       "59   [0.5957447  0.24999997 0.3584906 ]          sigmoid   \n",
       "275  [0.58947366 0.26229507 0.38167936]              elu   \n",
       "215  [0.5888889  0.31818178 0.3508772 ]             selu   \n",
       "287  [0.6387435  0.30985916 0.41666666]              elu   \n",
       "71   [0.57777774 0.3157895  0.34920633]          sigmoid   \n",
       "\n",
       "     activity_regularizer  batc_normalization  batch_size  bias_regularizer  \\\n",
       "0                  0.0001               False          32            0.0001   \n",
       "1                  0.0001               False          32            0.0001   \n",
       "288                0.0001               False          32            0.0001   \n",
       "2                  0.0001               False          32            0.0001   \n",
       "36                 0.0001                True          32            0.0001   \n",
       "..                    ...                 ...         ...               ...   \n",
       "59                 0.0001                True          32            0.0001   \n",
       "275                0.0001                True          32            0.0001   \n",
       "215                0.0001                True          32            0.0001   \n",
       "287                0.0001                True          32            0.0001   \n",
       "71                 0.0001                True          32            0.0001   \n",
       "\n",
       "     dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
       "0          0  100000            55              1             50   \n",
       "1          0  100000            55              1            100   \n",
       "288        0  100000            55              1             50   \n",
       "2          0  100000            55              1            150   \n",
       "36         0  100000            55              1             50   \n",
       "..       ...     ...           ...            ...            ...   \n",
       "59         0  100000           330             12            150   \n",
       "275        0  100000           330             12            150   \n",
       "215        0  100000           440             12            150   \n",
       "287        0  100000           440             12            150   \n",
       "71         0  100000           440             12            150   \n",
       "\n",
       "    kernel_initializer  kernel_regularizer_l1  kernel_regularizer_l2  \\\n",
       "0           orthogonal                 0.0001                 0.0001   \n",
       "1           orthogonal                 0.0001                 0.0001   \n",
       "288         orthogonal                 0.0001                 0.0001   \n",
       "2           orthogonal                 0.0001                 0.0001   \n",
       "36          orthogonal                 0.0001                 0.0001   \n",
       "..                 ...                    ...                    ...   \n",
       "59          orthogonal                 0.0001                 0.0001   \n",
       "275         orthogonal                 0.0001                 0.0001   \n",
       "215         orthogonal                 0.0001                 0.0001   \n",
       "287         orthogonal                 0.0001                 0.0001   \n",
       "71          orthogonal                 0.0001                 0.0001   \n",
       "\n",
       "    last_activation     lr  \n",
       "0           softmax  0.001  \n",
       "1           softmax  0.001  \n",
       "288         softmax  0.001  \n",
       "2           softmax  0.001  \n",
       "36          softmax  0.001  \n",
       "..              ...    ...  \n",
       "59          softmax  0.001  \n",
       "275         softmax  0.001  \n",
       "215         softmax  0.001  \n",
       "287         softmax  0.001  \n",
       "71          softmax  0.001  \n",
       "\n",
       "[360 rows x 20 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac73dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55,330,440], \n",
    "     'hidden_neuron':[50,100,150],\n",
    "\n",
    "     'hidden_layers':[1,3,6,12],  \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['softmax'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [32],\n",
    "\n",
    "    'lr':[0.001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "#    'batc_normalization':[False,True],\n",
    "#    'dropout': [0,0.2,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "  #  'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "'kernel_initializer': ['orthogonal'],\n",
    "   \n",
    "    'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    "  #  'activation_layer':['relu'],\n",
    "    'batc_normalization':[False,True]\n",
    " #   'batc_normalization':[False],\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "997676de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_layers')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEXCAYAAABPkyhHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcq0lEQVR4nO3de5gcZZn38e+PZEIQAglkgDBJCEcPsBwjgormEpaXM74GVBbCAi4syBpxwV1fVxFQd3UPrAaWk4AhghEkLMsloLgrGGA5GCAcg8opZIYQBkhIQmBy8H7/qGegp9Mz0zPTNdXT+X2uq6+pqqcOdz/dXXdVPTVPKSIwM7MN20ZFB2BmZsVzMjAzMycDMzNzMjAzM5wMzMwMJwMzM8PJoC5ICkk7p+HLJX2zmnn7sZ0TJN3Z3zgbmaQzJS2RtFLSVoO43a9Lumqwtley3f8raVF6v3tXKO/2e9bb90jS3ZL+qpuySWndw/sffc962r51z8mgBiT9StKFFaYfI+mVvnzxI+KMiPh2DWJa70cXEddHxCEDXXeFbU2R1Frr9Q4WSU3ARcAhEbFZRLye03bWq6eI+MeIKGLH9a/A36T3+2hfFszre2TFcjKojZnANEkqmz4NuD4i1g5+SNYH2wAjgaeKDmQQbc+G9X5zleeZzmBxMqiNW4AtgQM7J0gaAxwJzJK0n6T7JS2TtFjSJZJGVFqRpJmSvlMy/tW0zMuSTi2b9whJj0pank75zy8pnpv+LkuXAg6QdLKke0uW/6ik30l6M/39aEnZ3ZK+Lek+SSsk3SlpbF8rRtIH07qWSXpK0tElZYdLejqtv03SuWn6WEm/SMu8IekeSRW/q5J+mN77ckkPSyr9DPaTNC+VLZF0UYXldwV+X1JXv6l0VlV66aGzHiX9q6Slkl6QdFjJvFtK+nH6zJZKukXSpsAdwHbp81gpaTtJ50u6rmTZo1M9LUvb/GBJ2YuSzpX0ePrMbpA0spt62UjSNyQtlPSqpFmStpC0saSVwDDgMUnP9fDxHSzpj+k9/EfnwU6F79GfS3omxXQJoJKyYameXpP0PHBEWZxbSLo6fcfbJH1H0rBq6rkaknZKn+nrKYbrJY1OZV+VNKds/osl/aDK2O6T9O+S3gDOl7SzpN+menhN0g19ibVwEeFXDV7Aj4CrSsb/GpifhvcF9geGA5OABcDZJfMGsHMangl8Jw0fCiwBdgc2BX5aNu8U4M/Ikvoead5Pp7JJad7hJds5Gbg3DW8JLCU7exkOHJ/Gt0rldwPPAbsCm6Tx73Xz3qcArRWmNwHPAl8HRgCfAlYA70/li4ED0/AYYJ80/E/A5Wn5JrIkq262fSKwVXoP5wCvACNT2f3AtDS8GbB/N+voUlfd1N3dwF+V1OMa4DSyneqZwMudMQK3ATek99QEfLK7egLOB65Lw7sCbwF/npb7u1R/I1L5i8BDwHbp81sAnNHNezo1Lbtjeu83Az+p9J3rZvkAfgGMBiYC7cChFb5HY4HlwLEp5q8Aa0vq6gzgGWBCivmusrq+BbiC7Pu9dXp/f11NPfcQe+lntXOqz42BZrKDpB+ksnGpvken8eHAq8C+Vca2FvhSWm4TYDbwD2S/x5HAx4veL/VpH1Z0AI3yAj4OvAlsksbvA77SzbxnA/9ZMt5dMriGkh0w2c6i2x8x8APg39PwJHpOBtOAh8qWvx84OQ3fDXyjpOyLwC+72e4UKieDA8l2zhuVTJsNnJ+GXyJLmpuXLXch8F/dvc9ePoelwJ5peC5wATC2l2W61FU3dVe6gzkZeLak7H1p/m3TDuZPwJhq6omuyeCbwI0lZRsBbcCUNP4icGJJ+T8Dl3fznv4H+GLJ+PvJdqyd77GaZPDxkvEbga9V+B6dBDxQMp+A1pK6+g0lCQs4pLNuyS7PdZB+M6n8eOCu3uq5l8/z3c+qQtmngUdLxu8ATkvDRwJPp+FqYnupbN2zgCuB8X393tbDy5eJaiQi7iU7ejpG0o7Ah8mO5JG0a7rs8Yqk5cA/kh1R9WY7YFHJ+MLSQkkfkXSXpHZJb5IdhVV7KWe78vWl8ZaS8VdKhleRHWH2xXbAooj4UzfbmAocDixMp9cHpOn/QnZUe6ek5yV9rbsNSDpH0oJ0ar4M2IL36uALZAn0GWWXwY7sY/w9ebduImJVGtyM7Aj4jYhY2o91dvlMUr0ton+fSfnnu5D3dsDVqmZbXb6jke0VF3VXXhbT9mRnE4vTZbFlZEfiW1eKoayeqyJpa0k/S5d5lgPX0fU3ci3Z2SXp70/6EFvp+4LsTE7AQ+lS36kMIU4GtTWL7EhpGnBnRCxJ0y8jO1XeJSI2J7tsUt7YXMlisp1Lp4ll5T8FbgUmRMQWZJdWOtfbW3e0L5N94UtNJDsSrZWXgQnqer3/3W1ExO8i4hiyH9gtZEefRMSKiDgnInYEjgL+VtJB5StX1j7w98BnyY7ER5OdnSmt548RcXxa//eBm9K1+968lf6+r2TatlW942wHsWXndekyffpM0jX6CfTvMyn/fCeSXdZYUnn2fuvyHS2JuWI5Xb/Di8iOvsdGxOj02jwidqthfP9EVu97pN/eiXT97d0C7CFpd7Izg+v7EFuXzzMiXomI0yJiO7Iz3kvVz9vAi+BkUFuzgIPJrnFeWzJ9FNl11ZWSPkB27bMaNwInS/qQpPcB3yorH0V2FPqOpP2Avygpaye7XLFjN+u+HdhV0l9IGi7pc8CHyK4T94ukkaUvsmusbwF/J6lJ0hSynfvPJI1Qdr/6FhGxhqx+1qX1HJka41QyfV2FTY4i28G1A8MlnQdsXhLPiZKa0xH2sjS50nq6iIh2sh3wiakB9FRgp2rqICIWk116uFTSmPS+P5GKlwBbSdqim8VvBI6QdJCy213PIdsh/W812y4zG/iKpB0kbUZ2NnpD1P7OttuA3SR9RlmD+3S6Js4bgemSxiu7qeLds7xUV3cC/yZpc2WN3jtJ+mQN4xsFrCS7OaAF+GppYUS8A9xEdmD1UES81N/YJB0naXwaXUqWLHr9vtULJ4MaiogXyX64m5IdsXc6l2xHvYKsobmquwwi4g6ydoDfkF02+U3ZLF8ELpS0AjiPdGSdll0FfBe4L53m7l+27tfJjoTOAV4nO8U9MiJeqya2ClqAt8teE4CjgcOA14BLgZMi4pm0zDTgxXT6fgbvna7vAvw32Y/4fuDSiLi7wjZ/Rbbj/QPZ5Yd36HrqfijwlLK7Z34IfD79+KtxGtmO43VgN/q2Q55Gdn3+GbIGybMB0vueDTyfPpPtSheKiN+T1cHFZPV1FHBURKzuw7Y7XUN2yWMu8AJZ3XypH+vpUfq+HAd8j6yudiFrL+v0I7LP6THgEbKG7FInkd1c8DTZDvQmsnaXWrkA2IfsjPG2CtuH7MDtz3jvElF/Y/sw8GD6vt0KfDkiXhhQ9IOo8+4HM7MNkqSJZIl724hYXnQ8RfGZgZltsFJ71t8CP9uQEwFkdxeYmQ0p6VJMJYdFxD1VrmNTsnachWSXFDdovkxkZma+TGRmZkP0MtHYsWNj0qRJRYdhZjakPPzww69FRHOlsiGZDCZNmsS8efOKDsPMbEiRVN7rwLt8mcjMzJwMzMzMycDMzHAyMDMznAzMzAwnAzMzw8nAzMwYov9nYGZWC3PmzKGtbWDPc2pvbwegubni/3JVraWlhalTpw5oHQPhZGBmNgAdHR1Fh1ATTgZmtsGqxZH4jBkzAJg+ffqA11UktxmYmVm+ySA9C/chSY9JekrSBRXmkaQZkp6V9LikffKMyczM1pf3ZaIO4FMRsTI94PteSXdExAMl8xxG9tzUXYCPAJelv2ZmNkhyPTOITOcTiZrSq/xpOscAs9K8DwCjJdXygdhmZtaL3NsMJA2TNB94Ffh1RDxYNksLsKhkvDVNK1/P6ZLmSZrXeSuXmZnVRu7JICLWRcRewHhgP0m7l82iSotVWM+VETE5IiYP9H5eMzPratDuJoqIZcDdrP/g6VZgQsn4eODlwYnKzMwg/7uJmiWNTsObAAcDz5TNditwUrqraH/gzYhYnGdcZmbWVd53E40DrpU0jCzx3BgRv5B0BkBEXA7cDhwOPAusAk7JOSYzMyuTazKIiMeBvStMv7xkOICz8ozDzMx65v9ANjMzJwMzM3MyMDMznAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM5wMzMyMnJOBpAmS7pK0QNJTkr5cYZ4pkt6UND+9zsszJjMzW9/wnNe/FjgnIh6RNAp4WNKvI+LpsvnuiYgjc47FzMy6keuZQUQsjohH0vAKYAHQkuc2zcys7watzUDSJGBv4MEKxQdIekzSHZJ262b50yXNkzSvvb09z1DNzDY4g5IMJG0GzAHOjojlZcWPANtHxJ7AxcAtldYREVdGxOSImNzc3JxrvGZmG5rck4GkJrJEcH1E3FxeHhHLI2JlGr4daJI0Nu+4zMzsPXnfTSTgamBBRFzUzTzbpvmQtF+K6fU84zIzs67yvpvoY8A04AlJ89O0rwMTASLicuBY4ExJa4G3gc9HROQcl5mZlcg1GUTEvYB6mecS4JI84zAzs575P5DNzMzJwMzMnAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzMwOGFx2AmVl/zZkzh7a2tkJjaG1tBWDGjBmFxgHQ0tLC1KlT+7Wsk4GZDVltbW0sev45thlR3K6sac06AFa3LiwsBoAlq9cOaHknAzMb0rYZMZyTxo0pOozCzVq8dEDLu83AzMycDMzMzMnAzMxwMjAzM5wMzMwMJwMzM8PJwMzMqDIZSDpO0qg0/A1JN0vaJ9/QzMxssFR7ZvDNiFgh6ePA/wGuBS7rbSFJEyTdJWmBpKckfbnCPJI0Q9Kzkh53kjEzG3zVJoN16e8RwGUR8V/AiCqWWwucExEfBPYHzpL0obJ5DgN2Sa/TqSLJmJlZbVWbDNokXQF8Frhd0sbVLBsRiyPikTS8AlgAtJTNdgwwKzIPAKMljav6HZiZ2YBVmww+C/wKODQilgFbAl/ty4YkTQL2Bh4sK2oBFpWMt7J+wjAzsxxV21HdOOC2iOiQNAXYA5hV7UYkbQbMAc6OiOXlxRUWiQrrOJ3sMhITJ06sdtNmDWeg3Ta3t7cD0NzcPKA4BtJdstWfas8M5gDrJO0MXA3sAPy0mgUlNaXlr4+ImyvM0gpMKBkfD7xcPlNEXBkRkyNi8kC/xGYbso6ODjo6OooOw+pMtWcGf4qItZI+A/wgIi6W9GhvC0kSWfJYEBEXdTPbrcDfSPoZ8BHgzYhYXGVcZhucgR6Ndz6EZfr06bUIxxpEtclgjaTjgZOAo9K0piqW+xgwDXhC0vw07evARICIuBy4HTgceBZYBZxSZUxmZlYj1SaDU4AzgO9GxAuSdgCu622hiLiXym0CpfMEcFaVcZiZWQ6qSgYR8bSkc4FdJe0O/D4ivpdvaGZmPWtvb+edjrUDfspXI1jSsZaR6eaA/qgqGaQ7iK4FXiQ70p8g6S8jYm6/t2xmZnWj2stE/wYcEhG/B5C0KzAb2DevwMzMetPc3MzqjlV+BjLZM5BHDOBOy2pvLW3qTAQAEfEHqmtANjOzIaDaM4N5kq4GfpLGTwAezickMzMbbNUmgzPJ7viZTtZmMBe4NK+gzMxscFV7N1EHcFF6mZlZg+kxGUh6ggr9BHWKiD1qHpGZmQ263s4MjhyUKMzMrFA9JoOIWFjNSiTdHxEH1CYkMzMbbNXeWtqbkTVaj5mZFaBWyaDbdgUzM6t/tUoGZmY2hNUqGfTYM6mZmdW3WiWDaTVaj5mZFaC3/zNYQeX2AJE9imBzsoEnc4jNzMwGSW+3lo4arEDMzKw41fZNBICkrSm5jTQiXqp5RGZmNuiqajOQdLSkPwIvAL8le8jNHTnGZWZmg6jaBuRvA/sDf4iIHYCDgPtyi8rMzAZVtclgTUS8DmwkaaOIuAvYK7+wzMxsMFXbZrBM0mbAPcD1kl4F1uYXlpmZDaZqzwzmAqOBLwO/BJ4DjsopJjMzG2TVJgMBvwLuBjYDbkiXjczMrAFU+6SzC4ALJO0BfA74raTWiDg41+jMGsycOXNoa2srNIbW1lYAZsyYUWgcAC0tLUydOrXoMIw+/p8B8CrwCvA6sHXtwzFrbG1tbSx6/jm2GdHXn17tNK1ZB8Dq1qoeV5KbJavd7FhPqvpGSjqT7IygGbgJOC0ins4zMLNGtc2I4Zw0bkzRYRRu1uKlRYdgJao9PNkeODsi5ucYi5mZFaTaNoOv5R2ImZkVxw+3MTMzJwMzM3MyMDMzck4Gkq6R9Kqkig+/kTRF0puS5qfXeXnGY2ZmleV9s/NM4BJgVg/z3BMRR+Ych5mZ9SDXM4OImAu8kec2zMxs4OqhzeAASY9JukPSbt3NJOl0SfMkzWtvbx/M+MzMGl7RyeARYPuI2BO4GLiluxkj4sqImBwRk5ubmwcrPjOzDUKhySAilkfEyjR8O9AkaWyRMZmZbYgKTQaStpWkNLxfisddY5uZDbJc7yaSNBuYAoyV1Ap8C2gCiIjLgWOBMyWtBd4GPh8RkWdMZma2vlyTQUQc30v5JWS3nloDq0Uf/p03DQykvch955t1r7hO1c36oKOjo+gQzBqak4HlrhZH451P5Zo+ffqA12Vm6yv61lIzM6sDTgZmZubLRGY2tC1ZvbbQR2guTc+UHtM0rLAYIKuHCQNY3snAzIaslpaWokNgTWsrACPGjy80jgkMrD6cDMwGUXt7O+90FHskWy+WdKxl5AD7GauHW4Ub5eYGtxmYmZnPDMwGU3NzM6s7VnHSuDFFh1K4WYuXMsKdTtYNnxmYmZmTgZmZORmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGU4GZmaGO6qzXsyZM4e2traiw6A19Rnf2V1wUVpaWuqi22SzWnMysB61tbWx6Pnn2GZEsV+VpvQ0qdWtCwuLYcnqtYVt2yxvTgbWq21GDHeXy+AH0lhDc5uBmZk5GZiZmZOBmZnhZGBmZrgBuaJa3E7Z3t4OZM+8HQjfyth4lqxeW2hj9NJ0Z9aYpmGFxQBZPUwoNILa/NZrddtz0b91J4OcdHR0FB2C1aGWlpaiQ2BN2nmNGD++0DgmUB/1MVAbb7xx0SHUhJNBBbXIzp1HCdOnTx/wuqxx1MNZnr+b76mHz6NeuM3AzMzyTQaSrpH0qqQnuymXpBmSnpX0uKR98ozHzMwqy/sy0UzgEmBWN+WHAbuk10eAy9JfqxPt7e2801Fsg2e9WNKxlpHpxgCzRpPrmUFEzAXe6GGWY4BZkXkAGC1pXJ4xmZnZ+opuQG4BFpWMt6Zpi8tnlHQ6cDrAxIkTByU4y26NXd2xyn0TkfVNNGKAtwqb1auiG5BVYVpUmjEiroyIyRExeaD37puZWVdFJ4NW6PJ/J+OBlwuKxcxsg1V0MrgVOCndVbQ/8GZErHeJyMzM8pVrm4Gk2cAUYKykVuBbQBNARFwO3A4cDjwLrAJOGeg2/WSuror+F3czGxpyTQYRcXwv5QGcVctt+slc7/GTucysWkXfTZQLP5kr4/8NMLNqFd1mYGZmdaAhzwzMGtlA28Uapctlqy0nA+tV0f3vQ330wV8P/e/XQqN0uWy15WRgPaqX/ubroQ/+eul/30fjloeGSwbuWO09tehYrV52PO6D3yxfbkA2M7PGOzNwx2rvccdqZlYtnxmYmZmTgZmZORmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ0YD/gWz1pxaPIq1Ft8vuctmsew2ZDNzlcqZRulwGd7tslreGSwb10MUwuMvlUj4aN6t/DZcM6mXH4y6XzWwocQOymZk13plBLdRLgye40dPMBoeTQU7c4GlmQ4mTQQU+EjezDY3bDMzMzMnAzMycDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzAxQRRcfQZ5LagYVFx1GFscBrRQfRQFyfteO6rK2hUp/bR0RzpYIhmQyGCknzImJy0XE0Ctdn7bgua6sR6tOXiczMzMnAzMycDPJ2ZdEBNBjXZ+24LmtryNen2wzMzMxnBmZm5mRgZmY4GeRC0jWSXpX0ZNGxDHWSRkp6SNJjkp6SdEHRMQ11kkZLuknSM5IWSDqg6JiGkkq/b0n/kurzcUn/KWl0gSH2i5NBPmYChxYdRIPoAD4VEXsCewGHStq/2JCGvB8Cv4yIDwB7AgsKjmeomcn6v+9fA7tHxB7AH4D/N9hBDZSTQQ4iYi7wRtFxNILIrEyjTenlux76SdLmwCeAqwEiYnVELCs0qCGm0u87Iu6MiLVp9AFg/KAHNkBOBlb3JA2TNB94Ffh1RDxYcEhD2Y5AO/BjSY9KukrSpkUH1WBOBe4oOoi+cjKwuhcR6yJiL7Kjrf0k7V5wSEPZcGAf4LKI2Bt4C/hasSE1Dkn/AKwFri86lr5yMrAhI13OuBu3xwxEK9BacnZ1E1lysAGS9JfAkcAJMQT/gcvJwOqapObOOzMkbQIcDDxTaFBDWES8AiyS9P406SDg6QJDagiSDgX+Hjg6IlYVHU9/+D+QcyBpNjCFrFvbJcC3IuLqQoMaoiTtAVwLDCM7eLkxIi4sNqqhTdJewFXACOB54JSIWFpoUENIpd832d1DGwOvp9keiIgzCgmwn5wMzMzMl4nMzMzJwMzMcDIwMzOcDMzMDCcDMzPDycDMzHAysAYhaVKlLsMlXSjp4ArTp0j6RTfrelHS2BrGdr6kc2u1PrM8DC86ALM8RcR5RceQN0nDS3rMNOsXnxlYIxkm6UfpITh3StpE0kxJx0LWZUB6AMm9wGc6F5K0VZr/UUlXACopOzE9XGe+pCskDUvTV0r6bnrozgOStqkmQEmnSfpdWm6OpPdJGiXpBUlNaZ7N09lJk6SdJP1S0sOS7pH0gTTPTEkXSboL+L6kT6YY56f3MapmtWobBCcDayS7AP8REbsBy4CpnQWSRgI/Ao4CDgS2LVnuW8C9qRfPW4GJaZkPAp8DPpZ6TV0HnJCW2ZSsy4E9gbnAaVXGeHNEfDgttwD4QkSsIOuA74g0z+eBORGxBrgS+FJE7AucC1xasq5dgYMj4pxUdlaK80Dg7SrjMQOcDKyxvBAR89Pww8CkkrIPpPI/ph4lrysp+0TneETcBnT203MQsC/wu/Q8hYPIngcAsBrobHMo31ZPdk9H+E+QJZbd0vSrgFPS8ClkzxvYDPgo8PO0/SuAcSXr+nlErEvD9wEXSZoOjPZlI+srtxlYI+koGV4HbFJW3lNHXJXKBFwbEZUeYbimpJvidVT/W5oJfDoiHpN0MlmHZ0TEfakR/JPAsIh4Mj2VbFk62q/krXeDj/iepNuAw4EHJB0cEe7d1armMwPbUDwD7CBppzR+fEnZXNLlH0mHAWPS9P8BjpW0dSrbUtL2A4xjFLA4tQ+cUFY2C5gN/BggIpYDL0g6Lm1fkvastFJJO0XEExHxfWAe2ZmQWdWcDGyDEBHvAKcDt6UG5IUlxRcAn5D0CHAI8FJa5mngG8Cdkh4ne+j5OAbmm8CDaV3lR+7XkyWi2SXTTgC+IOkx4CngmG7We7akJ9N8bzMEH7toxXIX1mZ1It31dExETCs6FtvwuM3ArA5Iuhg4jOyav9mg85mBWY2kh6EfVzb55xHx3SLiMesLJwMzM3MDspmZORmYmRlOBmZmhpOBmZkB/x8ZGfMtXIKz2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_layers'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "178079c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of first_neuron')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'first_neuron'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34b014ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_neuron')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_neuron'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d26e6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batc_normalization')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEXCAYAAABPkyhHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAee0lEQVR4nO3dfbhVdZ338feHJ8VRUeRkiCg1atPDlOkRsbKY8m7QfKibnsgw6y4np+vGSvOeuhyzmpnybvJK8jazUREjexgcK8WTpSJqgh4cRE3zgSAOohwVDqKCcPjef/x+RzabfQ77wFl7b9mf13Xt66zn9V17/9b6rt9av7OWIgIzM2tug+odgJmZ1Z+TgZmZORmYmZmTgZmZ4WRgZmY4GZiZGU4GZmaGk8GAkBSSDsndl0n652qm3YH1nCrp5h2Nc1cm6UxJT0taJ2m/Gq73a5L+o1brK1nvhyQtz9v79grjd7ic7eokzZX02dxdyD5Vr3KxM+R/OgNJvwUWRMT5ZcNPAX4EHBgRm/qYP4BDI+LxKtZV1bSSxgF/Bob2te6BIGki8JOIOLDI9RRF0lBgLTAhIu4vcD0TaZDvSdITwJcj4le9jK+6TFaYdynw2Yj4/c5F2ZgkzSX9jgNysG6kcrEzXDNIZgBTJals+FRgVtEHY9tp+wO7Aw/VO5AaOpgm2F4lPk7VQkQ0/QcYDnQB7y4Zti+wHngbMB64G1gDrAQuAYaVTBvAIbl7BvAvJeO+kud5EvhM2bQfAP6bdFa7HLigZL6/5GnX5c8xwOnAnSXTvAO4N8d+L/COknFzgW8BdwHPAzcDo3rZ/olARy/j3piXtYZ08Dm5ZNwJwB/z8lcA5+Tho4Ab8jzPAXcAg3pZ/sV529cCC4FjS8aNB9rzuKeBiyrMfxjwQsl3dSswLvcPKfs+Ppu7TwfuBP4dWE2qgR1fMu1I4Kr8m60Grgf+CngJ2FzymxwAXEA6K+yZ9+T8Pa3J63xjybilwDnA4vyb/RzYvZfvZRBwHrAMWAXMBEYAu+V1R97uJ3qZP4BpwBLgGeC7Pb8B8Nf5e3o2j5sF7JPHXZO38aW8nnPz8HcBf8jbtRw4fTv71Azg/wE35vKxAPjrfpTdfyWV3ZeAQ/L2/CPwWF7et/J23J3Lxy/I+yRp370B6My/3w2k2n2vZSF3n1vy264DNgIz8rhPAw/ndS8B/iEPr2m5KPQ4WOsVNuoH+DHwHyX9/wAsyt1HAhOAIaQDzcPAF8t2vG2SATCJdBB7Sy40Py2bdiLwt6Qd/6152g/mcePY9oBWWnBH5oI+Ncc1JffvV1LgnyAdLIfn/u/0su0TqZAMgKHA48DXgGHAe/PO8IY8fiX54E3aAY/I3d8GLsvzDwWOJV+SrLCOTwL75W04G3iqZ0cg7ehTc/eepMtAlZax1XfVy3c3l60PABuBzwGDgTNJB/6ey6Y35h1y3xz/e3r7nijZ6dmSmP5Hnu/c/P31HKSWAveQDhYjSeXo871s02fyvK/P234dcE2lMtfL/AHcltdzEPBoyfYfkmPcDWgB5gHfL5l3KXBcSf9B+XefkrdrP+Dw7exPM0gnAuPzbzsL+Fk/yu5fgDfn8UPz9vwa2DsP3wDckr+fEaSTkk/l+fcDJgN7AHsBvwSu76Ms3Fkh/rG5TJyQ+z9ASj4C3gO8yJbyXrNyUeTH1a8trgY+Iml47j8tDyMiFkbE/IjYFBFLSfcR3lPFMj8KXBURD0bEC6QC8oqImBsRD0TE5ohYDFxb5XIhFc7HIuKaHNe1wCPASSXTXBURj0bES6Qzp8OrXHaPCaQD0Xci4uWIuJV0ljUlj98IvEnS3hGxOiLuKxk+Gjg4IjZGxB2RS325iPhJRDybt+F7pAPUG0qWc4ikURGxLiLm9zP+viyLiB9HRDfpdx4N7C9pNHA8aWdcneO/vcplfgy4MSJ+FxEbSTWP4aSz4B7TI+LJiHgO+A29/yankmpCSyJiHfBV4OOShvRjGy+MiOci4i/A98m/W0Q8nmPcEBGdwEX0Xe5OBX4fEdfm7+PZiFhUxfqvi4h7Il1mncWWba2m7M6IiIfy+I0l27M2Ih4CHgRuzt9PF3AT8Pa8fc9GxOyIeDEinifVMqrdr8jHgOuBiyNiTl7mjRHxRCS3k2rax1a5yIEsF4VxMsgi4k5StfIUSa8HjiKdySPpMEk3SHpK0lrg30iXQrbnAFKVusey0pGSjpZ0m6ROSV3A56tcbs+yl5UNWwaMKel/qqT7RdKBvT8OAJZHxOZe1jGZdKlomaTbJR2Th3+XdOZzs6Qlkv6ptxVIOlvSw5K6JK0hneX1fAf/i3RW9YikeyWd2M/4+/LKdxMRL+bOPUlnhM9FxOodWOZWv0n+3pazY79J+e+7jHSWvH8/4ikvewcASHqNpJ9JWpHL80/ou9yNJdUy+6u3ba2m7C5nW0+XdL9UoX9PAEl7SPqRpGV5++YB+0gaXGXcVwB/iogLewZIOl7SfEnP5XJ6Aju4r+5kuSiMk8HWZpJqBFNJZx09he2HpDOXQyNib9Jlk/KbzZWsJO1IPQ4qG/9TUtV3bESMIF1a6VluxTPpEk+SbiKWOoh07X6gPAmMLbuB98o6IuLeiDgFeA3pTOoXefjzEXF2RLyedLb3ZUnvK1+4pGOB/0OqQe0bEfuQrpkqL+exiJiSl38h8J+S/qqKuF/If/coGfbaqrY47aQjJe1TYVy/fpPcIGEsO/ablP++BwGb2PoAuD3lZe/J3P1t0ra8NZfnT7J1eS7fzuWkSyQDpZqyu73vui9nk2qXR+fte3cevt19Np+4vIF0ItIzbDdgNumMfv9cTuewg/vqTpaLwjgZbG0mcBzpWvLVJcP3It2kWifpb0jXmKvxC+B0SW+StAfw9bLxe5HOQtdLGg98omRcJ+mm1Ot7WfYc4DBJn5A0RNLHgDeRLuPsEEm7l35I1zFfAM6VNDQ3oTsJ+JmkYbmN9ohc9V0LdOflnCjpkFzoe4Z3V1jlXqQDXCcwRNL5pGvCPfF8UlJLPpNakwdXWs5W8qWPFcAnJQ2W9BmqPJhFxErSJYdLJe2bt7vnYPI0sJ+kEb3M/gvgA5Lel5u7nk26tv2HatZd5lrgS5JeJ2lPUm3059G/lm1fydswFjiLdB8E0ve+DlgjaQypkUOpp9m63M0CjpP00VzW9pN0+A5sU48BL7tl9iLVFNZIGsm2+11Fko4n3XT/YL602mMY6fJlJ7ApT/f+kvG1LBeFcTIoke8H/IF0s/fXJaPOIR2onyfdaP75NjNXXt5NpGu1t5Ium9xaNsk/At+U9DxwPvnMOs/7IrlFhaQ1kiaULftZ4ERSwXqWdFPqxIh4pprYKhhD2oFKP2NJrSCOJ7U6uRQ4LSIeyfNMBZbmqvjnSWeYAIcCvycdcO4GLo2IuRXW+VvSgfdRUjV6PVtfHpgEPCRpHanV0ccjYn2V2/M50kHuWdINx/7seFNJ9yseIbXk+SJA3u5rgSX5NzmgdKaI+BPpO/gB6fs6CTgpIl7ux7p7XElq2TOP1NppPfC/+7mMX5FaaC0i3RS/Ig//BnAEqRZ2I+nmdKlvA+flbTwn33M4gVTWnsvLe1s/Y3lFAWW33PdJ1+SfAeYDbVXO9zHSDfWHlf6Zb52ky/J9h2mk/XM16VjwyvGhxuWiMP6nMzMzc83AzMycDMxsB0l6qORySunn1HrHZv3ny0RmZkZ//oGlYYwaNSrGjRtX7zDMzF5VFi5c+ExEtFQa96pMBuPGjaO9vb3eYZiZvapIKv9nv1f4noGZmTkZmJmZk4GZmeFkYGZmOBmYWYPq6uri4osvZu3atfUOpSk4GZhZQ2pra2PJkiW0tVX7aCHbGU4GZtZwurq6WLBgARHB/PnzXTuoAScDM2s4bW1tdHenp5V3d3e7dlADTgZm1nDa29t73iVMRHDvvffWOaJdn5OBmTWcvffee6v+ESN6e2+MDRQnAzNrOM88s/V7bjo7O+sUSfMoNBnk1yfeI+n+/Ljbb1SYRpKmS3pc0mJJRxQZk5k1vvKnKfvpysUr+kF1G4D3RsS6/O7POyXdFBHzS6Y5nvSaxEOBo0kvnz+64LjMzKxEoTWDSNbl3qH5U57iTwFm5mnnA/tIGl1kXGZmtrXC7xlIGixpEenF4r+LiAVlk4xh65egd+Rh5cs5Q1K7pHZfPzQzG1iFJ4OI6I6Iw4EDgfGS3lI2iSrNVmE5l0dEa0S0trRUfDeDme0iBg0a1Ge/DbyafcMRsQaYC0wqG9UBjC3pPxB4sjZRmVkjcjKovaJbE7VI2id3DweOAx4pm+zXwGm5VdEEoCsiVhYZl5k1tlGjRvXZbwOv6NZEo4GrJQ0mJZ5fRMQNkj4PEBGXAXOAE4DHgReBTxcck5k1uNWrV/fZbwOv0GQQEYuBt1cYfllJdwBfKDIOM3t1aW1t5a677nql/6ijjqpjNM3BF+LMrOFMmjSJIUPSueqQIUOYNKn8VqMNNCcDM2s4I0aM4Oijj0YSEyZM2OZZRTbwir5nYGa2QyZNmsRTTz3lWkGNOBmYWUMaMWIEZ511Vr3DaBq+TGRmZk4GZmbmZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZg2qo6ODc889lxUrVtQ7lKbgZGBmDemqq65i/fr1XHXVVfUOpSk4GZhZw+no6KDnjYarVq1y7aAGnAzMrOGU1wZcOyiek4GZNZzy95yvWrWqTpE0DycDMzNzMjAzMycDM2tAgwYN6rPfBp6/YTNrOEceeeRW/a2trXWKpHk4GZhZwzn55JP77LeB52RgZg1nxIgRHHXUUQCMHz/er72sAb/pzMwa0sknn8xzzz3nWkGNOBmYWUPyay9ry5eJzMzMycDMzJwMzMwMJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzPDycDMzCg4GUgaK+k2SQ9LekjSNv9OKGmipC5Ji/Ln/CJjMjOzbRX9OIpNwNkRcZ+kvYCFkn4XEX8sm+6OiDix4FjMzKwXhdYMImJlRNyXu58HHgbGFLlOMzPrv5rdM5A0Dng7sKDC6GMk3S/pJklv7mX+MyS1S2ovf1m2mZntnJokA0l7ArOBL0bE2rLR9wEHR8TbgB8A11daRkRcHhGtEdHa0tJSaLxmZs2m8GQgaSgpEcyKiOvKx0fE2ohYl7vnAEMljSo6LjMz26Lo1kQCrgAejoiLepnmtXk6JI3PMT1bZFxmZra1olsTvROYCjwgaVEe9jXgIICIuAz4MHCmpE3AS8DHIyIKjsvMzEoUmgwi4k5A25nmEuCSIuMwM7O++T+Qm1xXVxcXX3wxa9eW39c3s2biZNDk2traWLJkCW1tbfUOxczqyMmgiXV1dXHPPfcQESxYsMC1A7Mm5mTQxNra2ti8eTMAmzdvdu3ArIk5GTSxhQsX0t3dDUB3dzft7e11jsjM6sXJoIkdeeSRDB48GIDBgwfT2tpa54jMrF6cDJrYpEmT6PmXjohg0qRJdY7IzOrFyaDJlSYDM2teTgZNrK2tjfwkECT5BrJZE3MyaGILFy7cqjWRbyCbNS8ngybmG8hm1sPJoIlNmjSJQYNSERg0aJBvIJs1saKfWmq9mD17NitWrKh3GK/cMxg+fDgzZsyoWxxjxoxh8uTJdVu/WbNzzaDJSUISI0eOrHcoZlZHrhnUSaOcBU+fPh2AadOm1TkSM6sn1wzMzMzJwMzMfJnIzCpohAYOnZ2dALS0tNQ1DmiOBg5OBmbWkDZs2FDvEJqKk4GZbaMRzoLduKG2fM/AzMycDMzMzMnAzMxwMjAzM5wMzMwMJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzOjymQg6SOS9srd50m6TtIRxYZmZma1Um3N4J8j4nlJ7wL+Hrga+OH2ZpI0VtJtkh6W9JCksypMI0nTJT0uabGTjJlZ7VWbDLrz3w8AP4yIXwHDqphvE3B2RLwRmAB8QdKbyqY5Hjg0f86giiRjZmYDq9pksELSj4CPAnMk7VbNvBGxMiLuy93PAw8DY8omOwWYGcl8YB9Jo6veAjMz22nVJoOPAr8FJkXEGmAk8JX+rEjSOODtwIKyUWOA5SX9HWybMMzMrEDVvs9gNHBjRGyQNBF4KzCz2pVI2hOYDXwxItaWj64wS1RYxhmky0gcdNBB1a7azMyqUG3NYDbQLekQ4ArgdcBPq5lR0tA8/6yIuK7CJB3A2JL+A4EnyyeKiMsjojUiWhvhNXhmZruSapPB5ojYBPxP4PsR8SVSbaFPkkRKHg9HxEW9TPZr4LTcqmgC0BURK6uMy8zMBkC1l4k2SpoCnAaclIcNrWK+dwJTgQckLcrDvgYcBBARlwFzgBOAx4EXgU9XGZOZmQ2QapPBp4HPA/8aEX+W9DrgJ9ubKSLupPI9gdJpAvhClXGYmVkBqrpMFBF/BM4hneG/BeiIiO8UGpmZmdVMVTWD3ILoamAp6Ux/rKRPRcS8wiIzM7OaqfYy0feA90fEnwAkHQZcCxxZVGBmZlY71bYmGtqTCAAi4lGqu4FsZmavAtXWDNolXQFck/tPBRYWE5KZmdVatcngTFKLn2mkewbzgEuLCsrMzGqrqmQQERuAi/LHzMx2MX0mA0kPUOE5QT0i4q0DHpGZmdXc9moGJ9YkCjMzq6s+k0FELKtmIZLujohjBiYkMzOrtWqblm7P7gO0HDMzq4OBSga93lcwM7PGN1DJwMzMXsUGKhn0+WRSMzNrbAOVDKYO0HLMzKwOtvd/Bs9T+X6ASK8i2JvU8WABsZmZWY1sr2npXrUKxMzM6qfaZxMBIOk1lDQjjYi/DHhEZmZWc1XdM5B0sqTHgD8Dt5NecnNTgXGZmVkNVXsD+VvABODRiHgd8D7grsKiMjOzmqo2GWyMiGeBQZIGRcRtwOHFhWVmZrVU7T2DNZL2BO4AZklaBWwqLiwzM6ulamsG84B9gLOANuAJ4KSCYjIzsxqrNhkI+C0wF9gT+Hm+bGRmZruAqpJBRHwjIt5MevXlAcDtkn5faGRmZlYz/X0cxSrgKeBZ4DUDH46ZmdVDtf9ncKakucAtwCjgc37lpZnZrqPa1kQHA1+MiEUFxmJmZnVSVTKIiH8qOhAzM6sfv9zGzMycDMzMzMnAzMzo5yOs+0vSlcCJwKqIeEuF8ROBX5GehgpwXUR8s8iYzBrZ7NmzWbFiRb3DaAgdHR0ATJ8+vc6RNIYxY8YwefLkwpZfaDIAZgCXADP7mOaOiDix4DjMXhVWrFjB8iVPsP+wonfNxjd0YzcAL3csq3Mk9ff0y8U/Cq7QEhcR8ySNK3IdZrua/YcN4bTR+9Y7DGsgM1euLnwdjXDP4BhJ90u6SdKbe5tI0hmS2iW1d3Z21jI+M7NdXr2TwX3AwRHxNuAHwPW9TRgRl0dEa0S0trS01Co+M7OmUNdkEBFrI2Jd7p4DDJU0qp4xmZk1o7omA0mvlaTcPT7H40djm5nVWNFNS68FJgKjJHUAXweGAkTEZcCHgTMlbQJeAj4eEVFkTODme6XcfG9rRTffM2tURbcmmrKd8ZeQmp7WlJvvbeHme1vUovmeWaNq2qOhm+9ZuVo03zNrVPVuTWRmZg3AycDMzJwMzMzMycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM2BIvQMwsy06OztZv2ETM1eurnco1kCe3rCJ3Ts7C12HawZmZuaagVkjaWlp4eUNL3La6H3rHYo1kJkrVzOspaXQdbhmYGZmTgZmZuZkYGZmOBmYmRlNegPZzfesklo03zNrVK4ZmJlZc9YM3HzPKqlF8z2zRlVozUDSlZJWSXqwl/GSNF3S45IWSzqiyHjMzKyyoi8TzQAm9TH+eODQ/DkD+GHB8ZiZWQWFJoOImAc818ckpwAzI5kP7CNpdJExmZnZtup9A3kMsLykvyMP24akMyS1S2rvdIsPM7MBVe9koArDotKEEXF5RLRGRGuLb/KZmQ2oeieDDmBsSf+BwJN1isXMrGnVOxn8GjgttyqaAHRFxMo6x2Rm1nQK/T8DSdcCE4FRkjqArwNDASLiMmAOcALwOPAi8Oki4zF7NXj6Zf93PMDqjd0A7Dt0cJ0jqb+nX9601SWUIhSaDCJiynbGB/CFImMwezUZM6Zi+4mmtLGjA4BhBx5Y50jqbyzFl42m/A9ks0Y1efLkeofQMKZPnw7AtGnT6hxJc6j3PQMzM2sATgZmZuZkYGZmTXzPwC02ErfY2KIWLTbMGlVTJgO32NjCLTa2qEWLDbNG1ZTJwC02tnCLDTMD3zMwMzOcDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzM8PJwMzMcDIwMzOcDMzMjCZ9hLWZ9W327NmsWLGirjF05Hdt9DxmvZ7GjBmzyz/63smgThphZ4PG2eGaYWez/tltt93qHUJTcTJoct7hrBIn5ubjZFAn3tnMrJH4BrKZmTkZmJmZk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZnhZGBmZoAiot4x9JukTmBZvePYhYwCnql3EGYVuGwOrIMjoqXSiFdlMrCBJak9IlrrHYdZOZfN2vFlIjMzczIwMzMnA0sur3cAZr1w2awR3zMwMzPXDMzMzMnAzMzwy212SZK6gQdKBn0wIpb2Mu26iNizJoGZZZL2A27Jva8FuoHO3D8+Il6uS2BNzPcMdkH9OcA7GVi9SboAWBcR/14ybEhEbKpfVM3Hl4magKQ9Jd0i6T5JD0g6pcI0oyXNk7RI0oOSjs3D3y/p7jzvLyU5cVghJM2QdJGk24ALJV0g6ZyS8Q9KGpe7PynpnlxefyRpcL3i3lU4GeyahuedZJGk/wLWAx+KiCOAvwO+J0ll83wC+G1EHA68DVgkaRRwHnBcnrcd+HLNtsKa0WGk8nZ2bxNIeiPwMeCdubx2A6fWJrxdl+8Z7JpeyjsJAJKGAv8m6d3AZmAMsD/wVMk89wJX5mmvj4hFkt4DvAm4K+eOYcDdtdkEa1K/jIju7UzzPuBI4N5cLocDq4oObFfnZNAcTgVagCMjYqOkpcDupRNExLycLD4AXCPpu8Bq4HcRMaXWAVvTeqGkexNbX73oKbMCro6Ir9Ysqibgy0TNYQSwKieCvwMOLp9A0sF5mh8DVwBHAPOBd0o6JE+zh6TDahi3NbelpHKIpCOA1+XhtwAflvSaPG5kLr+2E1wzaA6zgN9IagcWAY9UmGYi8BVJG4F1wGkR0SnpdOBaSbvl6c4DHi08YjOYDZwmaRHpMuajABHxR0nnATdLGgRsBL6AH2u/U9y01MzMfJnIzMycDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnA2tQksZJerAf058u6YAiYypC6XZKapU0fQeX8YmS/h1ajjU3JwPbVZwO1DwZDOTTMiOiPSKm7cCs40gPGtzZ5VgTczKwRjZE0tWSFkv6z/w4jPMl3ZsfZ3y5kg8DrcCs/KTW4ZKOkvQHSffnRx3vVWkFuUZxnaQ2SY9J+r8l46bkR34/KOnCkuHrJH1T0gLgmNx/oaSFkn4vabykuZKWSDo5zzNO0h35UeD3SXpHhVgmSrohd88pefJsl6RP9bGM7wDH5mm/VLackZKuz9/hfElvzcMvkHRlSZxOHs0uIvzxp+E+pLPdID2mGOBK4BxgZMk01wAn5e65QGvuHgYsAY7K/XsDQ3pZz+l52hGkB6EtA8aSahl/IT3gbwhwK+mNceS4PlqyjACOz93/BdwMDCU/CjwP3wPYPXcfCrSXbOeDuXsicENZfEcCi3N8vS1jq/lK+4EfAF/P3e8tiecC4A/AbsAo4FlgaL1/d3/q9/GziayRLY+Iu3L3T4BpwJ8lnUs6MI4EHgJ+UzbfG4CVEXEvQESs3c56bomILgBJfyQ9yG8/YG5EdObhs4B3A9eTnp8/u2T+l4G23P0AsCHSQwEfIB3sISWHSyQdnuff7gP/8vskriElni5JI/q7DOBdwGSAiLhV0n55OQA3RsQGYIOkVaTHmndUsUzbBTkZWCMrf3BWAJeSagDLlV6XuPs2c6VHHPfnoVsbSrq7SftF+ct/Sq2PrZ+5vzEieta3uWd5EbFZUs8+9iXgaVJtYRDphUO9yvcifgZ8MyJ6bqT3axk9i6owrCfWStttTcr3DKyRHSTpmNw9Bbgzdz+j9PrND5dM+zzQc1/gEeAASUcBSNqr5KBcrQXAeySNygfmKcDtO7IR2QhSbWUzMBXY3o3n7wCLI+JnVSyjdNvLzSO/BUzSROCZKmpK1oR8JmCN7GHgU5J+BDwG/BDYl3QpZinpscY9ZgCXSXoJOIb0WsQfSBoOvAQcR3o0d1UiYqWkrwK3kc6u50TEr3ZiWy4FZkv6SF7mC9uZ/hzgofz4ZoDz+1jGYmCTpPtJ38N/lyznAuAqSYuBF4FP7cQ22C7Mj7A2MzNfJjIzM18msiYh6e+BC8sG/zkiPlSPeMwajS8TmZmZLxOZmZmTgZmZ4WRgZmY4GZiZGfD/AY3j3nm/YYrtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batc_normalization'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe79063b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of activation_layer')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activation_layer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1bf4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7250767e",
   "metadata": {},
   "source": [
    "## Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70645b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(3, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adagrad(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=3,beta=1.0)])\n",
    "    \n",
    "  \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        class_weight={0 : 0.64147775,\n",
    "                                      1 : 2.42539683,\n",
    "                                      2 : 0.97201018},\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f733338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3,6],   \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001,0.0001,0.00001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.0001],#[0,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.0001],#[0,0.001,0.0001],\n",
    "    'bias_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "    'activity_regularizer':[0.0001],#[0,0.001,0.0001],\n",
    "\n",
    "    #'dropout': [0,0.1,0.2,0.3,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['orthogonal','identity','uniform'],\n",
    "\n",
    "    'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    " \n",
    "    'batc_normalization':[False,True],\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9725295c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                          | 0/180 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2258C3828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D57EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 92.\n",
      "Epoch 00142: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/450 [2:17:57<?, ?it/s]\n",
      "\n",
      "\n",
      "  1%|▍                                                                                 | 1/180 [00:08<25:25,  8.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211CC85E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21987CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|▉                                                                                 | 2/180 [00:13<18:58,  6.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225849D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225915AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|█▎                                                                                | 3/180 [00:17<16:22,  5.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21F3A58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219C9FE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 193.\n",
      "Epoch 00243: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|█▊                                                                                | 4/180 [00:30<24:32,  8.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D89558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21987C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|██▎                                                                               | 5/180 [00:36<21:46,  7.47s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F20E6D3F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21987C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|██▋                                                                               | 6/180 [00:41<18:45,  6.47s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2258491F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A0C318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|███▏                                                                              | 7/180 [00:45<16:55,  5.87s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2258C3798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219BBEAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|███▋                                                                              | 8/180 [00:50<15:39,  5.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D89558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F228326D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  5%|████                                                                              | 9/180 [00:55<15:01,  5.27s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D89678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A0C0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  6%|████▌                                                                            | 10/180 [01:04<18:40,  6.59s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225A0C288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D89EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  6%|████▉                                                                            | 11/180 [01:10<17:43,  6.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21987CA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9EAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|█████▍                                                                           | 12/180 [01:16<17:09,  6.13s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211CF5A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F228581F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 210.\n",
      "Epoch 00260: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|█████▊                                                                           | 13/180 [01:31<24:36,  8.84s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2193523A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D89798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 58.\n",
      "Epoch 00108: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  8%|██████▎                                                                          | 14/180 [01:39<24:23,  8.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2259159D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9EA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  8%|██████▊                                                                          | 15/180 [01:45<21:44,  7.91s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2258C30D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F228581708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  9%|███████▏                                                                         | 16/180 [01:52<20:58,  7.68s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2259154C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2258C3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  9%|███████▋                                                                         | 17/180 [01:59<19:38,  7.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D9E948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2283265E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|████████                                                                         | 18/180 [02:05<19:07,  7.08s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2020E8168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22983DCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 11%|████████▌                                                                        | 19/180 [02:14<20:13,  7.54s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22983DAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F228431798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 11%|█████████                                                                        | 20/180 [02:21<19:38,  7.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F228431048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F229B5D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 12%|█████████▍                                                                       | 21/180 [02:28<19:12,  7.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21C066828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225971438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 176.\n",
      "Epoch 00226: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 12%|█████████▉                                                                       | 22/180 [02:44<26:20, 10.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D9E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F228431048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Epoch 00091: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 13%|██████████▎                                                                      | 23/180 [02:53<25:16,  9.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F229B5DF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9E5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 00063: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 13%|██████████▊                                                                      | 24/180 [03:01<23:31,  9.05s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21EAE4828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D898B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 00093: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 14%|███████████▎                                                                     | 25/180 [03:09<23:07,  8.95s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2271ACD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2284B6C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Epoch 00094: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 14%|███████████▋                                                                     | 26/180 [03:18<23:02,  8.98s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219341EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2271ACEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 15%|████████████▏                                                                    | 27/180 [03:25<21:10,  8.30s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22710F948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B232A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 16%|████████████▌                                                                    | 28/180 [03:34<21:25,  8.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211AA2168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB324C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 16%|█████████████                                                                    | 29/180 [03:43<22:01,  8.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21991CD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B001708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█████████████▌                                                                   | 30/180 [03:53<22:32,  9.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D9E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C8E3A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 158.\n",
      "Epoch 00208: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█████████████▉                                                                   | 31/180 [04:12<30:04, 12.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B131318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B001D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 18%|██████████████▍                                                                  | 32/180 [04:22<27:48, 11.27s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2271ACC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B131E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Epoch 00067: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 18%|██████████████▊                                                                  | 33/180 [04:32<26:33, 10.84s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219BBEAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219371678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 19%|███████████████▎                                                                 | 34/180 [04:45<27:57, 11.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C7E2678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B1315E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 19%|███████████████▊                                                                 | 35/180 [04:53<25:43, 10.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'sigmoid', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D9EC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AF0FB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|████████████████▏                                                                | 36/180 [05:01<23:34,  9.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B001B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A0C708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 00116: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 21%|████████████████▋                                                                | 37/180 [05:09<21:41,  9.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CB8B438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A0CE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 21%|█████████████████                                                                | 38/180 [05:13<18:21,  7.76s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219BBEA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C266DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 22%|█████████████████▌                                                               | 39/180 [05:18<16:10,  6.89s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219BBEB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225971EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 192.\n",
      "Epoch 00242: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 22%|██████████████████                                                               | 40/180 [05:30<19:24,  8.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2258C3828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C266948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 23%|██████████████████▍                                                              | 41/180 [05:35<17:27,  7.54s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C266AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219BBE708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 23%|██████████████████▉                                                              | 42/180 [05:40<15:23,  6.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225A0C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2258C3AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 24%|███████████████████▎                                                             | 43/180 [05:45<13:51,  6.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B001798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21991C168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 24%|███████████████████▊                                                             | 44/180 [05:50<13:00,  5.74s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21AC9EDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2258C35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|████████████████████▎                                                            | 45/180 [05:55<12:25,  5.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211CF5948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2283269D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 115.\n",
      "Epoch 00165: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 26%|████████████████████▋                                                            | 46/180 [06:06<15:55,  7.13s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C8E3EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2259714C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 26%|█████████████████████▏                                                           | 47/180 [06:11<14:56,  6.74s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2258C38B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225971A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 27%|█████████████████████▌                                                           | 48/180 [06:17<14:10,  6.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C2668B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2258C3A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 208.\n",
      "Epoch 00258: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 27%|██████████████████████                                                           | 49/180 [06:32<19:29,  8.93s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F218234CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C8E3048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 53.\n",
      "Epoch 00103: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 28%|██████████████████████▌                                                          | 50/180 [06:40<18:40,  8.62s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2283265E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F229B5D8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 28%|██████████████████████▉                                                          | 51/180 [06:45<16:30,  7.68s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21991C288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B001EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 29%|███████████████████████▍                                                         | 52/180 [06:51<14:58,  7.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F218234288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219352168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 29%|███████████████████████▊                                                         | 53/180 [06:57<14:05,  6.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2271AC948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2297EEDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30%|████████████████████████▎                                                        | 54/180 [07:02<13:20,  6.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2271ACDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB320D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 31%|████████████████████████▊                                                        | 55/180 [07:08<13:01,  6.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F228326168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B143A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 31%|█████████████████████████▏                                                       | 56/180 [07:15<13:06,  6.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21991CA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2271AC438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 32%|█████████████████████████▋                                                       | 57/180 [07:22<13:18,  6.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F229B5D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F211FD0CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 182.\n",
      "Epoch 00232: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 32%|██████████████████████████                                                       | 58/180 [07:38<19:29,  9.59s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2182341F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2283265E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|██████████████████████████▌                                                      | 59/180 [07:47<18:32,  9.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2299B5708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2284B6948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███████████████████████████                                                      | 60/180 [07:54<17:01,  8.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225915828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F211FD09D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Epoch 00095: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 34%|███████████████████████████▍                                                     | 61/180 [08:03<17:32,  8.84s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C2661F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21991CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 34%|███████████████████████████▉                                                     | 62/180 [08:10<15:52,  8.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21991C558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225915798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 35%|████████████████████████████▎                                                    | 63/180 [08:16<14:40,  7.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C8E35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22985E5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 36%|████████████████████████████▊                                                    | 64/180 [08:24<15:10,  7.85s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2259718B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CC8CE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 36%|█████████████████████████████▎                                                   | 65/180 [08:33<15:36,  8.14s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D9E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B0BE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 37%|█████████████████████████████▋                                                   | 66/180 [08:42<15:39,  8.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225915A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C86C5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 149.\n",
      "Epoch 00199: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 37%|██████████████████████████████▏                                                  | 67/180 [09:07<25:04, 13.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219BBE8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CC1A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 38%|██████████████████████████████▌                                                  | 68/180 [09:16<22:40, 12.14s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CB8BA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C73D708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 38%|███████████████████████████████                                                  | 69/180 [09:26<20:56, 11.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2297EEDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB8B0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 72.\n",
      "Epoch 00122: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 39%|███████████████████████████████▌                                                 | 70/180 [09:40<22:15, 12.14s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226B149D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB8B5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 00097: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 39%|███████████████████████████████▉                                                 | 71/180 [09:52<22:04, 12.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C86C1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B14318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|████████████████████████████████▍                                                | 72/180 [10:01<20:08, 11.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22985E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B3E4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 90.\n",
      "Epoch 00140: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 41%|████████████████████████████████▊                                                | 73/180 [10:10<18:39, 10.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C73DB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2271AC708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 41%|█████████████████████████████████▎                                               | 74/180 [10:15<15:32,  8.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284B6948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219371048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 42%|█████████████████████████████████▊                                               | 75/180 [10:19<13:18,  7.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CB8B438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B232CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 192.\n",
      "Epoch 00242: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 42%|██████████████████████████████████▏                                              | 76/180 [10:30<14:52,  8.59s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22863CDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA7CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 43%|██████████████████████████████████▋                                              | 77/180 [10:35<12:55,  7.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2259158B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9E8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 43%|███████████████████████████████████                                              | 78/180 [10:40<11:14,  6.62s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2299B5438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B0011F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 44%|███████████████████████████████████▌                                             | 79/180 [10:44<10:08,  6.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21C02AB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C73DAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 44%|████████████████████████████████████                                             | 80/180 [10:48<09:03,  5.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F229B5D8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F211FD0708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 45%|████████████████████████████████████▍                                            | 81/180 [10:53<08:32,  5.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22985E048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9E3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 94.\n",
      "Epoch 00144: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 46%|████████████████████████████████████▉                                            | 82/180 [11:03<10:42,  6.56s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C73D948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C238558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 46%|█████████████████████████████████████▎                                           | 83/180 [11:08<10:09,  6.28s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21AC9E1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA7C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 47%|█████████████████████████████████████▊                                           | 84/180 [11:14<09:47,  6.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21991C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2182341F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 207.\n",
      "Epoch 00257: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 47%|██████████████████████████████████████▎                                          | 85/180 [11:29<13:59,  8.84s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2193718B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219410708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 58.\n",
      "Epoch 00108: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 48%|██████████████████████████████████████▋                                          | 86/180 [11:38<13:33,  8.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2258C33A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F211FD0F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 48%|███████████████████████████████████████▏                                         | 87/180 [11:43<12:05,  7.81s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22985E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2259151F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 49%|███████████████████████████████████████▌                                         | 88/180 [11:49<10:59,  7.16s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284B6B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C73D5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 49%|████████████████████████████████████████                                         | 89/180 [11:55<10:10,  6.71s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219371288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA7CC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|████████████████████████████████████████▌                                        | 90/180 [12:01<09:37,  6.42s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B0018B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B0BE948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 170.\n",
      "Epoch 00220: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 51%|████████████████████████████████████████▉                                        | 91/180 [12:15<13:08,  8.86s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F228326708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B14048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 51%|█████████████████████████████████████████▍                                       | 92/180 [12:21<11:47,  8.04s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22985E9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219C56318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 52%|█████████████████████████████████████████▊                                       | 93/180 [12:28<10:56,  7.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21C066948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B14558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 174.\n",
      "Epoch 00224: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 52%|██████████████████████████████████████████▎                                      | 94/180 [12:41<13:30,  9.42s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CB32168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B14678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Epoch 00085: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 53%|██████████████████████████████████████████▊                                      | 95/180 [12:49<12:24,  8.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211CF5C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B2320D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 00064: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 53%|███████████████████████████████████████████▏                                     | 96/180 [12:55<11:07,  7.95s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D9EE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B232948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 99.\n",
      "Epoch 00149: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 54%|███████████████████████████████████████████▋                                     | 97/180 [13:06<12:24,  8.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F20CC69438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B2320D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 54%|████████████████████████████████████████████                                     | 98/180 [13:12<11:05,  8.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C73D558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B001828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 55%|████████████████████████████████████████████▌                                    | 99/180 [13:18<10:09,  7.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2258C35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B13D048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 56%|████████████████████████████████████████████▍                                   | 100/180 [13:26<10:05,  7.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225849E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA9DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 56%|████████████████████████████████████████████▉                                   | 101/180 [13:34<10:18,  7.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219C56AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA02DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 57%|█████████████████████████████████████████████▎                                  | 102/180 [13:43<10:37,  8.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21991C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B09D048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 162.\n",
      "Epoch 00212: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 57%|█████████████████████████████████████████████▊                                  | 103/180 [14:02<14:24, 11.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284E5DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CAC44C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 58%|██████████████████████████████████████████████▏                                 | 104/180 [14:11<13:27, 10.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219B57EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2284E5948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 00064: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 58%|██████████████████████████████████████████████▋                                 | 105/180 [14:20<12:44, 10.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22985E4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219B57708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 90.\n",
      "Epoch 00140: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 59%|███████████████████████████████████████████████                                 | 106/180 [14:34<13:57, 11.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284B65E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226CF2DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 59%|███████████████████████████████████████████████▌                                | 107/180 [14:42<12:41, 10.43s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C735E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22985EE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|████████████████████████████████████████████████                                | 108/180 [14:51<11:49,  9.85s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22863CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219BBE1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 61%|████████████████████████████████████████████████▍                               | 109/180 [14:57<10:13,  8.64s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284E5AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 61%|████████████████████████████████████████████████▉                               | 110/180 [15:01<08:41,  7.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211FD0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2258C35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 62%|█████████████████████████████████████████████████▎                              | 111/180 [15:06<07:34,  6.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B232A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 192.\n",
      "Epoch 00242: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 62%|█████████████████████████████████████████████████▊                              | 112/180 [15:18<09:24,  8.30s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284E58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225990AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 63%|██████████████████████████████████████████████████▏                             | 113/180 [15:24<08:26,  7.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225A0CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA02E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 63%|██████████████████████████████████████████████████▋                             | 114/180 [15:29<07:18,  6.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B13D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9ECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 64%|███████████████████████████████████████████████████                             | 115/180 [15:33<06:29,  5.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21F3A5318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B232948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 64%|███████████████████████████████████████████████████▌                            | 116/180 [15:38<05:56,  5.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2298370D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B13D8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 65%|████████████████████████████████████████████████████                            | 117/180 [15:41<05:17,  5.04s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225990558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2284E5798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 118.\n",
      "Epoch 00168: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 66%|████████████████████████████████████████████████████▍                           | 118/180 [15:52<07:02,  6.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2299B5C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225990798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 66%|████████████████████████████████████████████████████▉                           | 119/180 [15:58<06:32,  6.43s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D9E318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA9D1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|█████████████████████████████████████████████████████▎                          | 120/180 [16:04<06:12,  6.21s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F229837708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA02048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 207.\n",
      "Epoch 00257: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|█████████████████████████████████████████████████████▊                          | 121/180 [16:19<08:41,  8.84s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225A0CE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F211FD0048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 58.\n",
      "Epoch 00108: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 68%|██████████████████████████████████████████████████████▏                         | 122/180 [16:27<08:23,  8.68s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211FD00D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21987CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 68%|██████████████████████████████████████████████████████▋                         | 123/180 [16:33<07:25,  7.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2297EEAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA02F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 69%|███████████████████████████████████████████████████████                         | 124/180 [16:38<06:40,  7.14s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C73D8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219B578B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 69%|███████████████████████████████████████████████████████▌                        | 125/180 [16:44<06:08,  6.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225990438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9ECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 70%|████████████████████████████████████████████████████████                        | 126/180 [16:50<05:45,  6.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA7CEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2259714C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 141.\n",
      "Epoch 00191: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 71%|████████████████████████████████████████████████████████▍                       | 127/180 [17:03<07:27,  8.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225971828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2284B6A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 71%|████████████████████████████████████████████████████████▉                       | 128/180 [17:09<06:42,  7.73s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225990438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C266EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 72%|█████████████████████████████████████████████████████████▎                      | 129/180 [17:15<06:10,  7.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C73DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22985E9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 180.\n",
      "Epoch 00230: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 72%|█████████████████████████████████████████████████████████▊                      | 130/180 [17:30<08:04,  9.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2194105E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219BBEB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 73%|██████████████████████████████████████████████████████████▏                     | 131/180 [17:37<07:09,  8.77s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2299B5438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B143A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 00063: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 73%|██████████████████████████████████████████████████████████▋                     | 132/180 [17:44<06:29,  8.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B13D708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B14D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 84.\n",
      "Epoch 00134: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 74%|███████████████████████████████████████████████████████████                     | 133/180 [17:54<06:51,  8.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA7CA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21987C828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "Epoch 00147: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 74%|███████████████████████████████████████████████████████████▌                    | 134/180 [18:05<07:18,  9.53s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226D9EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C8E3EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 75%|████████████████████████████████████████████████████████████                    | 135/180 [18:11<06:22,  8.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C73D678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DE8CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 76%|████████████████████████████████████████████████████████████▍                   | 136/180 [18:20<06:14,  8.50s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA02288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226C9B5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 76%|████████████████████████████████████████████████████████████▉                   | 137/180 [18:28<06:06,  8.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22985E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AFEA948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 77%|█████████████████████████████████████████████████████████████▎                  | 138/180 [18:37<05:57,  8.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219C56438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AF6ACA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 193.\n",
      "Epoch 00243: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 77%|█████████████████████████████████████████████████████████████▊                  | 139/180 [18:57<08:17, 12.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226B141F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AF3D828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 78%|██████████████████████████████████████████████████████████████▏                 | 140/180 [19:07<07:30, 11.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219B57708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AF3D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 78%|██████████████████████████████████████████████████████████████▋                 | 141/180 [19:17<07:08, 10.98s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C266C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C6CCAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 79%|███████████████████████████████████████████████████████████████                 | 142/180 [19:28<06:59, 11.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225A848B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AF3DF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Epoch 00083: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 79%|███████████████████████████████████████████████████████████████▌                | 143/180 [19:39<06:43, 10.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'elu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225A8F438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB11CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80%|████████████████████████████████████████████████████████████████                | 144/180 [19:49<06:25, 10.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA02288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219923318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 115.\n",
      "Epoch 00165: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 81%|████████████████████████████████████████████████████████████████▍               | 145/180 [19:58<06:01, 10.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C8E39D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21C008678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 81%|████████████████████████████████████████████████████████████████▉               | 146/180 [20:03<04:53,  8.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C238708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA7C0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 82%|█████████████████████████████████████████████████████████████████▎              | 147/180 [20:08<04:03,  7.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2199238B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB168B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 171.\n",
      "Epoch 00221: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 82%|█████████████████████████████████████████████████████████████████▊              | 148/180 [20:19<04:36,  8.64s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22AEFD168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D89AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|██████████████████████████████████████████████████████████████████▏             | 149/180 [20:25<04:00,  7.76s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA7CA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C775708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|██████████████████████████████████████████████████████████████████▋             | 150/180 [20:29<03:24,  6.81s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219923678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AEFD288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 84%|███████████████████████████████████████████████████████████████████             | 151/180 [20:34<02:57,  6.13s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211FD0B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB16B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 84%|███████████████████████████████████████████████████████████████████▌            | 152/180 [20:39<02:38,  5.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219B57168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AEFD168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 85%|████████████████████████████████████████████████████████████████████            | 153/180 [20:43<02:24,  5.34s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C775CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA02828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 86%|████████████████████████████████████████████████████████████████████▍           | 154/180 [20:52<02:45,  6.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C73DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C775EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 86%|████████████████████████████████████████████████████████████████████▉           | 155/180 [20:58<02:34,  6.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CB9B438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21C314B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 87%|█████████████████████████████████████████████████████████████████████▎          | 156/180 [21:03<02:24,  6.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C8E3288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226D9E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 207.\n",
      "Epoch 00257: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 87%|█████████████████████████████████████████████████████████████████████▊          | 157/180 [21:19<03:22,  8.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B0BEEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C7758B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 53.\n",
      "Epoch 00103: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 88%|██████████████████████████████████████████████████████████████████████▏         | 158/180 [21:26<03:07,  8.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22AEFDE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CA028B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 88%|██████████████████████████████████████████████████████████████████████▋         | 159/180 [21:32<02:40,  7.64s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22AFEAF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C7751F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 89%|███████████████████████████████████████████████████████████████████████         | 160/180 [21:37<02:18,  6.91s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F225990B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F229837318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 89%|███████████████████████████████████████████████████████████████████████▌        | 161/180 [21:42<02:00,  6.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219C56AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A0CC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 90%|████████████████████████████████████████████████████████████████████████        | 162/180 [21:47<01:46,  5.94s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284E53A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C6CC288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 91%|████████████████████████████████████████████████████████████████████████▍       | 163/180 [21:54<01:42,  6.05s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CB32B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A8FEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 91%|████████████████████████████████████████████████████████████████████████▉       | 164/180 [21:59<01:34,  5.93s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA7CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A8F3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 92%|█████████████████████████████████████████████████████████████████████████▎      | 165/180 [22:05<01:27,  5.84s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CC8CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB9BF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 177.\n",
      "Epoch 00227: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 92%|█████████████████████████████████████████████████████████████████████████▊      | 166/180 [22:20<02:01,  8.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CB9BEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB165E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 00089: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 93%|██████████████████████████████████████████████████████████████████████████▏     | 167/180 [22:28<01:51,  8.56s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B001D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CB9B798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 93%|██████████████████████████████████████████████████████████████████████████▋     | 168/180 [22:35<01:34,  7.88s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F211CF59D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A0CEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 92.\n",
      "Epoch 00142: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 94%|███████████████████████████████████████████████████████████████████████████     | 169/180 [22:45<01:33,  8.50s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA9D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226B14678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 94%|███████████████████████████████████████████████████████████████████████████▌    | 170/180 [22:51<01:18,  7.81s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2193920D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225A0CEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 95%|████████████████████████████████████████████████████████████████████████████    | 171/180 [22:57<01:05,  7.33s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B0BE438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219AD3828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 96%|████████████████████████████████████████████████████████████████████████████▍   | 172/180 [23:06<01:01,  7.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2286FEAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B10B048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 96%|████████████████████████████████████████████████████████████████████████████▉   | 173/180 [23:14<00:55,  7.92s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'orthogonal', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C6CCE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DE654C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 97%|█████████████████████████████████████████████████████████████████████████████▎  | 174/180 [23:22<00:48,  8.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C775288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22F0CF828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 145.\n",
      "Epoch 00195: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 97%|█████████████████████████████████████████████████████████████████████████████▊  | 175/180 [23:40<00:53, 10.80s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22DE83558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22AEF7318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████▏ | 176/180 [23:49<00:41, 10.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'identity', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22DE83F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CC2F0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00062: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████▋ | 177/180 [23:58<00:30, 10.06s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219B570D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C7754C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Epoch 00100: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 99%|███████████████████████████████████████████████████████████████████████████████ | 178/180 [24:13<00:23, 11.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2196BDB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219A09678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 99%|███████████████████████████████████████████████████████████████████████████████▌| 179/180 [24:22<00:10, 10.68s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F229B5D828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219B57798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 180/180 [24:31<00:00,  8.17s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c5eb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/051022112345.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8b0e981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142</td>\n",
       "      <td>0.788577</td>\n",
       "      <td>[0.18021978 0.27045074 0.39662445]</td>\n",
       "      <td>0.808301</td>\n",
       "      <td>[0.27868852 0.24324326 0.33928573]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>0.843735</td>\n",
       "      <td>[0.65913373 0.         0.23398326]</td>\n",
       "      <td>0.826114</td>\n",
       "      <td>[0.67647064 0.         0.1686747 ]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>0.852544</td>\n",
       "      <td>[0.3621795  0.22812499 0.00757576]</td>\n",
       "      <td>0.857135</td>\n",
       "      <td>[0.38993707 0.21518989 0.        ]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>[0.44206774 0.26959246 0.47839507]</td>\n",
       "      <td>0.691966</td>\n",
       "      <td>[0.5298013  0.30136985 0.5189873 ]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>0.832624</td>\n",
       "      <td>[0.57342654 0.20578778 0.42231074]</td>\n",
       "      <td>0.828862</td>\n",
       "      <td>[0.6231156  0.2535211  0.37500003]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>61</td>\n",
       "      <td>0.863381</td>\n",
       "      <td>[0.55925435 0.25757578 0.4093567 ]</td>\n",
       "      <td>0.858388</td>\n",
       "      <td>[0.7520001  0.39215687 0.27160496]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>62</td>\n",
       "      <td>0.881856</td>\n",
       "      <td>[0.53061223 0.17721517 0.2882096 ]</td>\n",
       "      <td>0.862929</td>\n",
       "      <td>[0.72727275 0.17142858 0.28915665]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>100</td>\n",
       "      <td>0.467637</td>\n",
       "      <td>[0.82368773 0.67353946 0.77327937]</td>\n",
       "      <td>0.688074</td>\n",
       "      <td>[0.7623763  0.20689656 0.59016395]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>51</td>\n",
       "      <td>0.729925</td>\n",
       "      <td>[0.49216306 0.2762431  0.39393938]</td>\n",
       "      <td>0.739474</td>\n",
       "      <td>[0.6728972  0.2        0.37288135]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>51</td>\n",
       "      <td>0.747841</td>\n",
       "      <td>[0.45739913 0.22792022 0.32283464]</td>\n",
       "      <td>0.743485</td>\n",
       "      <td>[0.6334842  0.19444443 0.17977528]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "0             142  0.788577  [0.18021978 0.27045074 0.39662445]  0.808301   \n",
       "1              51  0.843735  [0.65913373 0.         0.23398326]  0.826114   \n",
       "2              51  0.852544  [0.3621795  0.22812499 0.00757576]  0.857135   \n",
       "3             243  0.680800  [0.44206774 0.26959246 0.47839507]  0.691966   \n",
       "4              81  0.832624  [0.57342654 0.20578778 0.42231074]  0.828862   \n",
       "..            ...       ...                                 ...       ...   \n",
       "175            61  0.863381  [0.55925435 0.25757578 0.4093567 ]  0.858388   \n",
       "176            62  0.881856  [0.53061223 0.17721517 0.2882096 ]  0.862929   \n",
       "177           100  0.467637  [0.82368773 0.67353946 0.77327937]  0.688074   \n",
       "178            51  0.729925  [0.49216306 0.2762431  0.39393938]  0.739474   \n",
       "179            51  0.747841  [0.45739913 0.22792022 0.32283464]  0.743485   \n",
       "\n",
       "                        val_fbeta_score activation_layer  \\\n",
       "0    [0.27868852 0.24324326 0.33928573]          sigmoid   \n",
       "1    [0.67647064 0.         0.1686747 ]          sigmoid   \n",
       "2    [0.38993707 0.21518989 0.        ]          sigmoid   \n",
       "3    [0.5298013  0.30136985 0.5189873 ]          sigmoid   \n",
       "4    [0.6231156  0.2535211  0.37500003]          sigmoid   \n",
       "..                                  ...              ...   \n",
       "175  [0.7520001  0.39215687 0.27160496]             relu   \n",
       "176  [0.72727275 0.17142858 0.28915665]             relu   \n",
       "177  [0.7623763  0.20689656 0.59016395]             relu   \n",
       "178  [0.6728972  0.2        0.37288135]             relu   \n",
       "179  [0.6334842  0.19444443 0.17977528]             relu   \n",
       "\n",
       "     activity_regularizer  batc_normalization  batch_size  bias_regularizer  \\\n",
       "0                  0.0001               False          64            0.0001   \n",
       "1                  0.0001               False          64            0.0001   \n",
       "2                  0.0001               False          64            0.0001   \n",
       "3                  0.0001               False          64            0.0001   \n",
       "4                  0.0001               False          64            0.0001   \n",
       "..                    ...                 ...         ...               ...   \n",
       "175                0.0001                True          64            0.0001   \n",
       "176                0.0001                True          64            0.0001   \n",
       "177                0.0001                True          64            0.0001   \n",
       "178                0.0001                True          64            0.0001   \n",
       "179                0.0001                True          64            0.0001   \n",
       "\n",
       "     dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
       "0          0  100000            55              3             50   \n",
       "1          0  100000            55              3             50   \n",
       "2          0  100000            55              3             50   \n",
       "3          0  100000            55              3             50   \n",
       "4          0  100000            55              3             50   \n",
       "..       ...     ...           ...            ...            ...   \n",
       "175        0  100000            55              6             50   \n",
       "176        0  100000            55              6             50   \n",
       "177        0  100000            55              6             50   \n",
       "178        0  100000            55              6             50   \n",
       "179        0  100000            55              6             50   \n",
       "\n",
       "    kernel_initializer  kernel_regularizer_l1  kernel_regularizer_l2  \\\n",
       "0           orthogonal                 0.0001                 0.0001   \n",
       "1           orthogonal                 0.0001                 0.0001   \n",
       "2           orthogonal                 0.0001                 0.0001   \n",
       "3             identity                 0.0001                 0.0001   \n",
       "4             identity                 0.0001                 0.0001   \n",
       "..                 ...                    ...                    ...   \n",
       "175           identity                 0.0001                 0.0001   \n",
       "176           identity                 0.0001                 0.0001   \n",
       "177            uniform                 0.0001                 0.0001   \n",
       "178            uniform                 0.0001                 0.0001   \n",
       "179            uniform                 0.0001                 0.0001   \n",
       "\n",
       "    last_activation       lr  \n",
       "0           sigmoid  0.00100  \n",
       "1           sigmoid  0.00010  \n",
       "2           sigmoid  0.00001  \n",
       "3           sigmoid  0.00100  \n",
       "4           sigmoid  0.00010  \n",
       "..              ...      ...  \n",
       "175         sigmoid  0.00010  \n",
       "176         sigmoid  0.00001  \n",
       "177         sigmoid  0.00100  \n",
       "178         sigmoid  0.00010  \n",
       "179         sigmoid  0.00001  \n",
       "\n",
       "[180 rows x 20 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8870ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fe1f3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>142</td>\n",
       "      <td>0.462458</td>\n",
       "      <td>[0.7496423  0.59171593 0.72505087]</td>\n",
       "      <td>0.642904</td>\n",
       "      <td>[0.73333335 0.35820895 0.62222224]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>134</td>\n",
       "      <td>0.463715</td>\n",
       "      <td>[0.83029455 0.6125     0.8       ]</td>\n",
       "      <td>0.652939</td>\n",
       "      <td>[0.6871795  0.29411763 0.5546218 ]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>149</td>\n",
       "      <td>0.453368</td>\n",
       "      <td>[0.79096043 0.61829656 0.7554672 ]</td>\n",
       "      <td>0.662958</td>\n",
       "      <td>[0.70212764 0.2608696  0.54399997]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>140</td>\n",
       "      <td>0.401244</td>\n",
       "      <td>[0.8306011  0.72527474 0.8183556 ]</td>\n",
       "      <td>0.676724</td>\n",
       "      <td>[0.69      0.3272727 0.5511811]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>100</td>\n",
       "      <td>0.467637</td>\n",
       "      <td>[0.82368773 0.67353946 0.77327937]</td>\n",
       "      <td>0.688074</td>\n",
       "      <td>[0.7623763  0.20689656 0.59016395]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>51</td>\n",
       "      <td>1.043084</td>\n",
       "      <td>[0.40298507 0.18289088 0.33140656]</td>\n",
       "      <td>1.062600</td>\n",
       "      <td>[0.483146   0.19277109 0.3471074 ]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>51</td>\n",
       "      <td>1.090095</td>\n",
       "      <td>[0.32225913 0.21700878 0.34871793]</td>\n",
       "      <td>1.089784</td>\n",
       "      <td>[0.37209302 0.24390246 0.203125  ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>51</td>\n",
       "      <td>1.071666</td>\n",
       "      <td>[0.37320575 0.20630373 0.3768116 ]</td>\n",
       "      <td>1.091396</td>\n",
       "      <td>[0.33734939 0.11428571 0.39726025]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>51</td>\n",
       "      <td>1.052512</td>\n",
       "      <td>[0.44       0.25       0.36679533]</td>\n",
       "      <td>1.091743</td>\n",
       "      <td>[0.39215687 0.21428572 0.28965518]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>51</td>\n",
       "      <td>1.046461</td>\n",
       "      <td>[0.44171777 0.24000001 0.33532932]</td>\n",
       "      <td>1.095747</td>\n",
       "      <td>[0.22535212 0.10619469 0.28346458]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>orthogonal</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "168           142  0.462458  [0.7496423  0.59171593 0.72505087]  0.642904   \n",
       "132           134  0.463715  [0.83029455 0.6125     0.8       ]  0.652939   \n",
       "96            149  0.453368  [0.79096043 0.61829656 0.7554672 ]  0.662958   \n",
       "105           140  0.401244  [0.8306011  0.72527474 0.8183556 ]  0.676724   \n",
       "177           100  0.467637  [0.82368773 0.67353946 0.77327937]  0.688074   \n",
       "..            ...       ...                                 ...       ...   \n",
       "172            51  1.043084  [0.40298507 0.18289088 0.33140656]  1.062600   \n",
       "101            51  1.090095  [0.32225913 0.21700878 0.34871793]  1.089784   \n",
       "64             51  1.071666  [0.37320575 0.20630373 0.3768116 ]  1.091396   \n",
       "65             51  1.052512  [0.44       0.25       0.36679533]  1.091743   \n",
       "173            51  1.046461  [0.44171777 0.24000001 0.33532932]  1.095747   \n",
       "\n",
       "                        val_fbeta_score activation_layer  \\\n",
       "168  [0.73333335 0.35820895 0.62222224]             relu   \n",
       "132  [0.6871795  0.29411763 0.5546218 ]              elu   \n",
       "96   [0.70212764 0.2608696  0.54399997]             selu   \n",
       "105     [0.69      0.3272727 0.5511811]             selu   \n",
       "177  [0.7623763  0.20689656 0.59016395]             relu   \n",
       "..                                  ...              ...   \n",
       "172  [0.483146   0.19277109 0.3471074 ]             relu   \n",
       "101  [0.37209302 0.24390246 0.203125  ]             selu   \n",
       "64   [0.33734939 0.11428571 0.39726025]             tanh   \n",
       "65   [0.39215687 0.21428572 0.28965518]             tanh   \n",
       "173  [0.22535212 0.10619469 0.28346458]             relu   \n",
       "\n",
       "     activity_regularizer  batc_normalization  batch_size  bias_regularizer  \\\n",
       "168                0.0001                True          64            0.0001   \n",
       "132                0.0001                True          64            0.0001   \n",
       "96                 0.0001                True          64            0.0001   \n",
       "105                0.0001                True          64            0.0001   \n",
       "177                0.0001                True          64            0.0001   \n",
       "..                    ...                 ...         ...               ...   \n",
       "172                0.0001                True          64            0.0001   \n",
       "101                0.0001                True          64            0.0001   \n",
       "64                 0.0001                True          64            0.0001   \n",
       "65                 0.0001                True          64            0.0001   \n",
       "173                0.0001                True          64            0.0001   \n",
       "\n",
       "     dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
       "168        0  100000            55              3             50   \n",
       "132        0  100000            55              3             50   \n",
       "96         0  100000            55              3             50   \n",
       "105        0  100000            55              6             50   \n",
       "177        0  100000            55              6             50   \n",
       "..       ...     ...           ...            ...            ...   \n",
       "172        0  100000            55              6             50   \n",
       "101        0  100000            55              6             50   \n",
       "64         0  100000            55              6             50   \n",
       "65         0  100000            55              6             50   \n",
       "173        0  100000            55              6             50   \n",
       "\n",
       "    kernel_initializer  kernel_regularizer_l1  kernel_regularizer_l2  \\\n",
       "168            uniform                 0.0001                 0.0001   \n",
       "132            uniform                 0.0001                 0.0001   \n",
       "96             uniform                 0.0001                 0.0001   \n",
       "105            uniform                 0.0001                 0.0001   \n",
       "177            uniform                 0.0001                 0.0001   \n",
       "..                 ...                    ...                    ...   \n",
       "172         orthogonal                 0.0001                 0.0001   \n",
       "101         orthogonal                 0.0001                 0.0001   \n",
       "64          orthogonal                 0.0001                 0.0001   \n",
       "65          orthogonal                 0.0001                 0.0001   \n",
       "173         orthogonal                 0.0001                 0.0001   \n",
       "\n",
       "    last_activation       lr  \n",
       "168         sigmoid  0.00100  \n",
       "132         sigmoid  0.00100  \n",
       "96          sigmoid  0.00100  \n",
       "105         sigmoid  0.00100  \n",
       "177         sigmoid  0.00100  \n",
       "..              ...      ...  \n",
       "172         sigmoid  0.00010  \n",
       "101         sigmoid  0.00001  \n",
       "64          sigmoid  0.00010  \n",
       "65          sigmoid  0.00001  \n",
       "173         sigmoid  0.00001  \n",
       "\n",
       "[180 rows x 20 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caa23a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_layers')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_layers'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e55ccb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of lr')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'lr'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "553340f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of kernel_initializer')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_initializer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac4ac88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of activation_layer')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activation_layer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f800331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batc_normalization')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batc_normalization'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1d8f2",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(3, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adagrad(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=3,beta=1.0)])\n",
    "    \n",
    "  \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        class_weight={0 : 0.64147775,\n",
    "                                      1 : 2.42539683,\n",
    "                                      2 : 0.97201018},\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07c695af",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3],   \n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.1,0.01,0.001,0.0001],\n",
    "    'kernel_regularizer_l2':[0.1,0.01,0.001,0.0001],\n",
    "    'bias_regularizer':[0.1,0.01,0.001,0.0001],\n",
    "    'activity_regularizer':[0.1,0.01,0.001,0.0001],\n",
    "\n",
    "    #'dropout': [0,0.1,0.2,0.3,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['uniform'],\n",
    "\n",
    "    'activation_layer':['sigmoid','tanh','selu','elu','relu'],\n",
    " \n",
    "    'batc_normalization':[False,True],\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b0b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aba74c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/051022120641.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88908f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157</td>\n",
       "      <td>0.763430</td>\n",
       "      <td>[0.        0.        0.5107212]</td>\n",
       "      <td>0.763316</td>\n",
       "      <td>[0.        0.        0.5078125]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143</td>\n",
       "      <td>0.764906</td>\n",
       "      <td>[0.         0.24165708 0.        ]</td>\n",
       "      <td>0.764889</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148</td>\n",
       "      <td>0.764132</td>\n",
       "      <td>[0.        0.        0.5107212]</td>\n",
       "      <td>0.764032</td>\n",
       "      <td>[0.        0.        0.5078125]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>0.764111</td>\n",
       "      <td>[0.        0.        0.5107212]</td>\n",
       "      <td>0.763977</td>\n",
       "      <td>[0.        0.        0.5078125]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>203</td>\n",
       "      <td>0.753764</td>\n",
       "      <td>[0.         0.19631903 0.3800623 ]</td>\n",
       "      <td>0.753720</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2555</th>\n",
       "      <td>153</td>\n",
       "      <td>0.686211</td>\n",
       "      <td>[0.80551726 0.6261981  0.8040817 ]</td>\n",
       "      <td>0.922833</td>\n",
       "      <td>[0.69273746 0.2368421  0.5354331 ]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>204</td>\n",
       "      <td>0.557201</td>\n",
       "      <td>[0.9124669  0.8015564  0.89748555]</td>\n",
       "      <td>0.982316</td>\n",
       "      <td>[0.6631016  0.31428573 0.464     ]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>168</td>\n",
       "      <td>0.518566</td>\n",
       "      <td>[0.796034   0.64102566 0.78431374]</td>\n",
       "      <td>0.721161</td>\n",
       "      <td>[0.72131157 0.3030303  0.54135346]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>145</td>\n",
       "      <td>0.454206</td>\n",
       "      <td>[0.7788595 0.631579  0.780198 ]</td>\n",
       "      <td>0.652408</td>\n",
       "      <td>[0.6999999 0.3125    0.5084745]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>108</td>\n",
       "      <td>0.506794</td>\n",
       "      <td>[0.7363897 0.5400594 0.7261663]</td>\n",
       "      <td>0.659622</td>\n",
       "      <td>[0.6779662  0.35443038 0.5714286 ]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2560 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "0              157  0.763430     [0.        0.        0.5107212]  0.763316   \n",
       "1              143  0.764906  [0.         0.24165708 0.        ]  0.764889   \n",
       "2              148  0.764132     [0.        0.        0.5107212]  0.764032   \n",
       "3              149  0.764111     [0.        0.        0.5107212]  0.763977   \n",
       "4              203  0.753764  [0.         0.19631903 0.3800623 ]  0.753720   \n",
       "...            ...       ...                                 ...       ...   \n",
       "2555           153  0.686211  [0.80551726 0.6261981  0.8040817 ]  0.922833   \n",
       "2556           204  0.557201  [0.9124669  0.8015564  0.89748555]  0.982316   \n",
       "2557           168  0.518566  [0.796034   0.64102566 0.78431374]  0.721161   \n",
       "2558           145  0.454206     [0.7788595 0.631579  0.780198 ]  0.652408   \n",
       "2559           108  0.506794     [0.7363897 0.5400594 0.7261663]  0.659622   \n",
       "\n",
       "                         val_fbeta_score activation_layer  \\\n",
       "0        [0.        0.        0.5078125]          sigmoid   \n",
       "1     [0.         0.24770641 0.        ]          sigmoid   \n",
       "2        [0.        0.        0.5078125]          sigmoid   \n",
       "3        [0.        0.        0.5078125]          sigmoid   \n",
       "4     [0.         0.24770641 0.        ]          sigmoid   \n",
       "...                                  ...              ...   \n",
       "2555  [0.69273746 0.2368421  0.5354331 ]             relu   \n",
       "2556  [0.6631016  0.31428573 0.464     ]             relu   \n",
       "2557  [0.72131157 0.3030303  0.54135346]             relu   \n",
       "2558     [0.6999999 0.3125    0.5084745]             relu   \n",
       "2559  [0.6779662  0.35443038 0.5714286 ]             relu   \n",
       "\n",
       "      activity_regularizer  batc_normalization  batch_size  bias_regularizer  \\\n",
       "0                   0.1000               False          64            0.1000   \n",
       "1                   0.1000               False          64            0.1000   \n",
       "2                   0.1000               False          64            0.1000   \n",
       "3                   0.1000               False          64            0.1000   \n",
       "4                   0.1000               False          64            0.1000   \n",
       "...                    ...                 ...         ...               ...   \n",
       "2555                0.0001                True          64            0.0001   \n",
       "2556                0.0001                True          64            0.0001   \n",
       "2557                0.0001                True          64            0.0001   \n",
       "2558                0.0001                True          64            0.0001   \n",
       "2559                0.0001                True          64            0.0001   \n",
       "\n",
       "      dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
       "0           0  100000            55              3             50   \n",
       "1           0  100000            55              3             50   \n",
       "2           0  100000            55              3             50   \n",
       "3           0  100000            55              3             50   \n",
       "4           0  100000            55              3             50   \n",
       "...       ...     ...           ...            ...            ...   \n",
       "2555        0  100000            55              3             50   \n",
       "2556        0  100000            55              3             50   \n",
       "2557        0  100000            55              3             50   \n",
       "2558        0  100000            55              3             50   \n",
       "2559        0  100000            55              3             50   \n",
       "\n",
       "     kernel_initializer  kernel_regularizer_l1  kernel_regularizer_l2  \\\n",
       "0               uniform                 0.1000                 0.1000   \n",
       "1               uniform                 0.1000                 0.0100   \n",
       "2               uniform                 0.1000                 0.0010   \n",
       "3               uniform                 0.1000                 0.0001   \n",
       "4               uniform                 0.0100                 0.1000   \n",
       "...                 ...                    ...                    ...   \n",
       "2555            uniform                 0.0010                 0.0001   \n",
       "2556            uniform                 0.0001                 0.1000   \n",
       "2557            uniform                 0.0001                 0.0100   \n",
       "2558            uniform                 0.0001                 0.0010   \n",
       "2559            uniform                 0.0001                 0.0001   \n",
       "\n",
       "     last_activation     lr  \n",
       "0            sigmoid  0.001  \n",
       "1            sigmoid  0.001  \n",
       "2            sigmoid  0.001  \n",
       "3            sigmoid  0.001  \n",
       "4            sigmoid  0.001  \n",
       "...              ...    ...  \n",
       "2555         sigmoid  0.001  \n",
       "2556         sigmoid  0.001  \n",
       "2557         sigmoid  0.001  \n",
       "2558         sigmoid  0.001  \n",
       "2559         sigmoid  0.001  \n",
       "\n",
       "[2560 rows x 20 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79498780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b670816f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>145</td>\n",
       "      <td>0.443156</td>\n",
       "      <td>[0.77607787 0.63870966 0.737475  ]</td>\n",
       "      <td>0.628446</td>\n",
       "      <td>[0.70212764 0.26865673 0.5511811 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>129</td>\n",
       "      <td>0.482130</td>\n",
       "      <td>[0.77521616 0.60307693 0.78585464]</td>\n",
       "      <td>0.631428</td>\n",
       "      <td>[0.75132275 0.3548387  0.610687  ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>174</td>\n",
       "      <td>0.428074</td>\n",
       "      <td>[0.80547947 0.6513158  0.78542507]</td>\n",
       "      <td>0.634222</td>\n",
       "      <td>[0.7027027  0.36363637 0.5954198 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>150</td>\n",
       "      <td>0.459109</td>\n",
       "      <td>[0.79286695 0.6486486  0.7594434 ]</td>\n",
       "      <td>0.634712</td>\n",
       "      <td>[0.71219504 0.31578952 0.5333333 ]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>125</td>\n",
       "      <td>0.457421</td>\n",
       "      <td>[0.76676387 0.6292835  0.73704416]</td>\n",
       "      <td>0.640150</td>\n",
       "      <td>[0.70157063 0.32258064 0.5736434 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>75</td>\n",
       "      <td>1.066042</td>\n",
       "      <td>[0.7249647  0.55647385 0.67543864]</td>\n",
       "      <td>1.297326</td>\n",
       "      <td>[0.4166667  0.21487604 0.41025642]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>75</td>\n",
       "      <td>1.056130</td>\n",
       "      <td>[0.7427745 0.5756677 0.7735471]</td>\n",
       "      <td>1.303543</td>\n",
       "      <td>[0.62195116 0.25742573 0.4615385 ]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>158</td>\n",
       "      <td>0.826383</td>\n",
       "      <td>[0.8622589  0.74725276 0.8582231 ]</td>\n",
       "      <td>1.304378</td>\n",
       "      <td>[0.5189873  0.21428572 0.3857143 ]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>237</td>\n",
       "      <td>0.705741</td>\n",
       "      <td>[0.9352331  0.8644067  0.93461543]</td>\n",
       "      <td>1.308194</td>\n",
       "      <td>[0.6421053  0.2535211  0.36363637]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>147</td>\n",
       "      <td>0.847568</td>\n",
       "      <td>[0.89627665 0.7703703  0.88932806]</td>\n",
       "      <td>1.311022</td>\n",
       "      <td>[0.6631579  0.26666665 0.35897434]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2560 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "1007           145  0.443156  [0.77607787 0.63870966 0.737475  ]  0.628446   \n",
       "974            129  0.482130  [0.77521616 0.60307693 0.78585464]  0.631428   \n",
       "1407           174  0.428074  [0.80547947 0.6513158  0.78542507]  0.634222   \n",
       "1870           150  0.459109  [0.79286695 0.6486486  0.7594434 ]  0.634712   \n",
       "895            125  0.457421  [0.76676387 0.6292835  0.73704416]  0.640150   \n",
       "...            ...       ...                                 ...       ...   \n",
       "1610            75  1.066042  [0.7249647  0.55647385 0.67543864]  1.297326   \n",
       "2122            75  1.056130     [0.7427745 0.5756677 0.7735471]  1.303543   \n",
       "1657           158  0.826383  [0.8622589  0.74725276 0.8582231 ]  1.304378   \n",
       "1624           237  0.705741  [0.9352331  0.8644067  0.93461543]  1.308194   \n",
       "2121           147  0.847568  [0.89627665 0.7703703  0.88932806]  1.311022   \n",
       "\n",
       "                         val_fbeta_score activation_layer  \\\n",
       "1007  [0.70212764 0.26865673 0.5511811 ]             tanh   \n",
       "974   [0.75132275 0.3548387  0.610687  ]             tanh   \n",
       "1407  [0.7027027  0.36363637 0.5954198 ]             selu   \n",
       "1870  [0.71219504 0.31578952 0.5333333 ]              elu   \n",
       "895   [0.70157063 0.32258064 0.5736434 ]             tanh   \n",
       "...                                  ...              ...   \n",
       "1610  [0.4166667  0.21487604 0.41025642]              elu   \n",
       "2122  [0.62195116 0.25742573 0.4615385 ]             relu   \n",
       "1657  [0.5189873  0.21428572 0.3857143 ]              elu   \n",
       "1624  [0.6421053  0.2535211  0.36363637]              elu   \n",
       "2121  [0.6631579  0.26666665 0.35897434]             relu   \n",
       "\n",
       "      activity_regularizer  batc_normalization  batch_size  bias_regularizer  \\\n",
       "1007                0.0001                True          64            0.0010   \n",
       "974                 0.0001                True          64            0.1000   \n",
       "1407                0.0010                True          64            0.0001   \n",
       "1870                0.0010                True          64            0.1000   \n",
       "895                 0.0010                True          64            0.0001   \n",
       "...                    ...                 ...         ...               ...   \n",
       "1610                0.1000                True          64            0.1000   \n",
       "2122                0.1000                True          64            0.1000   \n",
       "1657                0.1000                True          64            0.0001   \n",
       "1624                0.1000                True          64            0.0100   \n",
       "2121                0.1000                True          64            0.1000   \n",
       "\n",
       "      dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
       "1007        0  100000            55              3             50   \n",
       "974         0  100000            55              3             50   \n",
       "1407        0  100000            55              3             50   \n",
       "1870        0  100000            55              3             50   \n",
       "895         0  100000            55              3             50   \n",
       "...       ...     ...           ...            ...            ...   \n",
       "1610        0  100000            55              3             50   \n",
       "2122        0  100000            55              3             50   \n",
       "1657        0  100000            55              3             50   \n",
       "1624        0  100000            55              3             50   \n",
       "2121        0  100000            55              3             50   \n",
       "\n",
       "     kernel_initializer  kernel_regularizer_l1  kernel_regularizer_l2  \\\n",
       "1007            uniform                 0.0001                 0.0001   \n",
       "974             uniform                 0.0001                 0.0010   \n",
       "1407            uniform                 0.0001                 0.0001   \n",
       "1870            uniform                 0.0001                 0.0010   \n",
       "895             uniform                 0.0001                 0.0001   \n",
       "...                 ...                    ...                    ...   \n",
       "1610            uniform                 0.0010                 0.0010   \n",
       "2122            uniform                 0.0010                 0.0010   \n",
       "1657            uniform                 0.0010                 0.0100   \n",
       "1624            uniform                 0.0010                 0.1000   \n",
       "2121            uniform                 0.0010                 0.0100   \n",
       "\n",
       "     last_activation     lr  \n",
       "1007         sigmoid  0.001  \n",
       "974          sigmoid  0.001  \n",
       "1407         sigmoid  0.001  \n",
       "1870         sigmoid  0.001  \n",
       "895          sigmoid  0.001  \n",
       "...              ...    ...  \n",
       "1610         sigmoid  0.001  \n",
       "2122         sigmoid  0.001  \n",
       "1657         sigmoid  0.001  \n",
       "1624         sigmoid  0.001  \n",
       "2121         sigmoid  0.001  \n",
       "\n",
       "[2560 rows x 20 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c8805eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of kernel_regularizer_l1')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_regularizer_l1'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e6404c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of kernel_regularizer_l2')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_regularizer_l2'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "759d4d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of bias_regularizer')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'bias_regularizer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d7688d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of activity_regularizer')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activity_regularizer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15139849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batc_normalization')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batc_normalization'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "881e4939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of activation_layer')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activation_layer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92a3ac",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85ffa125",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3],   #Done\n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001],#Done\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.1,0.0001,0.00001],\n",
    "    'kernel_regularizer_l2':[0.1,0.0001,0.00001],\n",
    "    'bias_regularizer':[0.0001,0.00001],\n",
    "    'activity_regularizer':[0.001,0.0001,0.00001],\n",
    "\n",
    "    #'dropout': [0,0.1,0.2,0.3,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['uniform'],#Done\n",
    "\n",
    "    'activation_layer':['tanh','selu'],\n",
    " \n",
    "    'batc_normalization':[False,True],\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e41a3ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/216 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F232328EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA5ECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "Epoch 00126: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▍                                                                                 | 1/216 [00:06<24:22,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240EB24C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23DF74AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "Epoch 00126: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                                 | 2/216 [00:13<24:49,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B2F0C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FE995E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 108.\n",
      "Epoch 00158: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▏                                                                                | 3/216 [00:22<27:06,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B2F0708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23DF73678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 365.\n",
      "Epoch 00415: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▌                                                                                | 4/216 [00:40<42:17, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DF743A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23B2F0828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▉                                                                                | 5/216 [00:45<32:07,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2339CD708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F232328168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▎                                                                               | 6/216 [00:49<26:02,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DF73F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23B2F0828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 364.\n",
      "Epoch 00414: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▋                                                                               | 7/216 [01:08<38:56, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCD58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23B2F0048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███                                                                               | 8/216 [01:12<30:58,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCD5E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F232328798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▍                                                                              | 9/216 [01:16<25:22,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DF74558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F232328708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 112.\n",
      "Epoch 00162: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▊                                                                             | 10/216 [01:24<26:18,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B2F00D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA5E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████▏                                                                            | 11/216 [01:31<25:36,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FB0AE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23B2F09D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 77.\n",
      "Epoch 00127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▌                                                                            | 12/216 [01:38<25:00,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F585AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23B2F05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 365.\n",
      "Epoch 00415: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▉                                                                            | 13/216 [01:56<35:49, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE99288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F220BC2558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████▎                                                                           | 14/216 [02:00<29:10,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23AFED168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FE991F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▋                                                                           | 15/216 [02:04<24:25,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F585B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23B2F0558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 365.\n",
      "Epoch 00415: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|██████                                                                           | 16/216 [02:24<36:40, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DF749D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239BDE168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▍                                                                          | 17/216 [02:28<29:48,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE865E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F220BC2EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▊                                                                          | 18/216 [02:32<24:46,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA5EE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADDB948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 74.\n",
      "Epoch 00124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▏                                                                         | 19/216 [02:41<26:08,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA490D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FBE2678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▌                                                                         | 20/216 [02:50<26:57,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F221197A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B132318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 116.\n",
      "Epoch 00166: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▉                                                                         | 21/216 [03:01<29:36,  9.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F220BC2EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F975798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 152.\n",
      "Epoch 00202: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                        | 22/216 [03:14<33:01, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240EB2048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F9E3B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 74.\n",
      "Epoch 00124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▋                                                                        | 23/216 [03:23<31:45,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2397C7EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F74B708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 00107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████                                                                        | 24/216 [03:31<29:49,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE99DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADDB1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 184.\n",
      "Epoch 00234: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▍                                                                       | 25/216 [03:46<34:21, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCB9DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23DF745E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 89.\n",
      "Epoch 00139: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▊                                                                       | 26/216 [03:55<32:45, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240EB2048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F975AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 00106: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▏                                                                      | 27/216 [04:03<30:22,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F232328AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240F48C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▌                                                                      | 28/216 [04:12<29:30,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2397C7EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240F48A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 74.\n",
      "Epoch 00124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▉                                                                      | 29/216 [04:21<28:49,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239945CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E220F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▎                                                                     | 30/216 [04:29<27:33,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2352B28B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E220EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 208.\n",
      "Epoch 00258: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▋                                                                     | 31/216 [04:42<31:28, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CC9C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FAB99D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 00107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████                                                                     | 32/216 [04:49<28:43,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239644C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FAB9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "Epoch 00126: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▍                                                                    | 33/216 [04:58<28:09,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E275798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FAB9D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 191.\n",
      "Epoch 00241: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████▊                                                                    | 34/216 [05:13<32:52, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2397C7168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FAB9678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 95.\n",
      "Epoch 00145: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████▏                                                                   | 35/216 [05:23<31:53, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC01DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239BDE5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 91.\n",
      "Epoch 00141: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▌                                                                   | 36/216 [05:33<31:09, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DF741F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240F48F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 78.\n",
      "Epoch 00128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▉                                                                   | 37/216 [05:40<28:14,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240F48288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F222E76708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 78.\n",
      "Epoch 00128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▎                                                                  | 38/216 [05:47<26:00,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DE8BEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F9E3EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▋                                                                  | 39/216 [05:54<24:28,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F585D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F222E76318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 364.\n",
      "Epoch 00414: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████                                                                  | 40/216 [06:13<33:24, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22ADDB318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F232328708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████▍                                                                 | 41/216 [06:17<26:37,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F222E76E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA37C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████▊                                                                 | 42/216 [06:21<21:58,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F222E76F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240F48CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 366.\n",
      "Epoch 00416: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▏                                                                | 43/216 [06:39<30:38, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284E1948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F585558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                | 44/216 [06:43<24:46,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F232328B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F222E76708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████▉                                                                | 45/216 [06:47<20:43,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9E3EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239BDEB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 82.\n",
      "Epoch 00132: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▎                                                               | 46/216 [06:54<20:24,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B4D91F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F222E76558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "Epoch 00131: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████▋                                                               | 47/216 [07:01<20:21,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCD51F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F9E35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "Epoch 00131: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████████████                                                               | 48/216 [07:08<20:20,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F222E76048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADDBD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 335.\n",
      "Epoch 00385: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████▍                                                              | 49/216 [07:25<28:25, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22ADDBE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F9E3438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████▊                                                              | 50/216 [07:29<23:07,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F975828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F222E76318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▏                                                             | 51/216 [07:34<19:25,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239BDE4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240F48438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 333.\n",
      "Epoch 00383: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▌                                                             | 52/216 [07:51<27:45, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC014C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23DF74558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████████████▉                                                             | 53/216 [07:55<22:35,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240F48438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23DF74168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▎                                                            | 54/216 [07:59<18:55,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9591F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CD5E4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▋                                                            | 55/216 [08:07<19:56,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226CE28B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC49C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████                                                            | 56/216 [08:16<20:49,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC49288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239644DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▍                                                           | 57/216 [08:24<21:13,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC01D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA093A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 215.\n",
      "Epoch 00265: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|█████████████████████▊                                                           | 58/216 [08:40<26:48, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CC9C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC49048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 59.\n",
      "Epoch 00109: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████▏                                                          | 59/216 [08:48<24:53,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCD5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC49168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 104.\n",
      "Epoch 00154: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████▌                                                          | 60/216 [08:58<25:33,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240F48288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CD5EAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 166.\n",
      "Epoch 00216: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████▊                                                          | 61/216 [09:12<28:11, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CA82798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226CE2A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 00121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|███████████████████████▎                                                         | 62/216 [09:20<25:59, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2354F1D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241055828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 73.\n",
      "Epoch 00123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|███████████████████████▋                                                         | 63/216 [09:29<24:37,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2284E1708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226CE2438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████                                                         | 64/216 [09:37<23:48,  9.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F232328168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F220BC28B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 78.\n",
      "Epoch 00128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▍                                                        | 65/216 [09:46<23:17,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B400DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC26EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 73.\n",
      "Epoch 00123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|████████████████████████▊                                                        | 66/216 [09:55<22:46,  9.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9E3558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FE86798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 183.\n",
      "Epoch 00233: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████████████████▏                                                       | 67/216 [10:09<26:22, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B228CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FE869D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 77.\n",
      "Epoch 00127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████████████████▌                                                       | 68/216 [10:18<24:59, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCD5B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F74BA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 117.\n",
      "Epoch 00167: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████████████████▊                                                       | 69/216 [10:29<25:29, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B59A1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CD5EE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 187.\n",
      "Epoch 00237: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▎                                                      | 70/216 [10:43<27:53, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23979C0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CD5E5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 117.\n",
      "Epoch 00167: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████▋                                                      | 71/216 [10:54<27:19, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F585EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E494168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████                                                      | 72/216 [11:03<25:16, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DF748B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC490D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████▍                                                     | 73/216 [11:10<22:59,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E12C8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CD5EA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████▊                                                     | 74/216 [11:17<20:59,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2339CD708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E12C168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 74.\n",
      "Epoch 00124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████▏                                                    | 75/216 [11:24<19:26,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE865E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADDB4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 334.\n",
      "Epoch 00384: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████▌                                                    | 76/216 [11:42<25:36, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA11E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC499D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|████████████████████████████▊                                                    | 77/216 [11:46<20:37,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9E3D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC49948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████▎                                                   | 78/216 [11:50<17:03,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCB9798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC49558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 334.\n",
      "Epoch 00384: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|█████████████████████████████▋                                                   | 79/216 [12:07<23:45, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F585678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CCB9CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████                                                   | 80/216 [12:11<19:09,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E34E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F222E761F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████████████████▍                                                  | 81/216 [12:15<16:00,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240EB23A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CCB99D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 79.\n",
      "Epoch 00129: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████████████████▊                                                  | 82/216 [12:23<16:13,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E494948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC49678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 82.\n",
      "Epoch 00132: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                 | 83/216 [12:31<16:43,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2323289D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA6A5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 82.\n",
      "Epoch 00132: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███████████████████████████████▌                                                 | 84/216 [12:39<17:09,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCD5948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CD5EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 364.\n",
      "Epoch 00414: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███████████████████████████████▉                                                 | 85/216 [13:00<25:36, 11.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22ADDB558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA47678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████▎                                                | 86/216 [13:05<21:03,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F220BC2828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E494C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████▋                                                | 87/216 [13:10<17:53,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F236842798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239AE9708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 362.\n",
      "Epoch 00412: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████████████████████                                                | 88/216 [13:30<25:28, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9E3948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CCB9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████████████████████▍                                               | 89/216 [13:35<20:48,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239AE9B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADDBEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████████████████████▊                                               | 90/216 [13:40<17:25,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239AE9F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CCA28B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▏                                              | 91/216 [13:50<18:25,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA11708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F881798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 72.\n",
      "Epoch 00122: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|██████████████████████████████████▌                                              | 92/216 [14:00<18:46,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCB9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239AE9708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|██████████████████████████████████▉                                              | 93/216 [14:10<19:05,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CD5E948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F229BC1318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 150.\n",
      "Epoch 00200: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████▎                                             | 94/216 [14:24<21:43, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22ACEA948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E494EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 79.\n",
      "Epoch 00129: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████▋                                             | 95/216 [14:33<20:44, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE99288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E08EA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "Epoch 00118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████                                             | 96/216 [14:43<20:11, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239D00CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226AC59D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 183.\n",
      "Epoch 00233: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████████████████████▍                                            | 97/216 [14:58<23:06, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CD5E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226AC5318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 74.\n",
      "Epoch 00124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████████████████████▊                                            | 98/216 [15:08<21:45, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9E3948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240FE6558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 00110: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████▏                                           | 99/216 [15:17<20:29, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22AD51288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CD5ECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "Epoch 00118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████                                           | 100/216 [15:27<19:52, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CD5E318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239AE9948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 00116: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|█████████████████████████████████████▍                                          | 101/216 [15:35<18:53,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA6A288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CCA2318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|█████████████████████████████████████▊                                          | 102/216 [15:44<17:46,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CC9C558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCE438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 199.\n",
      "Epoch 00249: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|██████████████████████████████████████▏                                         | 103/216 [15:59<21:17, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239AE94C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23DFFF5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 00116: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|██████████████████████████████████████▌                                         | 104/216 [16:09<20:00, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2354F1D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FDD5C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 105.\n",
      "Epoch 00155: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|██████████████████████████████████████▉                                         | 105/216 [16:20<20:12, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA091F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CCA2708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 145.\n",
      "Epoch 00195: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|███████████████████████████████████████▎                                        | 106/216 [16:34<21:34, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E08E798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC49558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 91.\n",
      "Epoch 00141: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████▋                                        | 107/216 [16:45<20:51, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA11E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA090D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 90.\n",
      "Epoch 00140: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████                                        | 108/216 [16:55<20:13, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CC9CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCEC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "Epoch 00131: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████▎                                       | 109/216 [17:03<18:16, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CC9C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F229BC1168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 74.\n",
      "Epoch 00124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|████████████████████████████████████████▋                                       | 110/216 [17:11<16:37,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FDD5A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCE438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "Epoch 00125: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████████████████████████████████████████                                       | 111/216 [17:19<15:42,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B4D91F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FDD50D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 335.\n",
      "Epoch 00385: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████████████████████████████████████████▍                                      | 112/216 [17:37<20:29, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240FE65E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E08E0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████████████████████████████████████████▊                                      | 113/216 [17:42<16:25,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CB76948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCE5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|██████████████████████████████████████████▏                                     | 114/216 [17:46<13:45,  8.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CB76168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FDD5168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 349.\n",
      "Epoch 00399: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|██████████████████████████████████████████▌                                     | 115/216 [18:06<19:40, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA5E828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FDD5948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████████████████████████▉                                     | 116/216 [18:11<15:53,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F74B168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CB76288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|███████████████████████████████████████████▎                                    | 117/216 [18:15<13:14,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FDD5E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F229BC18B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "Epoch 00125: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████████████████████████▋                                    | 118/216 [18:23<12:55,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCB9CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CB76168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 77.\n",
      "Epoch 00127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|████████████████████████████████████████████                                    | 119/216 [18:31<12:44,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F229BC1708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCED38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 77.\n",
      "Epoch 00127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████████████████████▍                                   | 120/216 [18:39<12:41,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CA214C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCEAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 364.\n",
      "Epoch 00414: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████████████████████▊                                   | 121/216 [18:59<18:18, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DFD7F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F229BC1708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████████████████████████████████████████████▏                                  | 122/216 [19:03<14:42,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE99A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCE558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████████████████████████████████████████████▌                                  | 123/216 [19:08<12:20,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C58AEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCEEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 366.\n",
      "Epoch 00416: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████████████████████████████████████████████▉                                  | 124/216 [19:27<17:29, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E08EB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CD5EEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|██████████████████████████████████████████████▎                                 | 125/216 [19:32<14:05,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E2751F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240FE6168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|██████████████████████████████████████████████▋                                 | 126/216 [19:36<11:47,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CB76558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E09CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "Epoch 00118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|███████████████████████████████████████████████                                 | 127/216 [19:46<12:32,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA09288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F91E3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|███████████████████████████████████████████████▍                                | 128/216 [19:55<12:45,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA375E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB7CAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 00116: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████▊                                | 129/216 [20:05<13:01,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FB5CD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240FB29D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 148.\n",
      "Epoch 00198: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████▏                               | 130/216 [20:19<15:02, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240FB21F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB7C798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 89.\n",
      "Epoch 00139: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|████████████████████████████████████████████████▌                               | 131/216 [20:30<15:00, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9E3B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240FE6678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 90.\n",
      "Epoch 00140: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|████████████████████████████████████████████████▉                               | 132/216 [20:40<14:35, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23AFED168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E08B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 170.\n",
      "Epoch 00220: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|█████████████████████████████████████████████████▎                              | 133/216 [20:55<16:21, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA6ACA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2412D8798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Epoch 00094: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|█████████████████████████████████████████████████▋                              | 134/216 [21:04<14:52, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FB7CAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E08B9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 62.\n",
      "Epoch 00112: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████████████████████████████                              | 135/216 [21:13<13:55, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239AE9048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E08B948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 72.\n",
      "Epoch 00122: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████████████████████████████████▎                             | 136/216 [21:22<13:24, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC01288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F8BCDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████████████████████████████████▋                             | 137/216 [21:31<13:01,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E2FAD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CB554C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 00121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|███████████████████████████████████████████████████                             | 138/216 [21:41<12:52,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2396B6DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F9B9558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 199.\n",
      "Epoch 00249: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|███████████████████████████████████████████████████▍                            | 139/216 [21:57<15:00, 11.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9B9D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC261F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 55.\n",
      "Epoch 00105: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████████████████▊                            | 140/216 [22:06<13:41, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239C79B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241242EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 103.\n",
      "Epoch 00153: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|████████████████████████████████████████████████████▏                           | 141/216 [22:18<13:48, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240FE6CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E0EE3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 176.\n",
      "Epoch 00226: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████████████████████████████▌                           | 142/216 [22:32<15:02, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9B9678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA6A1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 92.\n",
      "Epoch 00142: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████████████████████████████▉                           | 143/216 [22:43<14:15, 11.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240EB25E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E0EE708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "Epoch 00147: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████████████████████████████▎                          | 144/216 [22:55<13:57, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239AE9948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240FE64C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████████████████████████████▋                          | 145/216 [23:03<12:34, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23DF73708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241242828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 112.\n",
      "Epoch 00162: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████                          | 146/216 [23:12<11:48, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE99DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCE438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████▍                         | 147/216 [23:20<10:52,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E2FA5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E08E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 365.\n",
      "Epoch 00415: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████████████████████████████████████████████████████▊                         | 148/216 [23:38<13:35, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E2FA708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2412421F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|███████████████████████████████████████████████████████▏                        | 149/216 [23:42<10:41,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F236CCEAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F91E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|███████████████████████████████████████████████████████▌                        | 150/216 [23:45<08:40,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2352494C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FDD59D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 365.\n",
      "Epoch 00415: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████████████████████████████████████████████████████▉                        | 151/216 [24:03<11:50, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239AE9288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E2FA438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████████▎                       | 152/216 [24:07<09:26,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F240EB24C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FBE28B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████▋                       | 153/216 [24:11<07:45,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F222E76E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E2FA4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "Epoch 00126: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████████                       | 154/216 [24:18<07:28,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CD5E1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E2FA8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 78.\n",
      "Epoch 00128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|█████████████████████████████████████████████████████████▍                      | 155/216 [24:25<07:15,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B2F0CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240FE6CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "Epoch 00131: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|█████████████████████████████████████████████████████████▊                      | 156/216 [24:33<07:10,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CB763A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2412D8A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 365.\n",
      "Epoch 00415: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|██████████████████████████████████████████████████████████▏                     | 157/216 [24:51<10:28, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F74B8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239AE9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|██████████████████████████████████████████████████████████▌                     | 158/216 [24:56<08:26,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F91E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E2FA3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|██████████████████████████████████████████████████████████▉                     | 159/216 [25:00<06:57,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F975048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F236CCEDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 363.\n",
      "Epoch 00413: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████████████████████████████████████████████████████████▎                    | 160/216 [25:18<09:58, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239C79B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F91E948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████▋                    | 161/216 [25:22<07:58,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCB95E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA47678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|████████████████████████████████████████████████████████████                    | 162/216 [25:26<06:35,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA6A288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E2E2708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 00121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|████████████████████████████████████████████████████████████▎                   | 163/216 [25:35<06:47,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F9B9A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FE14E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████████████████████████████████▋                   | 164/216 [25:44<06:55,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22ACEA948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241277168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "Epoch 00118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|█████████████████████████████████████████████████████████████                   | 165/216 [25:52<06:54,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC260D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB7B558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 174.\n",
      "Epoch 00224: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████████████████████████████████▍                  | 166/216 [26:06<08:10,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E2FA318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FE144C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 88.\n",
      "Epoch 00138: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████████████████████████████████▊                  | 167/216 [26:15<07:51,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E165048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC269D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 00121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████▏                 | 168/216 [26:23<07:25,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F220CF2318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239AE94C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 187.\n",
      "Epoch 00237: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████▌                 | 169/216 [26:38<08:27, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22AF54E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB5D3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████████████████████████████████████▉                 | 170/216 [26:46<07:44, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F241277CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F24115EDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 00121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████████████████████▎                | 171/216 [26:54<07:03,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B4589D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2412775E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "Epoch 00118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████▋                | 172/216 [27:02<06:33,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239945AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2412778B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████                | 173/216 [27:10<06:08,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA093A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23B4D9048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████████████████████████████████████▍               | 174/216 [27:17<05:46,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC493A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239AE9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 149.\n",
      "Epoch 00199: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████████████████████████████████████▊               | 175/216 [27:28<06:08,  9.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F229BC1DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239AE9318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 00116: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████▏              | 176/216 [27:35<05:39,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F74BDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB5DAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 00110: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|█████████████████████████████████████████████████████████████████▌              | 177/216 [27:42<05:14,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CB55168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F229BC1678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 172.\n",
      "Epoch 00222: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|█████████████████████████████████████████████████████████████████▉              | 178/216 [27:54<05:46,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B4D9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2411BEB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 83.\n",
      "Epoch 00133: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|██████████████████████████████████████████████████████████████████▎             | 179/216 [28:02<05:25,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FBE28B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CB553A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 82.\n",
      "Epoch 00132: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|██████████████████████████████████████████████████████████████████▋             | 180/216 [28:10<05:07,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2412D8F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FA5EAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████████████████████████████████             | 181/216 [28:16<04:36,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CD5E1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241277C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████████████████████████████████▍            | 182/216 [28:23<04:14,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F220BC2678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E2E2F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "Epoch 00126: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|███████████████████████████████████████████████████████████████████▊            | 183/216 [28:29<03:56,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FB7CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F9B9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 364.\n",
      "Epoch 00414: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████████████████████████████████████████████████████████████████▏           | 184/216 [28:45<05:16,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FB5DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2412774C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████▌           | 185/216 [28:49<04:08,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B4589D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB5D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████▉           | 186/216 [28:53<03:21,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2411BE708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB5DC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 364.\n",
      "Epoch 00414: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|█████████████████████████████████████████████████████████████████████▎          | 187/216 [29:09<04:36,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F74B8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB7CDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|█████████████████████████████████████████████████████████████████████▋          | 188/216 [29:13<03:38,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23AFED798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2411BE9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████          | 189/216 [29:16<02:57,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FB5D5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2411BED38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "Epoch 00131: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████▎         | 190/216 [29:23<02:51,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2412773A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2411BE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 78.\n",
      "Epoch 00128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████▋         | 191/216 [29:30<02:45,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2411BE0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E08BEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "Epoch 00131: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████         | 192/216 [29:36<02:39,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F229BC1B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2411BE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 364.\n",
      "Epoch 00414: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████▍        | 193/216 [29:54<03:47,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CCB95E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2411BEE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████████▊        | 194/216 [29:58<02:59,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2227A54C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2412779D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████▏       | 195/216 [30:02<02:25,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FDD5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2411BEC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 349.\n",
      "Epoch 00399: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|████████████████████████████████████████████████████████████████████████▌       | 196/216 [30:19<03:21, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239BDEEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2411BE9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|████████████████████████████████████████████████████████████████████████▉       | 197/216 [30:23<02:36,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2412D8288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241461708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████████████████████████████████████████████████████████████████████▎      | 198/216 [30:27<02:06,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE99A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DE97318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 72.\n",
      "Epoch 00122: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████████████████████████████████████████████████████████████████████▋      | 199/216 [30:36<02:08,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CA82798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2413329D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|██████████████████████████████████████████████████████████████████████████      | 200/216 [30:45<02:06,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F91ECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DE97A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|██████████████████████████████████████████████████████████████████████████▍     | 201/216 [30:54<02:03,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F222E76168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC81828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 152.\n",
      "Epoch 00202: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|██████████████████████████████████████████████████████████████████████████▊     | 202/216 [31:06<02:11,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC81DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DE97CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 55.\n",
      "Epoch 00105: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|███████████████████████████████████████████████████████████████████████████▏    | 203/216 [31:14<01:55,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F585558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2412C5828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "Epoch 00125: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|███████████████████████████████████████████████████████████████████████████▌    | 204/216 [31:23<01:47,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CD5E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FB7C5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 183.\n",
      "Epoch 00233: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|███████████████████████████████████████████████████████████████████████████▉    | 205/216 [31:37<01:55, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F238344F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B12AF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|████████████████████████████████████████████████████████████████████████████▎   | 206/216 [31:45<01:38,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F238355438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219BA9708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 00102: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|████████████████████████████████████████████████████████████████████████████▋   | 207/216 [31:53<01:23,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F222E76DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F240F48CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████████████████████████████████████████████████████████████████████████   | 208/216 [32:02<01:11,  9.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B12A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241461B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████████████████████████████████████████████████████████████████████████▍  | 209/216 [32:10<01:02,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.1, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F241332EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241461828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████████████████████████████████████████████████████████████████████████▊  | 210/216 [32:19<00:53,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F8815E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F906438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 179.\n",
      "Epoch 00229: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|██████████████████████████████████████████████████████████████████████████████▏ | 211/216 [32:33<00:51, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FE99A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219BA9C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 99.\n",
      "Epoch 00149: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|██████████████████████████████████████████████████████████████████████████████▌ | 212/216 [32:42<00:40, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 0.0001, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FDD59D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C6B35E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 102.\n",
      "Epoch 00152: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|██████████████████████████████████████████████████████████████████████████████▉ | 213/216 [32:53<00:30, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.1, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226E4F0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C6B38B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 199.\n",
      "Epoch 00249: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|███████████████████████████████████████████████████████████████████████████████▎| 214/216 [33:07<00:22, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 0.0001, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2412ACB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC813A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 00106: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 215/216 [33:15<00:10, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 1e-05, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 1e-05, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239D00CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C6B3438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 87.\n",
      "Epoch 00137: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 216/216 [33:24<00:00,  9.28s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "833e627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/051022205556.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67dcf9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>0.698094</td>\n",
       "      <td>[0.56353587 0.07725322 0.18974358]</td>\n",
       "      <td>0.697960</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>0.697953</td>\n",
       "      <td>[0.         0.24165708 0.        ]</td>\n",
       "      <td>0.697823</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158</td>\n",
       "      <td>0.694507</td>\n",
       "      <td>[0.6838932 0.        0.       ]</td>\n",
       "      <td>0.694602</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>415</td>\n",
       "      <td>0.682143</td>\n",
       "      <td>[0.6838932 0.        0.       ]</td>\n",
       "      <td>0.682070</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>0.724922</td>\n",
       "      <td>[0.        0.        0.5107212]</td>\n",
       "      <td>0.724936</td>\n",
       "      <td>[0.        0.        0.5078125]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>149</td>\n",
       "      <td>0.461744</td>\n",
       "      <td>[0.78661084 0.5956113  0.76829267]</td>\n",
       "      <td>0.650207</td>\n",
       "      <td>[0.7195767 0.3030303 0.5354331]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>152</td>\n",
       "      <td>0.451926</td>\n",
       "      <td>[0.8130969 0.6449511 0.7663934]</td>\n",
       "      <td>0.640955</td>\n",
       "      <td>[0.680203   0.3030303  0.53781515]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>249</td>\n",
       "      <td>0.480027</td>\n",
       "      <td>[0.935611  0.8559671 0.9198473]</td>\n",
       "      <td>0.884010</td>\n",
       "      <td>[0.7291666  0.40625003 0.5555556 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>106</td>\n",
       "      <td>0.459597</td>\n",
       "      <td>[0.73020524 0.55976677 0.7554672 ]</td>\n",
       "      <td>0.648487</td>\n",
       "      <td>[0.554054   0.32989693 0.5839417 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>137</td>\n",
       "      <td>0.434915</td>\n",
       "      <td>[0.78720444 0.6092308  0.75206614]</td>\n",
       "      <td>0.621702</td>\n",
       "      <td>[0.6806283  0.30555555 0.5210084 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "0             126  0.698094  [0.56353587 0.07725322 0.18974358]  0.697960   \n",
       "1             126  0.697953  [0.         0.24165708 0.        ]  0.697823   \n",
       "2             158  0.694507     [0.6838932 0.        0.       ]  0.694602   \n",
       "3             415  0.682143     [0.6838932 0.        0.       ]  0.682070   \n",
       "4              51  0.724922     [0.        0.        0.5107212]  0.724936   \n",
       "..            ...       ...                                 ...       ...   \n",
       "211           149  0.461744  [0.78661084 0.5956113  0.76829267]  0.650207   \n",
       "212           152  0.451926     [0.8130969 0.6449511 0.7663934]  0.640955   \n",
       "213           249  0.480027     [0.935611  0.8559671 0.9198473]  0.884010   \n",
       "214           106  0.459597  [0.73020524 0.55976677 0.7554672 ]  0.648487   \n",
       "215           137  0.434915  [0.78720444 0.6092308  0.75206614]  0.621702   \n",
       "\n",
       "                        val_fbeta_score activation_layer  \\\n",
       "0    [0.         0.24770641 0.        ]             tanh   \n",
       "1    [0.         0.24770641 0.        ]             tanh   \n",
       "2       [0.6827586 0.        0.       ]             tanh   \n",
       "3       [0.6827586 0.        0.       ]             tanh   \n",
       "4       [0.        0.        0.5078125]             tanh   \n",
       "..                                  ...              ...   \n",
       "211     [0.7195767 0.3030303 0.5354331]             selu   \n",
       "212  [0.680203   0.3030303  0.53781515]             selu   \n",
       "213  [0.7291666  0.40625003 0.5555556 ]             selu   \n",
       "214  [0.554054   0.32989693 0.5839417 ]             selu   \n",
       "215  [0.6806283  0.30555555 0.5210084 ]             selu   \n",
       "\n",
       "     activity_regularizer  batc_normalization  batch_size  bias_regularizer  \\\n",
       "0                 0.00100               False          64           0.00010   \n",
       "1                 0.00100               False          64           0.00010   \n",
       "2                 0.00100               False          64           0.00010   \n",
       "3                 0.00100               False          64           0.00010   \n",
       "4                 0.00100               False          64           0.00010   \n",
       "..                    ...                 ...         ...               ...   \n",
       "211               0.00001                True          64           0.00001   \n",
       "212               0.00001                True          64           0.00001   \n",
       "213               0.00001                True          64           0.00001   \n",
       "214               0.00001                True          64           0.00001   \n",
       "215               0.00001                True          64           0.00001   \n",
       "\n",
       "     dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
       "0          0  100000            55              3             50   \n",
       "1          0  100000            55              3             50   \n",
       "2          0  100000            55              3             50   \n",
       "3          0  100000            55              3             50   \n",
       "4          0  100000            55              3             50   \n",
       "..       ...     ...           ...            ...            ...   \n",
       "211        0  100000            55              3             50   \n",
       "212        0  100000            55              3             50   \n",
       "213        0  100000            55              3             50   \n",
       "214        0  100000            55              3             50   \n",
       "215        0  100000            55              3             50   \n",
       "\n",
       "    kernel_initializer  kernel_regularizer_l1  kernel_regularizer_l2  \\\n",
       "0              uniform                0.10000                0.10000   \n",
       "1              uniform                0.10000                0.00010   \n",
       "2              uniform                0.10000                0.00001   \n",
       "3              uniform                0.00010                0.10000   \n",
       "4              uniform                0.00010                0.00010   \n",
       "..                 ...                    ...                    ...   \n",
       "211            uniform                0.00010                0.00010   \n",
       "212            uniform                0.00010                0.00001   \n",
       "213            uniform                0.00001                0.10000   \n",
       "214            uniform                0.00001                0.00010   \n",
       "215            uniform                0.00001                0.00001   \n",
       "\n",
       "    last_activation     lr  \n",
       "0           sigmoid  0.001  \n",
       "1           sigmoid  0.001  \n",
       "2           sigmoid  0.001  \n",
       "3           sigmoid  0.001  \n",
       "4           sigmoid  0.001  \n",
       "..              ...    ...  \n",
       "211         sigmoid  0.001  \n",
       "212         sigmoid  0.001  \n",
       "213         sigmoid  0.001  \n",
       "214         sigmoid  0.001  \n",
       "215         sigmoid  0.001  \n",
       "\n",
       "[216 rows x 20 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77e61463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6948938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>117</td>\n",
       "      <td>0.454319</td>\n",
       "      <td>[0.7977991  0.6298701  0.73427993]</td>\n",
       "      <td>0.605751</td>\n",
       "      <td>[0.7083334  0.38805968 0.5853659 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>121</td>\n",
       "      <td>0.467642</td>\n",
       "      <td>[0.7478006 0.5417868 0.7294589]</td>\n",
       "      <td>0.605768</td>\n",
       "      <td>[0.70114946 0.35294116 0.6       ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>123</td>\n",
       "      <td>0.442471</td>\n",
       "      <td>[0.7398844  0.58309036 0.73427993]</td>\n",
       "      <td>0.607350</td>\n",
       "      <td>[0.6979167  0.2972973  0.56896555]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>141</td>\n",
       "      <td>0.421877</td>\n",
       "      <td>[0.7903683  0.65573776 0.8085106 ]</td>\n",
       "      <td>0.611567</td>\n",
       "      <td>[0.7195767  0.33898303 0.5970149 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>140</td>\n",
       "      <td>0.437633</td>\n",
       "      <td>[0.79558015 0.6018809  0.75463915]</td>\n",
       "      <td>0.611757</td>\n",
       "      <td>[0.7089947 0.3030303 0.5511811]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>220</td>\n",
       "      <td>0.527404</td>\n",
       "      <td>[0.9141348  0.80308884 0.9140625 ]</td>\n",
       "      <td>0.988254</td>\n",
       "      <td>[0.6354166  0.30136985 0.4786325 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>226</td>\n",
       "      <td>0.520081</td>\n",
       "      <td>[0.915912   0.81781375 0.8937008 ]</td>\n",
       "      <td>0.996914</td>\n",
       "      <td>[0.64171124 0.3095238  0.45045045]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>199</td>\n",
       "      <td>0.576197</td>\n",
       "      <td>[0.89528793 0.7876448  0.86732674]</td>\n",
       "      <td>0.997160</td>\n",
       "      <td>[0.60818714 0.30588233 0.53968257]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>224</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>[0.92388445 0.82539684 0.92607003]</td>\n",
       "      <td>1.005207</td>\n",
       "      <td>[0.65968585 0.25714287 0.4958678 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>200</td>\n",
       "      <td>0.559865</td>\n",
       "      <td>[0.8932807 0.8015564 0.890625 ]</td>\n",
       "      <td>1.025954</td>\n",
       "      <td>[0.6455027  0.35616437 0.48333332]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "169           117  0.454319  [0.7977991  0.6298701  0.73427993]  0.605751   \n",
       "170           121  0.467642     [0.7478006 0.5417868 0.7294589]  0.605768   \n",
       "62            123  0.442471  [0.7398844  0.58309036 0.73427993]  0.607350   \n",
       "35            141  0.421877  [0.7903683  0.65573776 0.8085106 ]  0.611567   \n",
       "107           140  0.437633  [0.79558015 0.6018809  0.75463915]  0.611757   \n",
       "..            ...       ...                                 ...       ...   \n",
       "132           220  0.527404  [0.9141348  0.80308884 0.9140625 ]  0.988254   \n",
       "141           226  0.520081  [0.915912   0.81781375 0.8937008 ]  0.996914   \n",
       "174           199  0.576197  [0.89528793 0.7876448  0.86732674]  0.997160   \n",
       "165           224  0.517808  [0.92388445 0.82539684 0.92607003]  1.005207   \n",
       "93            200  0.559865     [0.8932807 0.8015564 0.890625 ]  1.025954   \n",
       "\n",
       "                        val_fbeta_score activation_layer  \\\n",
       "169  [0.7083334  0.38805968 0.5853659 ]             selu   \n",
       "170  [0.70114946 0.35294116 0.6       ]             selu   \n",
       "62   [0.6979167  0.2972973  0.56896555]             tanh   \n",
       "35   [0.7195767  0.33898303 0.5970149 ]             tanh   \n",
       "107     [0.7089947 0.3030303 0.5511811]             tanh   \n",
       "..                                  ...              ...   \n",
       "132  [0.6354166  0.30136985 0.4786325 ]             selu   \n",
       "141  [0.64171124 0.3095238  0.45045045]             selu   \n",
       "174  [0.60818714 0.30588233 0.53968257]             selu   \n",
       "165  [0.65968585 0.25714287 0.4958678 ]             selu   \n",
       "93   [0.6455027  0.35616437 0.48333332]             tanh   \n",
       "\n",
       "     activity_regularizer  batc_normalization  batch_size  bias_regularizer  \\\n",
       "169               0.00010                True          64           0.00010   \n",
       "170               0.00010                True          64           0.00010   \n",
       "62                0.00010                True          64           0.00010   \n",
       "35                0.00100                True          64           0.00001   \n",
       "107               0.00001                True          64           0.00001   \n",
       "..                    ...                 ...         ...               ...   \n",
       "132               0.00100                True          64           0.00010   \n",
       "141               0.00100                True          64           0.00001   \n",
       "174               0.00010                True          64           0.00001   \n",
       "165               0.00010                True          64           0.00010   \n",
       "93                0.00001                True          64           0.00010   \n",
       "\n",
       "     dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
       "169        0  100000            55              3             50   \n",
       "170        0  100000            55              3             50   \n",
       "62         0  100000            55              3             50   \n",
       "35         0  100000            55              3             50   \n",
       "107        0  100000            55              3             50   \n",
       "..       ...     ...           ...            ...            ...   \n",
       "132        0  100000            55              3             50   \n",
       "141        0  100000            55              3             50   \n",
       "174        0  100000            55              3             50   \n",
       "165        0  100000            55              3             50   \n",
       "93         0  100000            55              3             50   \n",
       "\n",
       "    kernel_initializer  kernel_regularizer_l1  kernel_regularizer_l2  \\\n",
       "169            uniform                0.00001                0.00010   \n",
       "170            uniform                0.00001                0.00001   \n",
       "62             uniform                0.00001                0.00001   \n",
       "35             uniform                0.00001                0.00001   \n",
       "107            uniform                0.00001                0.00001   \n",
       "..                 ...                    ...                    ...   \n",
       "132            uniform                0.00001                0.10000   \n",
       "141            uniform                0.00001                0.10000   \n",
       "174            uniform                0.00010                0.10000   \n",
       "165            uniform                0.00010                0.10000   \n",
       "93             uniform                0.00010                0.10000   \n",
       "\n",
       "    last_activation     lr  \n",
       "169         sigmoid  0.001  \n",
       "170         sigmoid  0.001  \n",
       "62          sigmoid  0.001  \n",
       "35          sigmoid  0.001  \n",
       "107         sigmoid  0.001  \n",
       "..              ...    ...  \n",
       "132         sigmoid  0.001  \n",
       "141         sigmoid  0.001  \n",
       "174         sigmoid  0.001  \n",
       "165         sigmoid  0.001  \n",
       "93          sigmoid  0.001  \n",
       "\n",
       "[216 rows x 20 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37df8a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of kernel_regularizer_l1')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEXCAYAAABPkyhHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjIklEQVR4nO3de5wcZZ3v8c83IRcVGAJELkkgsIKIChwSAgq7souLg0JwZUUQCOAFw+qKHq/rUQ+ruyu+vBxhFRFdhHgBVFhFzA54AblDEg2XcA0hmEkITCCZEJCEZH7nj3om1LQ9k+5J13TXzPf9es1rum5P/bqqun/9PFX1lCICMzMb2UY1OwAzM2s+JwMzM3MyMDMzJwMzM8PJwMzMcDIwMzOcDOomKSS9Kr2+SNLnapl3EOs5RdL1g41zOJN0tqQnJa2TtNMQrvczkr43VOvLrfcfJC1L7/d/VZk+6OOsUSQdKamzmTFUknSupB9uxfIDfr4bTdKlkv5tqNZXacQlA0nXSfpClfHHS1opaZtay4qI2RHxxQbENDV9oDevOyJ+FBFHb23ZVdbVch/aekgaA3wdODoito2Ipwtaz19sp4j4j4h4XxHr24KvAh9K7/ePTVj/iNSoz/dgSBor6WeSlqbvhiOLXueISwbApcBpklQx/jTgRxGxcehDsjrsAowHFjU7kCG0JwW/33p+BJV5nbWSNLoFyr0FOBVYWUQslUZiMvg5sCPw170jJE0AjgXmSJoh6XZJayQ9IembksZWK6iyWifpE2mZFZLeUzHv2yT9UdLaVOU/Nzf5pvR/TWoKeIOkMyTdklv+jZLmSepO/9+Ym3ajpC9KulXSs5Kul7RzvRtG0mtSWWskLZI0MzftrZLuT+Uvl/TxNH5nSdemZZ6RdLOkqseVpPPTe18raYGk/D6YIWl+mvakpK9XWX5f4KHctvpdtVpVeg/vS6/PkHSLpK9KWi3pMUnH5ObdUdL30z5bLennkl4B/A+we9of6yTtXtnsIGlm2k5r0jpfk5u2VNLHJd2T9tmVksb3s11GSfqspMclPSVpjqQ2SeMkrQNGA3dLenTgPQiSjkjb+G/T8HskPZDe23WS9szNG5I+KOkR4BGl2pCkj6U4npB0Zm7+cWk7/into4skvWxLMVXEt1TSpyTdAzwnaRtJh0m6LW3Hu5X7FSxpL0k3pePuN5K+1bsPVKX2lsp/cz/r/qmy2n93KvO1uWmXSvq2pLmSngP+VrnPt6Rf5o6FdZJ6JJ2Rpu0n6dfp+H9I0okDlVvLdoqIDRHxjYi4BdhU08bdWhEx4v6A7wLfyw1/AFiYXk8DDgO2AaYCDwAfyc0bwKvS60uBf0uv24EngdcBrwB+XDHvkcDryRLwAWnet6dpU9O82+TWcwZwS3q9I7CarPayDXByGt4pTb8ReBTYF3hZGj6vn/d+JNBZZfwYYDHwGWAs8HfAs8Cr0/QngL9OrycAB6fXXwIuSsuPIUuy6mfdpwI7pffwMbJfPOPTtNuB09LrbYHD+imjz7bqZ9vdCLwvtx1fBN5P9qV6NrCiN0bgV8CV6T2NAd7U33YCzgV+mF7vCzwH/H1a7pNp+41N05cCdwG7p/33ADC7n/f0nrTs3um9Xw38oNox18/yAbwKeAuwDJiRxr89lfuatM0/C9xWsdyvU3wvS+95I/CF9J7eCjwPTEjzfwO4Js2/HfBL4EsDHVdVYl0KLASmpHVOAp5O6xqVtufTwMTccfFVsmPyCGBtbh9U20dLgTdX7q/cdt4OGJfey8LctEuBbuDwFMd4cp/vinW0kx1DU8g+68uAM9M2PhhYBby2v3IH2Db9ra8TOLLo78WRWDMAuAx4Z+5Xzaw0johYEBF3RMTGiFgKfAd4Uw1lngh8PyLui4jnyA7EzSLixoi4NyJ6IuIe4PIaywV4G/BIRPwgxXU58CBwXG6e70fEwxHxZ+AnwEE1lt3rMLIvovMi+1XyO+BassQD2Rfq/pK2j4jVEfGH3PjdgD0j4sWIuDnSEVwpIn4YEU+n9/A1sg/lq3PlvErSzhGxLiLuqDP+gTweEd+NiE1k+3k3YBdJuwHHkH1Jr07x/77GMt8F/Coifh0RL5J9Yb0MeGNungsiYkVEPEP2xXlQP2WdAnw9IpZExDrgX4CTVF8zyjuBi4G3RsRdadwHyL6sH4is+fM/gIPytYM0/Zl03EC2H76QtsVcYB3wakkiS6gfTfM/m8o7qY4Ye10QEcvSOk8F5kbE3PTZ+DUwH3irpD2AQ4DPp2PyFrJkNCgRcUlEPBsR68k+nwdKasvN8ouIuDXF8UK1MpTVTucA74qIZWQtCksj4vvpuP4DcBXwj/WU2wpGZDJIB1UXcLykvckOuB9DtrOVNXuslLSW7ICvpclld7JfCL0ez0+UdKikGyR1SeoGZtdYbm/Zj1eMe5zsV1WvfLvi82Rf7PXYHVgWET39rOMEsl9vj0v6vaQ3pPFfIfv1eb2kJZI+3d8KUvPDA6mavgZo46Vt8F6yX9sPKmsGO7bO+AeyedtExPPp5bZkv+yeiYjVgyizzz5J220Zg9snlfv3cbJfmbvUEc9HgJ9ExL25cXsC56fmlzXAM4AqYswfswBPR9/zZr1xTwReDizIldeRxtcrv849yX6YrcmVewRZwt6dbP8838+yNZM0WtJ5kh5Nn+ulaVL+Mzhg2Slx/AL4XETcnIv/0Ir4TwF23dqYh9qITAbJHLIawWnA9RHxZBr/bbJf3ftExPZkzSaVJ5ureYLsy6XXHhXTf0z2q2ZKRLSRNa30lrulrmNXkB10eXsAy2uIq1YrgCnq296/eR0RMS8ijgdeSXbe5Sdp/LMR8bGI2JuspvK/JR1VWbiy8wOfIqtBTYiIHciqz0rlPBIRJ6fyvwz8TFnb/ZY8l/6/PDdu12ozVrEM2FHSDlWm1bVP0i/nKQxun1Tu3z3ImmuerD57Ve8E3i7pI7lxy4APRMQOub+XRcRtuXlq7bZ4FfBnsuaP3rLaIqLeHx2V61xG1iSWj/EVEXEe2WdqR0n5fZv/jD1Hbr8rOznbX3J6N3A88GayHyFTexfrJ64+0ufix8ANEfGdivh/XxH/thFxdi3ltpKRngzeTFb1vSw3fjuydsl1kvYja2OuxU+AMyTtnw7e/1sxfTuyXzkvSJpBdnD26gJ6yNqMq5kL7Cvp3emE27uA/cmacQZF0vj8H1n79nPAJyWNSSfxjgOuUHaZ2ymS2lKTyFrSSS1Jx0p6Vfoy7B1f7YTXdmRfcF3ANpI+D2yfi+dUSRPTL+w1afQWT5xFRBfZF/Cp6dffe4C/qmUbRMQTZCeKL5Q0Ib3vv0mTnwR2qmhGyPsJ8DZJRym73PVjwHrgtn7mH8jlwEeVnSzdlqw2emXUd2XbCuAo4MOS/imNuwj4l94TpcpOSr9zEPH11ny+C/w/Sa9M5U2S9JbBlJfzQ+A4SW9J+2+8shPDkyPicbImo3PTMfgG+jaNPgyMV3ZxxhiycyLj+lnPdmT752myBPIfdcb572TnB86pGH8t2WfztHT8jJF0iHIXEwyWshP2vRcdjE3bppYfpoMyYpNBOh9wG9kOzrdDfpzsi/pZsoP/yhrL+x+yk1K/I2s2+V3FLP8EfEHSs8DnSb+s07LPkx1st6aq5mEVZT9N1jb5MbKD+ZPAsRGxqpbYqphE9isv/zcFmEnWhr4KuBCYFREPpmVOA5amKvZssrZegH2A35C1Ld8OXBgRN1ZZ53VkX7wPkzWDvEDf6nM7sEjZ1TPnAyfV0b76fuATZNvmtdT3hXwaWTv5g8BTZM0tpPd9ObAk7ZPd8wtFxENk2+A/ybbXccBxEbGhjnX3ugT4AdlVZY+RbZt/rreQiPgTWUL4lKT3RcR/k9Wyrkj77T6y/TtYnyI7tu9I5f2Gl875DEpqdz+erAbeRXZMfIKXvptOAd5Atm//jezzuD4t2032ufoe2Q+C58hOtlYzh+y4Ww7cD9R7TupksvNqq/XSFUWnpHMnR5OdO1lB1jT4ZfpPSvV4iOyzOYns8/Nn/rKFoGF6r6gwM2t5kq4EHoyIypq3baURWzMws9aXmlz+Stm9GO1ktYifNzmsYall7wA0s/JRdjno/f1M3j81ZdVjV7L7LnYiawI6O0rcJYekRVRv6vlARPxoqOPJczORmZm5mcjMzEraTLTzzjvH1KlTmx2GmVmpLFiwYFVEVL0Xo5TJYOrUqcyfP7/ZYZiZlYqkyp4MNnMzkZmZORmYmZmTgZmZ4WRgZmY4GZg1VHd3N+effz5r165tdihmdXEyMGugjo4OlixZQkdHR7NDMauLk4FZg3R3d3PXXXcREdx5552uHVipOBmYNUhHRwc9PdmD4np6elw7sFJxMjBrkAULFrBpU/Y8nk2bNvnGSCsVJwOzBpk2bRqjR48GYPTo0UyfPr3JEZnVzsnArEHa29sZNSr7SI0aNYr29vYmR2RWOycDswZpa2tjxowZSOLQQw9l++233/JCZi2ilB3VmbWq9vZ2Vq5c6VqBlY6TgVkDtbW1cc455zQ7DLO6uZnIrIF8B7KVlZOBWQP5DmQrKycDswbxHchWZoUmA0mXSHpK0n39TJekCyQtlnSPpIOLjMesSL4D2cqs6JrBpcBAl1UcA+yT/s4Cvl1wPC3Pbc7l5TuQrcwKTQYRcRPwzACzHA/MicwdwA6SdisyplbnNufymjZtGpIAkOQ7kK1Umn3OYBKwLDfcmcaNSG5zLrfDDz+ciAAgIjj88MObHJFZ7ZqdDFRlXFSdUTpL0nxJ87u6ugoOqznc5lxut95664DDZq2s2cmgE5iSG54MrKg2Y0RcHBHTI2L6xIkThyS4oeY253JbsGBBn2HvPyuTZieDa4BZ6aqiw4DuiHiiyTE1jducy829llqZFX1p6eXA7cCrJXVKeq+k2ZJmp1nmAkuAxcB3gX8qMp5W5zbncnOvpVZmhfZNFBEnb2F6AB8sMoYyqdbmfOKJJzYpGqtXb6+lt912m3sttdJpdjOR5bjNufwOP/xwxo0b51qdlY6TQQtxm3P53Xrrraxfv95XElnpOBm0ELc5l5vvE7EyczJoIX5SVrn5PhErMyeDFtPe3s7ee+/tWkEJ+T4RKzMngxbT+6Qs1wrKx+d8rMycDMwaxOd8rMycDMwaxOd8rMycDMwa6MADD+zz36wsnAzMGujqq68mIrjqqquaHYpZXZwMzBqks7OTlStXArBy5UqWL1/e5IjMaudkYNYgc+bM6TN82WWXNSkSs/o5GZg1SG+toL9hs1bmZGDWILvuuuuAw2atzMnArEFmzZrVZ/j0009vUiRm9XMyaDHd3d2cf/757uSshCZPnry5NrDrrrsyadKkJkdkVjsngxbT0dHBkiVL3MlZSb3jHe9AEieccEKzQzGri5NBC3EXyOV399139/lvVhZOBi3EXSCXm5O5lZmTQQtxF8jl5mRuZeZk0EKmTZu2+bUkd4FcMk7mVmZOBi0k/xD1iPBD1Uvm9a9/fZ/hAw44oEmRmNXPyaCFXH/99QMOm5kVxcmghVRegbJw4cLmBGKDcu+99/YZvueee5oUiVn9nAxaSEQMOGytbdq0aX2edOZzPlYmTgYtZPz48QMOW2trb2/v8wxkP/bSysTJoIX0XpbY37C1Nj/20srMyaCFHHLIIX2GZ8yY0aRIbLDa29vZe++9XSuw0nEyaCGVl5L60tLyaWtr45xzznGtwErHyaCF3HDDDQMOW+tzr7NWVk4GLeQPf/hDn+EFCxY0KRIbLPc6a2XlZGDWIN3d3dx5551EBHfccYdrB1YqTgYt5OCDD+4znO+ryFpfR0dHn76JXDuwMnEyaCEzZ85EEpB1VDdz5swmR2T1mD9//uYbBSOCefPmNTkis9o5GbSQtra2zXetHnLIIb4ipWQmTJgw4LBZK9um2QFYXzNnzuSZZ55xraCEVq9ePeCwWStzzaDF+Dr18qrssvrAAw9sUiRm9XMyaDG+Tt3MmqHwZCCpXdJDkhZL+nSV6RMk/bekeyTdJel1RcfUynydenm5C2srs0KTgaTRwLeAY4D9gZMl7V8x22eAhRFxADALOL/ImFqZr1MvN3dhbWVWdM1gBrA4IpZExAbgCuD4inn2B34LEBEPAlMl7VJwXC3J16mXm7uwtjIrOhlMApblhjvTuLy7gXcASJoB7AlMLjiuluTr1MvNXVhbmRWdDFRlXOXju84DJkhaCPwz8Edg418UJJ0lab6k+V1dXQ0PtBX4OvXycxfWVlZF32fQCUzJDU8GVuRniIi1wJkAym6/fSz9UTHfxcDFANOnTx+Wz4P0derl13tpsFnZFF0zmAfsI2kvSWOBk4Br8jNI2iFNA3gfcFNKECNO5QnHyofdWOvzpcFWVoUmg4jYCHwIuA54APhJRCySNFvS7DTba4BFkh4ku+poxP6s8gnI8vOlwVZWhXdHERFzgbkV4y7Kvb4d2KfoOMqgra2NiRMnsnLlSiZOnOgTkCXT3d3NXXfdRURw55130t7e7n1opeE7kFtId3c3q1atAmDVqlVuaiiZjo4Oenp6AOjp6XHtwErFyaCFdHR09Lm01F8m5bJgwYI+94nMnz+/yRGZ1c7JoIX4y6Tcpk2b1uecj+9AtjJxMmgh/jIpt/b29j7dUfgCACsTJ4MW4i+TcvMdyFZmTgYtxF8m5ec7kK2s/KSzFtPe3s7KlSv9ZVJSvgPZysrJoMX4y8TMmsHNRGZm5mRgZmZOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGTUmA0nvlLRdev1ZSVdLOrjY0MzMbKjUWjP4XEQ8K+kI4C3AZcC3iwvLzMyGUq3JYFP6/zbg2xHxC2BsMSGZmdlQqzUZLJf0HeBEYK6kcXUsa2ZmLa7WL/QTgeuA9ohYA+wIfKKooMzMbGjV+jyD3YBfRcR6SUcCBwBzigrKzMyGVq01g6uATZJeBfwXsBfw48KiMjOzIVVrMuiJiI3AO4BvRMRHyWoLZmY2DNSaDF6UdDIwC7g2jRtTTEhmZjbUak0GZwJvAP49Ih6TtBfww+LCMjOzoVRTMoiI+4GPA/dKeh3QGRHnFRqZmZkNmZquJkpXEF0GLAUETJF0ekTcVFhkZmY2ZGq9tPRrwNER8RCApH2By4FpRQVmZmZDp9ZzBmN6EwFARDyMTyCbmQ0btdYM5kv6L+AHafgUYEExIZmZ2VCrNRmcDXwQ+DDZOYObgAuLCsrMzIZWTckgItYDX09/ZmY2zAyYDCTdC0R/0yPigIZHZGZmQ25LNYNjhyQKMzNrqgGTQUQ8Xkshkm6PiDc0JiQzMxtqjXpAzfj+Jkhql/SQpMWSPl1lepukX0q6W9IiSWc2KCYzM6tRo5JB1fMKkkYD3wKOAfYHTpa0f8VsHwTuj4gDgSOBr0nyIzXNzIZQ0Y+unAEsjoglEbEBuAI4vmKeALaTJGBb4BlgY8FxmZlZTqOSgfoZPwlYlhvuTOPyvgm8BlgB3AucExE9DYrLzMxq0KhkcFo/46slicompbcAC4HdgYOAb0ra/i8Kks6SNF/S/K6urq0I1czMKg2YDCQ9K2ltlb9nJa3tnS8i7uuniE5gSm54MlkNIO9M4OrILAYeA/arLCgiLo6I6RExfeLEibW8NzMzq9GWLi3dbivLnwfskx6Gsxw4CXh3xTx/Ao4Cbpa0C/BqYMlWrtfMzOpQa99EAEh6JbnLSCPiTwPNHxEbJX0IuA4YDVwSEYskzU7TLwK+CFya7nYW8KmIWFXf2zAzs61R68NtZpI902B34ClgT+AB4LVbWjYi5gJzK8ZdlHu9Aji69pDNzKzRaj2B/EXgMODhiNiLrFnn1sKiMjOzIVVrMngxIp4GRkkaFRE3kF35Y2Zmw0Ct5wzWSNoWuBn4kaSn8I1hZmbDRq01g5uAHYBzgA7gUeC4gmIyM7MhVmsyENkVQTeSdRlxZWo2MjOzYaCmZBAR/xoRryXrVG534PeSflNoZGZmNmTq7Y7iKWAl8DTwysaHY2ZmzVBTMpB0tqQbgd8COwPv9yMvzcyGj1qvJtoT+EhELCwwFjMza5KakkFE/MUTyszMbPgo+uE2ZmZWAk4GZmbmZGBmZk4GZmaGk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmOBmYmRk1PgPZrBmuuuoqli9f3vByu7q6WL9+fcPLLdq4ceOYOHFiw8udNGkSJ5xwQsPLtXJxMrCWtXz5cpYteZRdxjb2MO15cRPREw0tcyj0vLiBDeufb2iZT27Y2NDyrLycDKyl7TJ2G2btNqHZYQxbc55Y3ewQrEX4nIGZmTkZmJmZk4GZmeFkYGZmOBmYmRm+mshaWFdXFy+s3+grXgr05PqNjO/qanYY1gIKrxlIapf0kKTFkj5dZfonJC1Mf/dJ2iRpx6LjMjOzlxRaM5A0GvgW8PdAJzBP0jURcX/vPBHxFeAraf7jgI9GxDNFxmXlMHHiRDasf973GRRozhOrGVvAXc1WPkU3E80AFkfEEgBJVwDHA/f3M//JwOUFx2RmBSuqKxEoZ3ciRXUlAo3rTqToZDAJWJYb7gQOrTajpJcD7cCHCo7JzAq2cOFC1nZ3M1ZqeNkbI+hpeKnFevGFF3hh7dqGl7shgq6urlIkg2pHQn+dwhwH3NpfE5Gks4CzAPbYY4/GRGdmpbNNAQkGsiRTZPmtruhk0AlMyQ1PBlb0M+9JDNBEFBEXAxcDTJ8+vem9jBXZoybg3imt1A466KDSNRP1pDI1blzDyy66magRik4G84B9JO0FLCf7wn935UyS2oA3Aac2cuVlbLfsLbOIsru6ugrbHk40llfkseAfYsUoNBlExEZJHwKuA0YDl0TEIkmz0/SL0qz/AFwfEc81cv1FdYEM0NbwEjOrR2VV1Alsanzh659nQ+fjDS/W3SDbUGr1L9WyKvyms4iYC8ytGHdRxfClwKWNXndXV1f/Zyha1IQxo5sdQv3ipV9VZlZO7o7CzMyGd3cUvmlpaPjGJbPyc83AzMycDMzMzMnAzMxwMjAzM4b5CWQrvyc3lOt5BqtfzO4PKcslwk9u2NiniwAbuZwMrGU16jb7ofRiZycAYydPbnIktZlCObezNZ6TgbWsMt5pesEFFwDw4Q9/uMmRmNXH5wzMzMzJwMzMnAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzM8PJwMzMGAF3ILtvm+K5fxuz8hvWyaCMfa6UrW8bcP82ZsPBsE4G7tvGzKw2PmdgZmZOBmZm5mRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmDPP7DMyqueqqq1i+fHkhZXemmwZ77xdppEmTJpXy3hkrBycDswYaN25cs0MwGxQnAxtx/Ova7C/5nIGZmTkZmJmZk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZkxBPcZSGoHzgdGA9+LiPOqzHMk8A1gDLAqIt5UdFxbq6i7WH0Hq5k1Q6HJQNJo4FvA3wOdwDxJ10TE/bl5dgAuBNoj4k+SXllkTK3Od7CaWTMUXTOYASyOiCUAkq4Ajgfuz83zbuDqiPgTQEQ8VXBMDeFf2GY2nBR9zmASsCw33JnG5e0LTJB0o6QFkmYVHJOZmVUoumagKuOiSgzTgKOAlwG3S7ojIh7uU5B0FnAWwB577FFAqGZmI1fRNYNOYEpueDKwoso8HRHxXESsAm4CDqwsKCIujojpETF94sSJhQVsZjYSFZ0M5gH7SNpL0ljgJOCainl+Afy1pG0kvRw4FHig4LjMzCyn0GaiiNgo6UPAdWSXll4SEYskzU7TL4qIByR1APcAPWSXn95XZFxmZtaXIiqb8Fvf9OnTY/78+c0Ow8ysVCQtiIjp1ab5DmQzM3MyMDOzkjYTSeoCHm92HAXaGVjV7CBs0Lz/ymu477s9I6Lq5ZilTAbDnaT5/bXrWevz/iuvkbzv3ExkZmZOBmZm5mTQqi5udgC2Vbz/ymvE7jufMzAzM9cMzMzMycDMzHAyKJykSyQ9Janu/pYkTZN0r6TFki6QpDT+DEldkhamv/c1PvKRS1K7pIfSdv90lelK+2OxpHskHbylZSXtKOnXkh5J/yek8TtJukHSOknfHJp3aDXs4/0k3S5pvaSPNyPGoeZkULxLgfZBLvttsmc47JP+8uVcGREHpb/vbV2I1iv3qNZjgP2BkyXtXzHbMby0T84i209bWvbTwG8jYh/gt2kY4AXgc8CI+MJpBTXu42eADwNfHeLwmsbJoGARcRPZgbWZpL+S1JGe7HazpP0ql5O0G7B9RNwe2Vn+OcDbhyTokW3zo1ojYgPQ+6jWvOOBOZG5A9gh7a+Blj0euCy9voy0L9NzPG4hSwo2NLa4jyPiqYiYB7zYjACbwcmgOS4G/jkippH9IrywyjyTyB7806vykaEnpCaKn0magjVKLY9q7W+egZbdJSKeAEj/X9nAmK0+tezjEafox15aBUnbAm8EfppOAQCMqzZrlXG91wH/Erg8ItanZ0NcBvxdo2MdoWp5VGt/89SyrDWf91MVTgZDbxSwJiIOyo9M7ZgL0uA1ZO3Qk3OzbH5kaEQ8nRv/XeDLRQU7AtX6qNZq84wdYNknJe0WEU+kJqWnGhq11aOWfTziuJloiEXEWuAxSe+EzVemHBgRm3InhD+fmhKelXRYuopoFtkjQnvPJ/SaiR8T2ki1PKr1GmBW2neHAd1pfw207DXA6en16aR9aU1Ryz4eeSLCfwX+AZcDT5CdiOoE3gvsBXQAdwP3A5/vZ9npwH3Ao8A3eemO8S8Bi9LyNwD7Nft9Dqc/4K3Aw2m7/580bjYwO70W2dUojwL3AtMHWjaN34nsKqJH0v8dc9OWkl1ksC4dI/s3exsM978a9vGuaV+sBdak19s3O+4i/9wdhZmZuZnIzMycDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnA2sBkqYOpovvrVjfuqFaV8V6z6i3m2pJ0yVdUGBM63KvOyStkXRtUeuz1uXuKKy0JG0TERsLLH90RGwqqvwa1r9NRMwH5m9lObW+j68ALwc+sDXrs3JyzcBaiqS9Jf1R0qHVuvmWdKmkr0u6AfhyGr5A0m2Slkj6x1xZn5A0L/Xu+q81rv/I9LCZHwP3Shot6Su5cj6Q5hsl6UJJiyRdK2lu77olLZW0c3o9XdKNVdZznKQ703v9jaRd0vhzJV0s6XpgTorn2jRtrl56oFG3pNMHiK/P+6jlvUfEb4Fna5nXhh/XDKxlSHo1Wd/yZwJfI+sa4BFJh5J1893bM+u+wJsjYpOkS4HdgCOA/cj6mPmZpKPJHj4zg6z7iGsk/U1kz5fYkhnA6yLiMUlnkfU9dIikccCt6Yt6GjAVeD1Zd9QPAJfU8XZvAQ6LiFD2pLpPAh9L06YBR0TEnyUd2btARLw1badpwPeBn5N1b1Itvj7vo464bIRyMrBWMZGs87YTgMcZuJvvn1Y0e/w8InqA+3t/YQNHp78/puFtyZJDLcngrtwX6NHAAbkaR1sq54gURw+wMtVU6jEZuDJ1OjgWyH9hXxMRf662UKpx/AA4MSK6U9KrFt+GivdhNiAnA2sV3WQPHDk8/V8TFd185zxXMbw+91q5/1+KiO8MIpZ8+SJ7ENF1+RkkvW2A5TfyUhPs+H7m+U/g6xFxTfr1f24/68+vczRZzekLEdF7wr2/+I7srxyzanzOwFrFBrJHQc4CjqVKN991lncd8B5lDxNC0iRJg3m62HXA2ZLGpHL2lfQKsmaeE9K5g12AI3PLLCVr6oGsplNNG7A8vT69n3kqnQfcExFX1BCfWV1cM7CWERHPSToW+DXwQ+C9kj4LjCH7RXx3HWVdL+k1wO2pqWkdcCr1P1Tme2TnBv6grKAusqR1FXAUWRfjDwN3ktVuAP4V+C9Jn0njqzmXrBlsOXAHWbfmW/JxYJGkhWn48wPEVzdJN5Odd9lWUifw3soahw1f7sLabJAkbRsR6yTtBNwFHB4RK5sdl9lguGZgNnjXStqB7ATwF50IrMxcM7ARSdLrya7KyVsfEYc2I54ipZrLb6tMOir6Pk/bRjAnAzMz89VEZmbmZGBmZjgZmJkZTgZmZgb8f2uKqki8SVitAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_regularizer_l1'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "475c2ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of kernel_regularizer_l2')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_regularizer_l2'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce76a4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.7)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEXCAYAAABYsbiOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAi6UlEQVR4nO3de7xVdZ3/8debq3dAw9IDXirJasomGNBGGybLKDWbH5OpeevmaGNoP7s4/bKsZqbmZ9OMTBoxZkaWWuok4xDYRTTLC1CkIqmICAdR0AQFSwQ+88f3e2Sd7V6HvY9nnX0OvJ+Px37svdb6fr/rs9bae3/WfSkiMDMzq2dAqwMwM7O+y0nCzMxKOUmYmVkpJwkzMyvlJGFmZqWcJMzMrJSTRA+RFJJenT9Pk3RBI2W7MZ4PSLqpu3FuzySdJelxSesl7dWL4/2spMt6a3yF8f6NpBV5ev+8zvBuf896iqSJktpbGUMtSRdKuvIl1O/y993TJF0h6R97a3y1nCQySXMkfalO/+MkPSZpUKNtRcSZEfHlHojpgPxDf2HcEfH9iDjqpbZdZ1x97sfcDEmDga8DR0XEbhHxZEXjedF8ioh/joiPVDG+bfgacHae3t+2YPw7pJ76fXeHpEMl/VTSHyStkfQjSftUOU4nia2uAE6RpJr+pwDfj4hNvR+SNeHlwE7AolYH0ov2p+LpbWblqD+Ps1GSBra43RHAdOAA0vJ/BvhOFTF1cJLY6sfAnsARHT0kjQCOAWZIGi/pdklrJa2S9A1JQ+o1VLt5KOlTuc6jkj5UU/ZoSb+V9HTedXBhYfCt+X1t3qVwmKTTJd1WqP8WSfMkrcvvbykMmyvpy5J+JekZSTdJelmzM0bSa3NbayUtkvSewrB3S7ovt79S0idz/5dJujHX+YOkX0qq+32TdHGe9qclLZBUXAbjJc3Pwx6X9PU69ccA9xfm1S/qbYXlafhI/ny6pNskfU3SU5IelvSuQtk9JX0nL7OnJP1Y0q7AT4B98/JYL2nf2t0Xkt6T59PaPM7XFoYtk/RJSXfnZXaNpJ1K5ssASZ+T9Iik1ZJmSBomaaik9cBA4HeSHup6CYKkw/M8/uvc/SFJi/O0zZG0f6FsSPp7SQ8CDypvPUk6L8exStIHC+WH5vm4PC+jaZJ23lZMNfEtk/QZSXcDGyQNUlpr/nWej7+TNLFQ/kBJt+bv3c8kXdKxDFRnay+3//aScf9IaW/Butzm6wvDrpD0TUmzJG0A/lqF37ek/y58F9ZL2iLp9DzsYG1d679f0vFdtdvIfIqIn0TEjyLi6Yh4FvgG8JeN1O22iPArv4D/BC4rdP8dsDB/HgscCgwiZfHFwLmFsgG8On++AvjH/HkS8DjwZ8CuwA9qyk4E3kBK2G/MZd+bhx2Qyw4qjOd04Lb8eU/gKdLWziDgxNy9Vx4+F3gIGAPsnLu/WjLtE4H2Ov0HA0uAzwJDgLeR1l5ek4evAo7In0cAb86fvwJMy/UHk5KvSsZ9MrBXnobzgMeAnfKw24FT8ufdgENL2ug0r0rm3VzgI4X5+DzwUdKf7VnAox0xAv8DXJOnaTDwV2XzCbgQuDJ/HgNsAN6R6306z78hefgy4C5g37z8FgNnlkzTh3LdV+Zpvx74Xr3vXEn9AF4NvBNYAYzP/d+b231tnuefA35dU++nOb6d8zRvAr6Up+ndwLPAiFz+34GZufzuwH8DX+nqe1Un1mXAQmB0Hmcb8GQe14A8P58ERha+F18jfScPB54uLIN6y2gZ8Pba5VWYz7sDQ/O0LCwMuwJYR/ojHkDaWr2C/PuuGcck0ndoNOm3vgL4YJ7HbwaeAF5f1m4X86bu+PKwc4E7Kv1frLLx/vbKX7Z1wM65+1fAJ7pYOP9V6C5LEpdT+GMm/YmU/rjzl/Tf8ucD6DpJnALcVVP/duD0/Hku8LnCsI8Bs0vG+6IfVu5/BOlPe0Ch31XAhfnzclIy3aOm3peAG8qmcxvL4SngkPz5VuCLwMu2UafTvCqZd3PpnCSWFIbtksu/AtgH2EL+E9zWfKJzkrgA+GFh2ABgJTAxdy8DTi4M///AtJJp+jnwsUL3a0iJrWMaG0kS/wA8Aryh0P8nwIdrYnwW2L9Q72010/zHmnm5mrTSJFJSfFVh2GHAw119r+rEugz4UKH7MxQSYu43BzgN2I+UtHYpDLuSbiaJmnLD8/QPi62/5Rk1Za6g5k+b9LtezdYVpvcDv6wp8y3gC2XtdjFvXjS+3P+NwB86xlnVy7ubCiLiNmANcJykVwJ/QVrzR9IYpd0nj0l6GvhnoJFdN/uS1ig6PFIcKGmCpJuVDkKtA85ssN2Oth+p6fcIaS2sw2OFz8+S1kibsS+wIiK2lIxjMmlt7xFJt0g6LPe/iLS2epOkpZLOLxtB3o2xOG/urwWGsXUefJj0A/y90u60Y5qMvysvzJtIm+6Q5s9o4A8R8VQ32uy0TPJ8W0H3lknt8n2EtFb68ibiOZeUtO4p9NsfuDjvxllL+qNRTYzF7yzAk9H5uFxH3CNJCXZBob3ZuX+ziuPcH3hfR5u53cNJCXxf0vJ5tqRuwyQNlPRVSQ/l3/WyPKj4G+yybUnDSCtEF0TELwvxT6iJ/wOklZCXFHMe56tJyf6cwjgr4STxYjOAU0lr6TdFxOO5/zeB3wMHRcQepN0vtQe561lF+tPpsF/N8B+QNtVHR8Qw0i6ajnZjG20/SvoyFu1HWnPtKY8Co9X5eMIL44iIeRFxHLA36bjOD3P/ZyLivIh4JXAs8H8lHVnbuNLxh88Ax5PW3IeTtuaU23kwIk7M7f8LcK3SsYFt2ZDfdyn0e0W9gnWsAPaUNLzOsKaWiSSRln93lknt8u1Yg368fvG63ge8V9K5hX4rgL+LiOGF184R8etCmW1NZ4cnSFsZry+0NSwiml0ZqR3nCtKWRDHGXSPiq6Tf1J6Sisu2+BvbQGG5Kx0ULktaJwHHAW8nrZwc0FGtJK5O8u/iB8DNEfGtmvhvqYl/t4g4q5F2u5KPH/0M+HJEfK87bTTDSeLFZpC+MB8Fvlvovztpv+d6SQeT9mE34ofA6ZJel7/UX6gZvjtprehPksaTvrQd1pB2e7yypO1ZwBhJJ+UDfe8HXgfc2GBsLyJpp+KLtP98A/BpSYPzwcNjgaslDVG6bmNYRDxPmj+bczvHSHp1/pPs6L+5zih3J/3xrQEGSfo8sEchnpMljcxr5Gtz73rtdBIRa0h/zCfntcUPAa9qZB5ExCrSWtqlkkbk6X5rHvw4sFdee6znh8DRko5UOi33POA54Ncl5btyFfCJfJB2N9LW6zXR3Jl2jwJHAlMkfSz3mwb8Q8cBWqWD4e/rRnwdW0r/CfybpL1ze22S3tmd9gquBI6V9M68/HZSOiA9KiIeAeYDF+bv4GGk72SHB4CdlE4KGUw65jK0ZDy7k5bPk6TE8s9NxvlPpOMP59T0v5H02zwlf38GS/oLFU5i6A5JbcAvgEsiYtpLaatRThI1ImIZ6Qe9K2kNv8MnSX/gz5B+FNc02N5PSMcZfkHa/fKLmiIfA74k6Rng8+Q18Vz3WdKX8Fd5k/XQmrafJJ19dR7pS/5p4JiIeKKR2OpoI60VFl+jgfcA7yKtNV4KnBoRv891TgGW5U31M0kHoQEOIq3trCcdJ7k0IubWGecc0h/yA6TdKX+i82b4JGCR0tk8FwMnRMSfGpyejwKfIs2b19PcH/UppP3/vyftaz4XIE/3VcDSvEz2LVaKiPtJ8+A/SPPrWODYiNjYxLg7XA58j3Rc5mHSvPl4s41ExHJSoviMpI9ExH+RtsquzsvtXtLy7a7PkL7bd+T2fkY6ftJtEbGCtIb/WdIKxArSsuz4z/oA6djHk8A/kn6Pz+W660i/q8tIKwobgLJrgGaQvncrgfuAO5oM9UTSsZmntPUMpw9ExDPAUcAJpET9GGmelyWrRn2EtNL4hcL41r/ENrvUcSaHmVm/Jeka4PcRUbulbi+RtyTMrN/Ju25epXQtySTSVsePWxzWdqnyJCFpktKFJEvqneGidKHZwvy6V9JmSXs2UtfM+gdJ+6nzRWfFV+3JHI14BemU5vXAVOCs6Me3JlG6+LLevPlAy2OrcndTPqvgAdKFMO3APODEiLivpPyxpOsS3tZsXTMz63lVb0mMJ12wtDQfuLuatFlY5kTSQcHu1DUzsx5W9Y202uh8pko7MKFewXx66CTg7GbqSjoDOANg1113HXvwwQc3FNjq1auJjRvZa3Al9+sy4MnnN6MhQ9h77717tF0vu97h5dd/NbvsFixY8ERE1L2WpOokUe9is7L9W8cCv4qIPzRTNyKmk+6KyLhx42L+/PkNBTZ16lQ2tj/CqfuMaKi8NW/GqqcYMmp/pkyZ0qPtetn1Di+//qvZZSep9s4NL6g6SbTT+UrIUaRzhus5ga27mpqta2b9xJo1a/jTc5uYsao7dz2xRjz+3CZ2WrOmR9qq+pjEPOCgfMXoEFIimFlbKF+9+lek+580VdfMzKpT6ZZERGySdDbpqtqBwOURsUjSmXl4x2Xlf0O6T9KGbdWtMl4zq97IkSPZ+Nyz3t1UoRmrnmLIyO7cY/HFKn8CVETMIt1jqNhvWk33FaTb4W6zrpmZ9R5fcW1mZqWcJMzMrJSThJmZlXKSMDOzUk4SZmZWqvKzm8x6mi/G6h09eUGW9V87bJLwH031/Cdj1v/tsEnC+i9fjNU7evKCLOu/dtgk4T+a6vlPxqz/84FrMzMr5SRhZmalnCTMzKyUk4SZmZVykjAzs1JOEmZmVspJwszMSjlJmJlZKScJMzMr5SRhZmalnCTMzKyUk4SZmZVykjAzs1JOEmZmVspJwszMSjlJmJlZKScJMzMr5SRhZmalnCTMzKyUk4SZmZUa1OoAzGzH8/jGTcxY9VSrw2jIU89vBmDE4IEtjqRxj2/cxOgeastJwsx6VVtbW6tDaMrz7e0ADBk1qsWRNG40PTefnSTMrFdNnjy51SE0ZerUqQBMmTKlxZG0ho9JmJlZKScJMzMr5d1NZrZduO6661i5cmWPt9uej0l07HbqaW1tbX16F1zlWxKSJkm6X9ISSeeXlJkoaaGkRZJuKfT/RO53r6SrJO1UdbxmZkVDhw5l6NChrQ6jZSrdkpA0ELgEeAfQDsyTNDMi7iuUGQ5cCkyKiOWS9s7924ApwOsi4o+SfgicAFxRZcxm1j/15bXx/qzqLYnxwJKIWBoRG4GrgeNqypwEXB8RywEiYnVh2CBgZ0mDgF2ARyuO18zMCqpOEm3AikJ3e+5XNAYYIWmupAWSTgWIiJXA14DlwCpgXUTcVDsCSWdImi9p/po1ayqZCDOzHVXVSUJ1+kVN9yBgLHA08E7gAkljJI0gbXUcCOwL7Crp5Bc1FjE9IsZFxLiRI0f2bPRmZju4qs9uaodOV4eP4sW7jNqBJyJiA7BB0q3AIXnYwxGxBkDS9cBbgCurDdnMzDpUvSUxDzhI0oGShpAOPM+sKXMDcISkQZJ2ASYAi0m7mQ6VtIskAUfm/mZm1ksq3ZKIiE2SzgbmAAOByyNikaQz8/BpEbFY0mzgbmALcFlE3Asg6VrgN8Am4LfA9CrjNTOzziq/mC4iZgGzavpNq+m+CLioTt0vAF+oNEAzMyvl23KYmVkpJwkzMyvlJGFmZqWcJMzMrJSThJmZlXKSMDOzUk4SZmZWyknCzMxKOUmYmVkpJwkzMyvlJGFmZqWcJMzMrJSThJmZlXKSMDOzUk4SZmZWyknCzMxKVf7QIbMqPL5xEzNWPdXqMBr21PObARgxeGCLI2nc4xs3dXpAve2YnCSs32lra2t1CE17vr0dgCGjRrU4ksaNpn/Oa+tZThLW70yePLnVITRt6tSpAEyZMqXFkZg1x8ckzMyslJOEmZmVcpIwM7NSThJmZlbKScLMzEo5SZiZWSknCTMzK+UkYWZmpZwkzMyslJOEmZmVcpIwM7NSThJmZlbKScLMzEo5SZiZWSknCTMzK1V5kpA0SdL9kpZIOr+kzERJCyUtknRLof9wSddK+r2kxZIOqzpeMzPbqtKHDkkaCFwCvANoB+ZJmhkR9xXKDAcuBSZFxHJJexeauBiYHRF/K2kIsEuV8ZqZWWdVP5luPLAkIpYCSLoaOA64r1DmJOD6iFgOEBGrc9k9gLcCp+f+G4GNPRlcf3pOsp+RbGatUHWSaANWFLrbgQk1ZcYAgyXNBXYHLo6IGcArgTXAdyQdAiwAzomIDcXKks4AzgDYb7/9Gg+snz27189INrNWqDpJqE6/qBPDWOBIYGfgdkl35P5vBj4eEXdKuhg4H7igU2MR04HpAOPGjattu1R/e06yn5FsZq1Q9YHrdui0x2EU8GidMrMjYkNEPAHcChyS+7dHxJ253LWkpGFmZr2k6iQxDzhI0oH5wPMJwMyaMjcAR0gaJGkX0u6oxRHxGLBC0mtyuSPpfCzDzMwqVunupojYJOlsYA4wELg8IhZJOjMPnxYRiyXNBu4GtgCXRcS9uYmPA9/PCWYp8MEq4zUzs86qPiZBRMwCZtX0m1bTfRFwUZ26C4FxVcZnZmblGtrdJOl9knbPnz8n6XpJPj5gZrada/SYxAUR8Yykw4F3At8FvlldWGZm1hc0miQ25/ejgW9GxA3AkGpCMjOzvqLRJLFS0reA44FZkoY2UdfMzPqpRv/ojyedoTQpItYCewKfqiooMzPrGxo9u2kf4H8i4jlJE4E3AjOqCsrMzPqGRrckrgM2S3o18G3gQOAHlUVlZmZ9QqNJYktEbAL+D/DvEfEJ0taFmZltxxpNEs9LOhE4Fbgx9xtcTUhmZtZXNJokPggcBvxTRDws6UDgyurCMjOzvqChJJGfJPdJ4B5Jf0a6O+tXK43MzMxarqGzm/IZTd8FlpGeETFa0mkRcWtlkZmZWcs1egrsvwJHRcT9AJLGAFeRHhZkZmbbqUaPSQzuSBAAEfEAPnBtZrbda3RLYr6kbwPfy90fID1z2szMtmONJomzgL8HppCOSdwKXFpVUGZm1jc0lCQi4jng6/llZmY7iC6ThKR7gCgbHhFv7PGIzMysz9jWlsQxvRKFmZn1SV0miYh4pJFGJN0eEYf1TEhmZtZX9NSDg3bqoXbMzKwP6akkUXrcwszM+i8/gtTMzEr1VJJQD7VjZmZ9SE8liVN6qB0zM+tDtnWdxDPUP94gICJiD9KHeyuIzczMWmxbp8Du3luBmJlZ39PovZsAkLQ3hdNdI2J5j0dkZmZ9RkPHJCS9R9KDwMPALaSHD/2kwrjMzKwPaPTA9ZeBQ4EHIuJA4EjgV5VFZWZmfUKjSeL5iHgSGCBpQETcDLypurDMzKwvaPSYxFpJuwG/BL4vaTWwqbqwzMysL2h0S+JWYDhwDjAbeAg4tqKYzMysj2g0SQiYA8wFdgOuybufzMxsO9ZQkoiIL0bE60mPMN0XuEXSzyqNzMzMWq7Z23KsBh4DngT2bqSCpEmS7pe0RNL5JWUmSlooaZGkW2qGDZT0W0k3NhmrmZm9RA0duJZ0FvB+YCRwLfDRiLivgXoDgUuAdwDtwDxJM4t1JQ0HLgUmRcTyfMFe0TnAYmCPRmI1M7Oe0+iWxP7AuRHx+oj4QiMJIhsPLImIpRGxEbgaOK6mzEnA9R1Xb0fE6o4BkkYBRwOXNTg+MzPrQY0ekzg/IhZ2o/02YEWhuz33KxoDjJA0V9ICSacWhv078GlgS9kIJJ0hab6k+WvWrOlGiGZmVqapezd1Q73nTNTeVXYQMJZ0FffOwO2S7iAlj9URsUDSxLIRRMR0YDrAuHHj/IQ8M7MeVHWSaAdGF7pHAY/WKfNERGwANki6FTgEeDPwHknvJt1UcA9JV0bEyRXHbGZmWdWPL50HHCTpQElDgBOAmTVlbgCOkDRI0i7ABGBxRPxDRIyKiANyvV84QZiZ9a5KtyQiYpOks0kX4g0ELo+IRZLOzMOnRcRiSbOBu0nHHi7zQ4zMzPqGqnc3ERGzgFk1/abVdF8EXNRFG3NJV3ubmVkvqnp3k5mZ9WNOEmZmVspJwszMSjlJmJlZKScJMzMr5SRhZmalnCTMzKyUk4SZmZVykjAzs1JOEmZmVspJwszMSjlJmJlZKScJMzMr5SRhZmalnCTMzKyUk4SZmZVykjAzs1JOEmZmVspJwszMSjlJmJlZKScJMzMr5SRhZmalnCTMzKyUk4SZmZVykjAzs1JOEmZmVspJwszMSjlJmJlZKScJMzMr5SRhZmalBrU6ALO+5LrrrmPlypU93m57ezsAU6dO7fG229ramDx5co+3awZOEma9YujQoa0OwaxbnCTMCrxGbtaZj0mYmVmpypOEpEmS7pe0RNL5JWUmSlooaZGkW3K/0ZJulrQ49z+n6ljNzKyzSnc3SRoIXAK8A2gH5kmaGRH3FcoMBy4FJkXEckl750GbgPMi4jeSdgcWSPppsa6ZmVWr6i2J8cCSiFgaERuBq4HjasqcBFwfEcsBImJ1fl8VEb/Jn58BFgNtFcdrZmYFVSeJNmBFobudF//RjwFGSJoraYGkU2sbkXQA8OfAnXWGnSFpvqT5a9as6bnIzcys8iShOv2ipnsQMBY4GngncIGkMS80IO0GXAecGxFPv6ixiOkRMS4ixo0cObLnIjczs8pPgW0HRhe6RwGP1inzRERsADZIuhU4BHhA0mBSgvh+RFxfcaxmZlaj6i2JecBBkg6UNAQ4AZhZU+YG4AhJgyTtAkwAFksS8G1gcUR8veI4zcysjkq3JCJik6SzgTnAQODyiFgk6cw8fFpELJY0G7gb2AJcFhH3SjocOAW4R9LC3ORnI2JWlTGbmdlWlV9xnf/UZ9X0m1bTfRFwUU2/26h/TMPMzHqJr7g2M7NSThJmZlbKScLMzEo5SZiZWSknCTMzK+UkYWZmpZwkzMyslJOEmZmVcpIwM7NSThJmZlbKScLMzEo5SZiZWSknCTMzK+UkYWZmpZwkzMyslJOEmZmVcpIwM7NSThJmZlbKScLMzEo5SZiZWSknCTMzK+UkYdYL1q1bx8UXX8zTTz/d6lDMmuIkYdYLZs+ezdKlS5k9e3arQzFripOEWcXWrVvHXXfdRURw5513emvC+pVBrQ5ge3PdddexcuXKHm+3vb0dgKlTp/Z42wBtbW1Mnjy5krZ3dLNnz2bLli0AbNmyhdmzZ3P88ce3OCqzxnhLop8YOnQoQ4cObXUY1g0LFixg8+bNAGzevJn58+e3OCKzxnlLood5bdxqjR07ljvuuIPNmzczcOBAxo0b1+qQzBrmLQmzik2aNIkBA9JPbcCAAUyaNKnFEZk1zknCrGLDhg1j/PjxSGLChAnssccerQ7JrGHe3WTWCyZNmsRjjz3mrQjrd5wkzHrBsGHDOOecc1odhlnTvLvJzMxKOUmYmVkpJwkzMyvlJGFmZqUqTxKSJkm6X9ISSeeXlJkoaaGkRZJuaaaumZlVp9KzmyQNBC4B3gG0A/MkzYyI+wplhgOXApMiYrmkvRuta2Zm1ap6S2I8sCQilkbERuBq4LiaMicB10fEcoCIWN1EXTMzq1DV10m0ASsK3e3AhJoyY4DBkuYCuwMXR8SMBusi6QzgjNy5XtL9PRN6n/Qy4IlWB2Hd5uXXf23vy27/sgFVJwnV6Rd1YhgLHAnsDNwu6Y4G6xIR04HpLzHOfkHS/Ijw3eH6KS+//mtHXnZVJ4l2YHShexTwaJ0yT0TEBmCDpFuBQxqsa2ZmFar6mMQ84CBJB0oaApwAzKwpcwNwhKRBknYh7VJa3GBdMzOrUKVbEhGxSdLZwBxgIHB5RCySdGYePi0iFkuaDdwNbAEui4h7AerVrTLefmCH2K22HfPy67922GWniBft5jczMwN8xbWZmXXBScLMzEo5SbSIpMslrZZ0bzfqjpV0T75dyVRJyv1Pl7Qm3+JkoaSP9HzkO65t3SZGydQ8/G5Jb95WXUl7SvqppAfz+4jcfy9JN0taL+kbvTOF1sAyPljS7ZKek/TJVsTY25wkWucKoLuPKfsm6QLCg/Kr2M41EfGm/LrspYVoHQq3iXkX8DrgREmvqyn2LrYukzNIy2lbdc8Hfh4RBwE/z90AfwIuAHaIP6K+oMFl/AdgCvC1Xg6vZZwkWiQibiV94V4g6VWSZktaIOmXkg6urSdpH2CPiLg90lkHM4D39krQO7ZGbhNzHDAjkjuA4Xl5dVX3OOC7+fN3ycsyIjZExG2kZGG9Y5vLOCJWR8Q84PlWBNgKThJ9y3Tg4xExlrQGeWmdMm2kCw07tOd+HSbnXR3XShqN9ZR6t4lpa7BMV3VfHhGrAPL73j0YszWnkWW8w/EzrvsISbsBbwF+lA8xAAytV7ROv47zmP8buCoinsvXonwXeFtPx7qDauQ2MWVlGrrFjLWcl1MdThJ9xwBgbUS8qdgz7yddkDtnkvZzjyoUeeF2JRHxZKH/fwL/UlWwO6BGbzFTr8yQLuo+LmmfiFiVd02txlrFtwKqw7ub+oiIeBp4WNL74IUzZQ6JiM2FA9Gfz7sknpF0aD6r6VTSrU06jld0eA/p9ibWMxq5TcxM4NS87A4F1uXl1VXdmcBp+fNp5GVpLeFbAdUTEX614AVcBawiHQBrBz4MHAjMBn4H3Ad8vqTuOOBe4CHgG2y9cv4rwKJc/2bg4FZP5/b0At4NPJDn+//L/c4EzsyfRTo75iHgHmBcV3Vz/71IZzU9mN/3LAxbRjq5YX3+jryu1fNge381sIxfkZfF08Da/HmPVsdd5cu35TAzs1Le3WRmZqWcJMzMrJSThJmZlXKSMDOzUk4SZmZWyknCzMxKOUlYnybpgO7cTv0ljG99b42rZrynN3tLcEnjJE2tMKb1+f1N+fbYi/J9wd5f1Tit7/FtOWy7JGlQRGyqsP2BEbG5qvYbGP+giJgPzH+J7TQyHc8Cp0bEg5L2BRZImhMRa1/KuK1/8JaE9RuSXinpt5Im1LuluqQrJH1d0s3Av+TuqZJ+LWmppL8ttPUpSfPymvEXGxz/xPwgoB8A90gaKOmiQjt/l8sNkHRpXvO+UdKsjnFLWibpZfnzOElz64znWEl35mn9maSX5/4XSpou6SZgRo7nxjxslrY+bGqdpNO6iK/TdGxruiPigYh4MH9+lHR/qZGNzDPr/7wlYf2CpNeQ7u//QeBfSbdJeFDSBNIt1TvudjsGeHtEbJZ0BbAPcDhwMOk+PNdKOor0YKDxpFtpzJT01kjP+NiW8cCfRcTDks4g3Z/pLyQNBX6V/8DHAgcAbyDd+nsxcHkTk3sbcGhEhNLTBT8NnJeHjQUOj4g/SprYUSEi3p3n01jgO8CPSbd6qRdfp+loIi4kjSfdsPChZupZ/+UkYf3BSNKN7yYDj9D1LdV/VLP75McRsQW4r2ONHDgqv36bu3cjJY1GksRdhT/Wo4A3FrZQhuV2Ds9xbAEey1s2zRgFXJNv2DgEKP6Rz4yIP9arlLdQvgccHxHrcjKsF9/GmuloSI7ne8BpedpsB+AkYf3BOtLDYP4yv6+NmluqF2yo6X6u8FmF969ExLe6EUuxfZEeEjWnWEDS0V3U38TW3bw7lZT5D+DrETEzby1cWDL+4jgHkra0vhQRHQf6y+KbWNZOGUl7AP8DfC7SU/dsB+FjEtYfbCQ91vNU4Bjq3FK9yfbmAB9SetATktokdeeJcHOAsyQNzu2MkbQraXfR5Hxs4uXAxEKdZaRdRpC2jOoZBqzMn08rKVPrq8DdEXF1A/E1Jd82+79Ij2b9UbP1rX/zloT1CxGxQdIxwE+BK4EPS/ocMJi0Bv27Jtq6SdJrgdvzLqv1wMk0/8Cfy0jHHn6j1NAaUjK7DjiSdDv3B4A7SVtDAF8Evi3ps7l/PReSdqetBO4g3UJ+Wz4JLJK0MHd/vov4mnU88FZgL0mn536nR8TC0hq23fCtws0qIGm3iFgvaS/gLuAvI+KxVsdl1ixvSZhV40ZJw0kHnr/sBGH9lbckzGpIegPpLJ6i5yJiQiviqVLe0vl5nUFHRudnptsOyknCzMxK+ewmMzMr5SRhZmalnCTMzKyUk4SZmZX6X+deDs2jARRTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_regularizer_l2'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n",
    "plt.ylim(0.6,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09669121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of bias_regularizer')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEXCAYAAABPkyhHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAirElEQVR4nO3de5xcdX3/8dc7CSSgZBMgEMgGAjWosUSabBKQXmj1p4uXgCIUFMJFpVip2KpU+lOrbW29tD4KPwSKCiFeuFho3SpdbFVEIkKykATDRUMAs4GQQMIGogkk+fz+ON9NZiazm9lkz5zZnffz8djHzvdcP3POmfnM93suX0UEZmbW3EYUHYCZmRXPycDMzJwMzMzMycDMzHAyMDMznAzMzAwng7qSFJJelV5fI+lTtUy7B+t5r6Qf7Gmcw5mkD0p6RtKLkg6q43r/RtLX6rW+kvW+U9Kq9H5/r8r4Po+z4XQcSbpT0vv3Yv7lkk4avIgaj3yfQe0k3QHcGxGfrhh+CvBvQGtEbO1n/gCmRsSKGtZV07SSpgCPA/v0t+7BkD4M34yI1jzXkxdJ+wAbgeMjYmmO6zmJBtlOkh4D/ioivtvH+JqPyaFM0p1k+6TuCXmocM1gYOYD50hSxfBzgG/l/WVse+1QYAywvOhA6uhIhsD7lTSq6BiqySuuRny/TgYD85/AgcAf9A6QNB54O7BA0mxJ90h6XtLTkq6UtG+1BUmaL+kfSsofT/M8JemCimnfJukBSRtTlf8zJaPvSv+fT00BJ0g6T9LdJfO/QdIiST3p/xtKxt0p6e8lLZT0gqQfSDp4oBtG0mvTsp5PVeq5JePeKumhtPzVkj6Whh8s6XtpnvWSfiqp6jEp6fL03jdK6pJUug9mS1qcxj0j6ctV5j8GeLRkW/1I0pTUTDKqZLodzQm921HSP0vaIOlxSSeXTHugpOvTPtsg6T8lvQL4b+DwtD9elHS4pM9I+mbJvHPTdno+rfO1JeOekPQxScvSPrtZ0pg+tssISZ+U9KSktZIWSGqRNFrSi8BIYGmqIfTlrZJWSnpW0pd690GV42iv9kFF3L3b/n2Sfg38KA2/QNLDaXveIenIknneLOnRtE2ukvSTkn1VuX132bcl434n7f/n0nv+lqRxFdv/ryUtAzZJGpWGvSmN7/2svShpU1rPlDTu7ZKWpGl+Jml6f8vtbxvVXUT4bwB/wFeBr5WU/wxYkl7PBI4HRgFTgIeBj5RMG8Cr0uv5wD+k1+3AM8DvAq8Avl0x7UnAsWTJe3qa9tQ0bkqadlTJes4D7k6vDwQ2kNVeRgFnpfJBafydwGPAMcB+qfz5Pt77SUB3leH7ACuAvwH2Bf4EeAF4dRr/NPAH6fV4YEZ6/U/ANWn+fciSrPpY99nAQek9fBRYA4xJ4+4BzkmvX0nWDFRtGWXbqo9tdyfw/pLt+DLwAbIv1Q8CT/XGCHwfuDm9p32AP+prOwGfIWumIG3rTcD/SfNdmrbfvmn8E8B9wOFp/z0MXNTHe7ogzXt0eu+3Ad+odsz1MX8AP07rOQL4ZcX7v3sw90GVfbGA7JjfDzg1vZfXpnV8EvhZmv5gsia+d6Vxl6R98/7K7dvHvi7dr69K2340MIHsB9W/lsz7BLAEmAzsVzLsTVXexz+m+fcBZgBrgTnpeDk3zTe6r+U20p9rBgN3A3C6pP1SeV4aRkR0RcTPI2JrRDxBdh7hj2pY5hnA9RHxi4jYRHZg7xARd0bEgxGxPSKWATfWuFyAtwG/iohvpLhuBB4B3lEyzfUR8cuI+C1wC3BcjcvudTzZF8DnI+KliPgR8D2yxAPZh3aapLERsSEi7i8ZfhhwZES8HBE/jfSpqRQR34yI59J7+BeyD/KrS5bzKkkHR8SLEfHzAcbfnycj4qsRsY1sPx8GHCrpMOBksi/pDSn+n9S4zD8Fvh8R/xMRLwP/TPZl+IaSaa6IiKciYj3wX/S9T94LfDkiVkbEi8BlwJkD/NX5hYhYHxG/Bv6VnfutTE774DMRsSkde38G/FNEPBxZk+s/Asel2sFbgeURcVsadwVZMhqwiFiRtv2WiFgHfJldP09XRMSqFFdVkv4UeA9wWtqPHwD+LSLujYhtEXEDsIXs81HzcoviZDBAEXE3sA44RdLRwCyyX/JIOkZZs8caSRvJDuZamlwOB1aVlJ8sHSlpjqQfS1onqQe4qMbl9i77yYphTwKTSsqlH6rfkH2xD8ThwKqI2N7HOk4j+zA/mar2J6ThXyL7JfiD1Ezxib5WIOmjqfmgR9LzQAs7t8H7yH5tP6KsGeztA4y/Pzu2TUT8Jr18Jdmvu/URsWEPllm2T9J2W8We7ZPK/fsk2S/nQwcQT+Wxd3i1iXLaB6XrPhK4PDWxPA+sB0S2Xco+I+lHQ3eN66h8H4dIuklZk+VG4Jvs+nlaVWXW0mX8HnAl8M6UUHrj/2hv/Ok9TKZ8e/a73CI5GeyZBWQ1gnOAH0TEM2n41WS/uqdGxFiyZpPKk83VPE120PQ6omL8t4EOYHJEtJA1rfQud3eXgz1FdpCWOgJYXUNctXoKmKzy9v4d64iIRRFxCnAI2XmXW9LwFyLioxFxNFlN5a8kvbFy4alt+q/JalDjI2Ic0EPaBhHxq4g4Ky3/C8C/K2u7351N6f/+JcMm1vSOsw/1gaVtzSUGtE8kiWz/78k+qdy/RwBbyZoSa1V57D1VOUGO+6B0W60C/iwixpX87RcRPyP7jOy4Oitts9KrtTZR+378p7Te6elzeja7fk773IeSJgD/AVwcEQ9UxP+5ivj3T7Xx3S63aE4Ge2YB8CayauENJcMPIGvXfFHSa8jamGtxC3CepGmS9gf+tmL8AWS/QjdLmk1WNe21DthO1mZcze3AMZLek06E/SkwjawZZ49IGlP6R9a+vQm4VNI+yi6tfAdwk6R9lV2v3pKq0huBbWk5b5f0qvTB7h2+rcoqDyD7glsHjJL0aWBsSTxnS5qQfmE/nwZXW06Z9ItuNXC2pJHKTtz/Ti3bICKeJjtRfJWk8el9/2Ea/QxwkKSWPma/BXibpDcqu9z1o2TNCT+rZd0VbgT+UtJRkl5JVhu9OQZ2ZdvH03uYTNYWf3OVaXLZBxWuAS6T9Lq0zBZJp6dx3weOlXRqagL7EOVf+EuAP5R0RNrul/WzngOAF8kuJJgEfLzWANO6byW7erByO30VuCjV5CXpFcou/jig1uUXyclgD6TzAT8jO/HVUTLqY2Rf1C+QHRjVPlTVlvffZG21PyJrNvlRxSR/DvydpBeAT5N+Wad5fwN8DliYqqal7ZNExHNkVzt9FHiO7GTl2yPi2Vpiq2IS8NuKv8nAXLI29GeBq4B5EfFImucc4IlUJb+I7JcYwFTgf8k+mPcAV0XEnVXWeQfZF+8vyZoxNlNe3W4Hliu7euZy4MyI2Fzj+/kA2ZfBc8DrGNgX8jlkbeWPkJ04/AhAet83AivTPilrdomIR8m2wf8j217vAN4RES8NYN29rgO+QXYS83GybfMXA1zGd4Eusi/U7wNfrzJNnvsAgIj4D7JaxU3pWPkF2TFFOl5PB75Itq+mAYvJkigR8T9kn7dl6b3092Pns2Qne3vS+71tAGG2kl3o8BHtvKLoRUlHRMRisuPpSrKLNFaQnYQfEnzTmZkNOalJsht4b0T8uOh4hgPXDMxsSJD0FknjJI1m5/m4wbxyrKk5GZhZLtK5oher/O3pHdEnkN0T09u0dmojXqI5VLmZyMzMXDMwM7Ps5pQh5+CDD44pU6YUHYaZ2ZDS1dX1bERMqDZuSCaDKVOmsHjx4qLDMDMbUiRVPo1gBzcTmZmZk4GZmTkZmJkZTgZmZoaTgZk1qJ6eHi6//HI2btxYdChNwcnAzBpSZ2cnK1eupLOzs+hQmoKTgZk1nJ6eHu677z4ignvvvde1gzpwMjCzhtPZ2cn27VnHedu3b3ftoA6cDMys4XR1dbFtW9Y3zrZt23yTaR04GZhZw5k5cyYjR44EYOTIkbS1tRUc0fDnZGBmDae9vZ0RI7KvpxEjRtDe3l5wRMOfk4GZNZyWlhZmz56NJObMmcPYsWN3P5PtlSH5oDozG/7a29tZs2aNawV14mRgZg2ppaWFSy65pOgwmoabicysIfkO5PpyMjCzhuQ7kOvLycDMGo7vQK6/XJOBpOskrZX0iz7GS9IVklZIWiZpRp7xmNnQ4DuQ6y/vmsF8oL9LAU4Gpqa/C4Grc47HKnR3d3PppZeyevXqokMx28F3INdfrskgIu4C1vczySnAgsj8HBgn6bA8Y7JyCxYsYPPmzdxwww1Fh2K2g+9Arr+izxlMAlaVlLvTMKuD7u5u1qxZA8CaNWtcO7CG4TuQ66/oZKAqw6LqhNKFkhZLWrxu3bqcw2oOCxYsKCu7dmCNwncg11/RyaAbmFxSbgWeqjZhRFwbEW0R0TZhwoS6BDfc9dYK+iqbFenEE09k9OjRnHjiiUWH0hSKTgYdwLx0VdHxQE9EPF1wTE1j4sSJ/ZbNirRw4UK2bNnCwoULiw6lKeR9aemNwD3AqyV1S3qfpIskXZQmuR1YCawAvgr8eZ7xWLl58+aVlc8999yCIjEr5/sM6i/XZxNFxFm7GR/Ah/KMwfrW2trKxIkTWbNmDRMnTmTSJJ+7t8ZQ7T6DM844o+Cohreim4msYO9617uQxGmnnVZ0KGY7+D6D+nMyaHJLly4t+2/WCHyfQf05GTQxt8tao/J9BvXnZNDE/PwXa1S+z6D+nAyamNtlrZG1t7dz9NFHu1ZQJ04GTcztstbIens6c62gPpwMmpjbZc2sl5NBE3O7rJn1cjJocm6XtUblvjbqy8mgybld1hqV+9qoLycDM2s47muj/pwMzKzhuK+N+nMyMLOG47426s/JwMwajvvaqD8nAzNrOO5ro/6cDMys4bS2ttLbve0hhxzivjbqwMnAzBpSbzJwn+f14WRgZg2np6eHRx55BICHH37Yj1evAycDM2s4HR0dZY9X7+joKDii4c/JoMn5ln9rRPfff39Zuaurq6BImoeTQZO7/vrr2bx5M9dff33RoZhZgZwMmlh3dzfr1q0DYO3ata4dWMOYMWNGWXnmzJkFRdI8nAyaWGVtwLUDaxR//Md/3G/ZBp+TQRPrrRX0Wrt2bUGRmJVbuHBhv2UbfE4GZtZwKvvjXrRoUUGRNA8ngyb2+te/vqx83HHHFROIWYXx48f3W7bB52TQxN797nf3WzYryoYNG/ot2+BzMmhiLS0tHHjggQAcdNBB7u3MGkZbW1tZedasWQVF0jycDJpYT0/Pjtv8S1+bFa29vR1JAEhyH9114GTQxDo7O4kIACKCzs7OgiMys6I4GTSxrq4utm3bBsC2bdt2uYLDrCj+oVJ/TgZNbObMmYwcORKAkSNH7tJOa1YUX1paf04GTay9vb3s15fbZa1R+NLS+nMyaHKlycCsUfjS0vpzMmhinZ2dZVdsuF3WGoUvLa0/J4Mm1tXVVdaBiE8gW6Nob29n1KhRAIwaNcpNmHXgZNDEjj322LLy9OnTC4rErFxLSwtz5sxBEscff7xviKwDJwMza0gnnngio0eP5sQTTyw6lKaQezKQ1C7pUUkrJH2iyvjxkv5D0jJJ90n63bxjssyDDz5YVl62bFlBkZjtauHChWzZssWPr66TXJOBpJHAV4CTgWnAWZKmVUz2N8CSiJgOzAMuzzMm22nmzJmMGJEdAiNGjPB9BtYwenp6uO+++4gI7r33Xj8qpQ7yrhnMBlZExMqIeAm4CTilYpppwA8BIuIRYIqkQ3OOy2CXk3I+SWeNorOzs+ziBl/plr+8k8EkYFVJuTsNK7UUeBeApNnAkUBrznFZ4vsMrBH5USn1l3cyUJVhld86nwfGS1oC/AXwALB1lwVJF0paLGlxZXeNtmd8n4E1Kj8qpf7yTgbdwOSScivwVOkEEbExIs6PiOPIzhlMAB6vXFBEXBsRbRHRNmHChBxDbh6+z8AaVXt7e9n5LDdh5i/vZLAImCrpKEn7AmcCHaUTSBqXxgG8H7grIny2qA7868saVUtLC7Nnz0YSc+bM8X0GdZBrMoiIrcDFwB3Aw8AtEbFc0kWSLkqTvRZYLukRsquOLskzJtupvb29rGbgX1/WSNrb2zn66KN9XNbJqLxXEBG3A7dXDLum5PU9wNS847DqfALZGlVLSwuXXOLfhvXiO5CbWEdHR79lM2seTgZN7P777y8rd3V1FRSJmRXNycDMzJwMmtmMGTPKyjNnziwoEjMrmpNBE5s7d27ZTWdz584tOCIzK4qTQRNraWnZcW/BrFmzfC23WRPL/dJSa2xz585l/fr1rhWYNTkngybna7nNDNxMZGZmOBmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGTUmA0mnSzogvf6kpNskzdjdfGZmNjTUWjP4VES8IOn3gbcANwBX5xeWmZnVU63JYFv6/zbg6oj4LrBvPiGZmVm91ZoMVkv6N+AM4HZJowcwr5mZNbhav9DPAO4A2iPieeBA4ON5BWVmZvVVa38GhwHfj4gtkk4CpgML8grKzMzqq9aawa3ANkmvAr4OHAV8O7eozMysrmpNBtsjYivwLuBfI+IvyWoLZmY2DNSaDF6WdBYwD/heGrZPPiGZmVm91ZoMzgdOAD4XEY9LOgr4Zn5hmZlZPdWUDCLiIeBjwIOSfhfojojP5xqZmZnVTU1XE6UriG4AngAETJZ0bkTclVtkZmZWN7VeWvovwJsj4lEASccANwIz8wrMzMzqp9ZzBvv0JgKAiPglPoFsZjZs1FozWCzp68A3Uvm9QFc+IZmZWb3Vmgw+CHwI+DDZOYO7gKvyCsrMzOqrpmQQEVuAL6c/MzMbZvpNBpIeBKKv8RExfdAjMjOzuttdzeDtdYnCzMwK1W8yiIgna1mIpHsi4oTBCcnMzOptsDqoGdPXCEntkh6VtELSJ6qMb5H0X5KWSlou6fxBisnMzGo0WMmg6nkFSSOBrwAnA9OAsyRNq5jsQ8BDEfF64CTgXyS5S00zszrKu+vK2cCKiFgZES8BNwGnVEwTwAGSBLwSWA9szTkuMzMrMVjJQH0MnwSsKil3p2GlrgReCzwFPAhcEhHbBykuMzOrwWAlg3P6GF4tSVQ2Kb0FWAIcDhwHXClp7C4Lki6UtFjS4nXr1u1FqGZmVqnfZCDpBUkbq/y9IGlj73QR8Ys+FtENTC4pt5LVAEqdD9wWmRXA48BrKhcUEddGRFtEtE2YMKGW92ZmZjXa3aWlB+zl8hcBU1NnOKuBM4H3VEzza+CNwE8lHQq8Gli5l+s1M7MBqPXZRABIOoSSy0gj4tf9TR8RWyVdDNwBjASui4jlki5K468B/h6Yn+52FvDXEfHswN6GmZntjVo7t5lL1qfB4cBa4EjgYeB1u5s3Im4Hbq8Ydk3J66eAN9cespmZDbZaTyD/PXA88MuIOIqsWWdhblGZmVld1ZoMXo6I54ARkkZExI/JrvwxM7NhoNZzBs9LeiXwU+BbktbiG8PMzIaNWmsGdwHjgEuATuAx4B05xWRmZnVWazIQ2RVBd5I9MuLm1GxkZmbDQE3JICI+GxGvI3uo3OHATyT9b66RmZlZ3Qz0cRRrgTXAc8Ahgx+OmZkVoaZkIOmDku4EfggcDHzAXV6amQ0ftV5NdCTwkYhYkmMsZmZWkJqSQUTs0kOZmZkNH3l3bmNmZkOAk4GZmTkZmJmZk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmOBmYmRlOBmZmRo19INvgu/XWW1m9enXRYbBu3ToAJkyYUGgckyZN4rTTTis0BtupEY7PRjk2oTmOTyeDJrdly5aiQzCrysdmfSkiio5hwNra2mLx4sVFhzEsXHHFFQB8+MMfLjgSs3I+NgefpK6IaKs2zucMzMzMycDMzJwMzMwMJwMzM8PJwMzMcDIwMzPqkAwktUt6VNIKSZ+oMv7jkpakv19I2ibpwLzjMjOznXJNBpJGAl8BTgamAWdJmlY6TUR8KSKOi4jjgMuAn0TE+jzjMjOzcnnfgTwbWBERKwEk3QScAjzUx/RnATfmHFND3GrfKLq7u4GdN/g0u6IfO+Bjcycfm+XyPjbzTgaTgFUl5W5gTrUJJe0PtAMX5xwTq1evZtXKxzh0Xz+NY5+XtwHwUveTBUdSvGde2lp0CD42S/jY3Kkex2beR5yqDOvr+RfvABb21UQk6ULgQoAjjjhir4Jat25d31E0mfH7jCw6hMYROx+OVhQfmzv52CxRh2Mz7xPI3cDkknIr8FQf055JP01EEXFtRLRFRFsjPMXQzGw4ybtmsAiYKukoYDXZF/57KieS1AL8EXB2zvEA2SNxX9ryG+YdNr4eq7MhYsHTG9i34B8aPjatmnocm7kmg4jYKuli4A5gJHBdRCyXdFEaf02a9J3ADyJiU57xmJlZdbmfpYqI24HbK4ZdU1GeD8zPOxYzM6vOdyCbmZmTgZmZNXG3l8+8tJUFT28oOozCbUjXcvsyvuyYmLz7yXLnYzPjY3OnehybTZkMJk2aVHQIDePldJfnvq2tBUdSvMkUf2wUvf5G4mNzp3ocm+4Ducm5n1lrVD42B5/7QDYzs345GZiZmZOBmZk5GZiZGU4GZmaGk4GZmeFkYGZmNOlNZ2bWv0bofrORur0sujvUenAyKEgjfNigcT5wzfBhs4EZPXp00SE0FSeDJieJ3/72t2zatIlXvOIVRYdjDaIREnNPTw/z58/nvPPOY+zYsUWHM+w5GRSkET5sAJdeeikAGzZs4LLLLis4GrOdOjo6eOyxx+jo6ODss+vSCWJT8wnkJvbII4+wefNmADZv3syjjz5acERmmZ6eHnqfP7Zo0SI2btxYcETDn5NBE7vuuuvKyl//+tcLisSsXEdHB70P0YwIOjo6Co5o+HMyaGK9tYK+ymZF6erqKiv7KcX5czIws4azffv2fss2+JwMmtjUqVPLysccc0xBkZhZ0ZwMmti4ceP6LZsVZfz48f2WbfA5GTSxBx98sKy8bNmygiIxKyeprDxihL+q8uYt3MSOPfbYsvL06dMLisSs3Pr168vKzz33XEGRNA8nAzNrOPvtt1+/ZRt8TgZNrLJZaOnSpQVFYlZu69at/ZZt8DkZNLHK5720tLQUFIlZucqLGXwCOX9OBk3s2WefLSuvW7euoEjMylWeI6g8Vm3wORk0sd7b/fsqmxWl8mqiyrINPicDM2s4M2bMKCvPnDmzoEiah5OBmTWcuXPn7qgNSGLu3LkFRzT8ORmYWcNpaWmhra0NgFmzZrlzmzpw5zZNbMSIEWUPAPNdntZI5s6dy/r1610rqBN/+ptYZTts7y8xs0bQ0tLCJZdc4lpBnTgZNLFZs2b1WzYrUk9PD5dffrl7OasTJ4Mmdtttt5WVb7311oIiMdvVrbfeymOPPebjsk6cDJrYmjVr+i2bFaWnp4clS5YA8MADD7h2UAdOBk1s4sSJ/ZbNilJZG3DtIH+5JwNJ7ZIelbRC0if6mOYkSUskLZf0k7xjssy8efPKyueee25BkZiVq3xoYm8twfKTazKQNBL4CnAyMA04S9K0imnGAVcBcyPidcDpecZkO7W2tu6oDUycOJFJkyYVHJFZxo9Kqb+8awazgRURsTIiXgJuAk6pmOY9wG0R8WuAiFibc0xWYt68eYwZM8a1AmsoEyZM6Ldsgy/vZDAJWFVS7k7DSh0DjJd0p6QuSfOwumltbeWLX/yiawXWUM4///yy8gUXXFBQJM0j72RQ7VGDlfW9UcBM4G3AW4BPSTpmlwVJF0paLGmxH7VsNry1trbuqA1MmDDBP1bqIO9k0A1MLim3Ak9VmaYzIjZFxLPAXcDrKxcUEddGRFtEtLnKaDb8nX/++YwZM8a1gjrJOxksAqZKOkrSvsCZQEfFNN8F/kDSKEn7A3OAh3OOy8wanJsw6yvXB9VFxFZJFwN3ACOB6yJiuaSL0vhrIuJhSZ3AMmA78LWI+EWecZmZWTkNxUu22traYvHixUWHYWY2pEjqioiqT6T0HchmZuZkYGZmQ7SZSNI64Mmi4xhGDgaeLToIsyp8bA6uIyOi6uWYQzIZ2OCStLivdkSzIvnYrB83E5mZmZOBmZk5GVjm2qIDMOuDj8068TkDMzNzzcDMzJwMzMwMJ4NhRdJ1ktZKGvCznSTNlPRg6p70CklKw8+TtC51S7pE0vsHP3IbrnbX7a0yV6TxyyTN2N28kg6U9D+SfpX+j0/DD5L0Y0kvSrqyPu9w+HAyGF7mA+17OO/VwIXA1PRXupybI+K49Pe1vQvRmkUt3d6mcb3H3IVkx+Hu5v0E8MOImAr8MJUBNgOfAj6W13sazpwMhpGIuAtYXzpM0u9I6ky9yP1U0msq55N0GDA2Iu6J7IqCBcCpdQnahrNaur09BVgQmZ8D49Lx2N+8pwA3pNc3kI7V1CfK3WRJwQbIyWD4uxb4i4iYSfaL6aoq00wi62SoV2X3pKelKvy/S5qMWW1q6fa2r2n6m/fQiHgaIP0/ZBBjblq59mdgxZL0SuANwHfSKQCA0dUmrTKs95rj/wJujIgtqR+KG4A/GexYbViqpdvbvqapZV4bRE4Gw9sI4PmIOK50YGqP7UrFDrJ22taSSXZ0TxoRz5UM/yrwhbyCtWGn1m5vq02zbz/zPiPpsIh4OjUprR3UqJuUm4mGsYjYCDwu6XTYceXG6yNiW8kJ4U+nqvYLko5PVxHNI+uOtPd8Qq+5uEtSq10t3d52APPSsXk80JOOx/7m7QDOTa/PJR2rtndcMxhGJN0InAQcLKkb+FvgvcDVkj4J7EN2Im5pldk/SHY10n7Af6c/gA9LmgtsJTs5fV5+78CGk1q6vQVuB94KrAB+A5zf37xp0Z8HbpH0PuDXwOm965T0BDAW2FfSqcCbI+KhvN/rcODHUZiZmZuJzMzMycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMrAhStKUao/qlvS1Kk/GbDiS7pTUNsB5/k7Sm/KKyZqbbzqzYSUiculvQdKoiNiax7JrXP/IiPj0ICyn0Pdhjcs1AxvKRkm6oeSJqvuX/uKWdLWkxZKWS/ps70ySPi/poTTfP/e1cEnzJX1Z0o+BL/T1OPA0/OeSFqVf7y+m4SdJ+l7J8q6UdF6V9fQV5xOSPi3pbuD0FM+7JbVpZ2dDD0qKkjiqxVf2PvZuk9tw5ZqBDWWvBt4XEQslXQf8ecX4/xsR69OD+X4oaTrZg9HeCbwmIkLSuN2s4xjgTRGxTdIPgYsi4leS5pA9DvxPgMuByyPixt5HLQzQLnFGxLI0bnNE/D5kPX8BRMRi4Lg07EtAZ5r22j7iK3sfexCfNQEnAxvKVkXEwvT6m8CHK8afIelCsuP8MLIesx4i6/zka5K+D3yP/n0nJYL+Hgd+Ajs7A/o20Gdtow/V4uxNBjf3NZOkM4AZwJtreFz5d5wIrD9OBjaUVT5Ya0dZ0lFknfnMiogNkuYDY9ID0GYDbyR7EubF9N8/w6b0v+rjwHdjK+VNsWMqJ+grzirrr5zvdcBngT9MyWp38VVdjlkvnzOwoewISSek12cBd5eMG0v2Bdgj6VCyvnR7O/xpiYjbgY+Qmlt2p6/HgafRPwdOS6/PLJntSWCapNGSWsgSUKWqcfYnLesmYF5ErKshPrPdcjKwoexh4FxJy4ADSZ2pA0TEUuABYDlwHdDbnHQA8L00z0+AvxzA+t4LvE/S0rTc3j55PwL8laT7yJp5elIMq4BbyJp8vpXiKdNPnP05FTgS+GrvieTdxGe2W36EtdlekrQ/8Nt0QvpM4KyI8BexDSk+Z2C292YCVyo7c/s8cEGx4ZgNnGsG1vQk/V9KestKvhMRnysiHrMiOBmYmZlPIJuZmZOBmZnhZGBmZjgZmJkZ8P8BSWvA6oR+eMwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'bias_regularizer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b3d8966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.7)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'bias_regularizer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n",
    "plt.ylim(0.6,0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f81776",
   "metadata": {},
   "source": [
    "## 5.5 activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89c0a22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of activity_regularizer')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activity_regularizer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25ba930f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.7)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activity_regularizer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n",
    "plt.ylim(0.6, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d0c4529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activation_layer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n",
    "plt.ylim(0.6, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35752c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.7)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batc_normalization'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n",
    "plt.ylim(0.6, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152af61",
   "metadata": {},
   "source": [
    "## Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6944cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3],   #Done\n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001],#Done (?)\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.00001,0.000001,0.0000001],\n",
    "    'kernel_regularizer_l2':[0.00001,0.000001,0.0000001],\n",
    "    'bias_regularizer':[0.0001],#Done\n",
    "    'activity_regularizer':[0.0001],#Done\n",
    "\n",
    "    #'dropout': [0,0.1,0.2,0.3,0.4],\n",
    "    'dropout': [0],\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['uniform'],#Done\n",
    "\n",
    "    'activation_layer':['tanh','selu'],\n",
    " \n",
    "    'batc_normalization':[False,True],\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed035de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/36 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CE12678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2351AD3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|██▎                                                                                | 1/36 [00:03<02:18,  3.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FD89558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C73C318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|████▌                                                                              | 2/36 [00:07<02:15,  3.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F24271D318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2351AD708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▉                                                                            | 3/36 [00:11<02:09,  3.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2351AD798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F227266C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█████████▏                                                                         | 4/36 [00:15<02:01,  3.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21F436438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2351AD4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|███████████▌                                                                       | 5/36 [00:19<02:00,  3.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CE125E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2351AD168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█████████████▊                                                                     | 6/36 [00:23<01:56,  3.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2382C0D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F24271D798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|████████████████▏                                                                  | 7/36 [00:27<01:52,  3.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F24271D828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CE128B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██████████████████▍                                                                | 8/36 [00:30<01:46,  3.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F227266E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CE12EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|████████████████████▊                                                              | 9/36 [00:34<01:45,  3.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FBCE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F9B9CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 63.\n",
      "Epoch 00113: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██████████████████████▊                                                           | 10/36 [00:42<02:13,  5.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226DAE9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23F906828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "Epoch 00147: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|█████████████████████████                                                         | 11/36 [00:52<02:40,  6.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2351AD168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C65D5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 59.\n",
      "Epoch 00109: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███████████████████████████▎                                                      | 12/36 [00:59<02:42,  6.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E395C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C65DD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 53.\n",
      "Epoch 00103: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|█████████████████████████████▌                                                    | 13/36 [01:07<02:40,  6.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CD5E1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C65D5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 58.\n",
      "Epoch 00108: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███████████████████████████████▉                                                  | 14/36 [01:14<02:36,  7.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F227266558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FD89288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 96.\n",
      "Epoch 00146: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|██████████████████████████████████▏                                               | 15/36 [01:23<02:41,  7.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FB5C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C65DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 63.\n",
      "Epoch 00113: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████████████████████████████████████▍                                             | 16/36 [01:31<02:34,  7.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239698948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C65D048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "Epoch 00101: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|██████████████████████████████████████▋                                           | 17/36 [01:38<02:25,  7.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21F436288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22B12A5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 89.\n",
      "Epoch 00139: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████████████████████████████████████████                                         | 18/36 [01:47<02:24,  8.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F227266558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226DD8288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|███████████████████████████████████████████▎                                      | 19/36 [01:51<01:54,  6.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CE12798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F227266F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████████████████████████████████████████████▌                                    | 20/36 [01:55<01:33,  5.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F24271D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2399C0F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|███████████████████████████████████████████████▊                                  | 21/36 [01:59<01:18,  5.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E395798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21F436168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|██████████████████████████████████████████████████                                | 22/36 [02:03<01:08,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC81168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2351ADD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|████████████████████████████████████████████████████▍                             | 23/36 [02:07<00:59,  4.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2399C0CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226DD8CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████████████████████████████▋                           | 24/36 [02:11<00:52,  4.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23C65DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21F436288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|████████████████████████████████████████████████████████▉                         | 25/36 [02:14<00:46,  4.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23C65D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23C73C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████████████████████████████████████████████████████████▏                      | 26/36 [02:18<00:41,  4.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E395708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FD89168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 27/36 [02:22<00:36,  4.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2351ADA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DE97F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "Epoch 00125: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████████████████████████████████████████████████████████████▊                  | 28/36 [02:31<00:43,  5.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FD89948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2271D3CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 120.\n",
      "Epoch 00170: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|██████████████████████████████████████████████████████████████████                | 29/36 [02:41<00:48,  6.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-05, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226DD88B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C8FED38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "Epoch 00118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 30/36 [02:49<00:43,  7.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC81B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219AE34C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 00119: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|██████████████████████████████████████████████████████████████████████▌           | 31/36 [02:58<00:38,  7.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23C65D828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2283ED318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Epoch 00090: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%|████████████████████████████████████████████████████████████████████████▉         | 32/36 [03:04<00:29,  7.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2412ACAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2283ED708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Epoch 00100: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 33/36 [03:12<00:22,  7.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C816168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2382C0288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 62.\n",
      "Epoch 00112: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▍    | 34/36 [03:20<00:14,  7.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-06, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239698318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADDBEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 89.\n",
      "Epoch 00139: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 35/36 [03:29<00:08,  8.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'selu', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-07, 'kernel_regularizer_l2': 1e-07, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2396981F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CC54288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 78.\n",
      "Epoch 00128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [03:39<00:00,  6.09s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dee873e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/051022213813.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef26126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "355f82cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>119</td>\n",
       "      <td>0.455790</td>\n",
       "      <td>[0.7834483  0.5791045  0.74358976]</td>\n",
       "      <td>0.602989</td>\n",
       "      <td>[0.7363184  0.3448276  0.55284554]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>139</td>\n",
       "      <td>0.431280</td>\n",
       "      <td>[0.7478754 0.601227  0.75     ]</td>\n",
       "      <td>0.606752</td>\n",
       "      <td>[0.70454544 0.26190475 0.5409836 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>118</td>\n",
       "      <td>0.459298</td>\n",
       "      <td>[0.78      0.589404  0.7908746]</td>\n",
       "      <td>0.607775</td>\n",
       "      <td>[0.7032967 0.21875   0.6323529]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>146</td>\n",
       "      <td>0.435728</td>\n",
       "      <td>[0.7711864  0.58507466 0.72989684]</td>\n",
       "      <td>0.612712</td>\n",
       "      <td>[0.70157063 0.2647059  0.5203252 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>139</td>\n",
       "      <td>0.443510</td>\n",
       "      <td>[0.7705382  0.59756094 0.7611336 ]</td>\n",
       "      <td>0.617971</td>\n",
       "      <td>[0.6931818  0.32500002 0.5873016 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>113</td>\n",
       "      <td>0.466099</td>\n",
       "      <td>[0.75455827 0.55727553 0.7276423 ]</td>\n",
       "      <td>0.623791</td>\n",
       "      <td>[0.6961326  0.32500002 0.5123967 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>125</td>\n",
       "      <td>0.436483</td>\n",
       "      <td>[0.78055555 0.6149068  0.7613169 ]</td>\n",
       "      <td>0.626052</td>\n",
       "      <td>[0.74489796 0.28571427 0.5203252 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>147</td>\n",
       "      <td>0.398572</td>\n",
       "      <td>[0.8286517  0.6111111  0.80894303]</td>\n",
       "      <td>0.627108</td>\n",
       "      <td>[0.6984127  0.30769232 0.56250006]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>108</td>\n",
       "      <td>0.487426</td>\n",
       "      <td>[0.7219796  0.56811595 0.72177416]</td>\n",
       "      <td>0.632810</td>\n",
       "      <td>[0.6590909  0.27499998 0.53968257]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>103</td>\n",
       "      <td>0.475075</td>\n",
       "      <td>[0.7474748  0.57988167 0.7444668 ]</td>\n",
       "      <td>0.635259</td>\n",
       "      <td>[0.69148934 0.25641027 0.4827586 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>170</td>\n",
       "      <td>0.410904</td>\n",
       "      <td>[0.7681365  0.613924   0.77013755]</td>\n",
       "      <td>0.636844</td>\n",
       "      <td>[0.6813187  0.3376623  0.55284554]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>128</td>\n",
       "      <td>0.437650</td>\n",
       "      <td>[0.76420456 0.5939394  0.74493927]</td>\n",
       "      <td>0.639437</td>\n",
       "      <td>[0.65142864 0.24390246 0.55999994]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>113</td>\n",
       "      <td>0.467130</td>\n",
       "      <td>[0.75714284 0.60493827 0.7380952 ]</td>\n",
       "      <td>0.641384</td>\n",
       "      <td>[0.65536726 0.2682927  0.55284554]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>90</td>\n",
       "      <td>0.501518</td>\n",
       "      <td>[0.7267684  0.56811595 0.6753246 ]</td>\n",
       "      <td>0.644140</td>\n",
       "      <td>[0.65555555 0.28571427 0.50450456]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100</td>\n",
       "      <td>0.496934</td>\n",
       "      <td>[0.744186   0.57228917 0.6580645 ]</td>\n",
       "      <td>0.645103</td>\n",
       "      <td>[0.6242775 0.2197802 0.4915254]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>109</td>\n",
       "      <td>0.483612</td>\n",
       "      <td>[0.74709296 0.57817113 0.72654694]</td>\n",
       "      <td>0.647432</td>\n",
       "      <td>[0.6703297  0.3255814  0.49122807]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>101</td>\n",
       "      <td>0.503096</td>\n",
       "      <td>[0.68914956 0.5240964  0.7237354 ]</td>\n",
       "      <td>0.651741</td>\n",
       "      <td>[0.71874994 0.16666667 0.46153846]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>112</td>\n",
       "      <td>0.466060</td>\n",
       "      <td>[0.74366194 0.59375    0.7148594 ]</td>\n",
       "      <td>0.658236</td>\n",
       "      <td>[0.67428577 0.21212122 0.4964539 ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687245</td>\n",
       "      <td>[0.         0.24165708 0.        ]</td>\n",
       "      <td>0.687268</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687319</td>\n",
       "      <td>[0.         0.24165708 0.        ]</td>\n",
       "      <td>0.687327</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687324</td>\n",
       "      <td>[0.         0.24165708 0.        ]</td>\n",
       "      <td>0.687396</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687524</td>\n",
       "      <td>[0.6838932 0.        0.       ]</td>\n",
       "      <td>0.687431</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687452</td>\n",
       "      <td>[0.         0.2378753  0.00754717]</td>\n",
       "      <td>0.687471</td>\n",
       "      <td>[0.         0.24884795 0.03030303]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687532</td>\n",
       "      <td>[0.         0.24214202 0.04411765]</td>\n",
       "      <td>0.687515</td>\n",
       "      <td>[0.         0.25714284 0.08219179]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687632</td>\n",
       "      <td>[0.68363637 0.16149068 0.00749064]</td>\n",
       "      <td>0.687573</td>\n",
       "      <td>[0.6956522  0.19512196 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687726</td>\n",
       "      <td>[0.6838932 0.        0.       ]</td>\n",
       "      <td>0.687646</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687641</td>\n",
       "      <td>[0.         0.23214287 0.20749278]</td>\n",
       "      <td>0.687686</td>\n",
       "      <td>[0.         0.20408164 0.13793103]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687720</td>\n",
       "      <td>[0.         0.24221453 0.        ]</td>\n",
       "      <td>0.687713</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687728</td>\n",
       "      <td>[0.         0.2436195  0.02985075]</td>\n",
       "      <td>0.687724</td>\n",
       "      <td>[0.         0.23255813 0.02941176]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687713</td>\n",
       "      <td>[0.         0.23693381 0.03703704]</td>\n",
       "      <td>0.687730</td>\n",
       "      <td>[0.         0.23255813 0.02941176]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51</td>\n",
       "      <td>0.690604</td>\n",
       "      <td>[0.6838932 0.        0.       ]</td>\n",
       "      <td>0.690542</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51</td>\n",
       "      <td>0.690619</td>\n",
       "      <td>[0.        0.        0.5019608]</td>\n",
       "      <td>0.690562</td>\n",
       "      <td>[0.        0.        0.5078125]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>0.690802</td>\n",
       "      <td>[0.00992556 0.24333721 0.        ]</td>\n",
       "      <td>0.690806</td>\n",
       "      <td>[0.         0.23148148 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>0.690791</td>\n",
       "      <td>[0.         0.24165708 0.        ]</td>\n",
       "      <td>0.690816</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>0.690867</td>\n",
       "      <td>[0.         0.19815668 0.36441892]</td>\n",
       "      <td>0.690852</td>\n",
       "      <td>[0.         0.2295082  0.42342344]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>51</td>\n",
       "      <td>0.690878</td>\n",
       "      <td>[0.13913043 0.23880596 0.01515152]</td>\n",
       "      <td>0.690853</td>\n",
       "      <td>[0.18333334 0.22335027 0.        ]</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "30           119  0.455790  [0.7834483  0.5791045  0.74358976]  0.602989   \n",
       "34           139  0.431280     [0.7478754 0.601227  0.75     ]  0.606752   \n",
       "29           118  0.459298     [0.78      0.589404  0.7908746]  0.607775   \n",
       "14           146  0.435728  [0.7711864  0.58507466 0.72989684]  0.612712   \n",
       "17           139  0.443510  [0.7705382  0.59756094 0.7611336 ]  0.617971   \n",
       "15           113  0.466099  [0.75455827 0.55727553 0.7276423 ]  0.623791   \n",
       "27           125  0.436483  [0.78055555 0.6149068  0.7613169 ]  0.626052   \n",
       "10           147  0.398572  [0.8286517  0.6111111  0.80894303]  0.627108   \n",
       "13           108  0.487426  [0.7219796  0.56811595 0.72177416]  0.632810   \n",
       "12           103  0.475075  [0.7474748  0.57988167 0.7444668 ]  0.635259   \n",
       "28           170  0.410904  [0.7681365  0.613924   0.77013755]  0.636844   \n",
       "35           128  0.437650  [0.76420456 0.5939394  0.74493927]  0.639437   \n",
       "9            113  0.467130  [0.75714284 0.60493827 0.7380952 ]  0.641384   \n",
       "31            90  0.501518  [0.7267684  0.56811595 0.6753246 ]  0.644140   \n",
       "32           100  0.496934  [0.744186   0.57228917 0.6580645 ]  0.645103   \n",
       "11           109  0.483612  [0.74709296 0.57817113 0.72654694]  0.647432   \n",
       "16           101  0.503096  [0.68914956 0.5240964  0.7237354 ]  0.651741   \n",
       "33           112  0.466060  [0.74366194 0.59375    0.7148594 ]  0.658236   \n",
       "7             51  0.687245  [0.         0.24165708 0.        ]  0.687268   \n",
       "8             51  0.687319  [0.         0.24165708 0.        ]  0.687327   \n",
       "26            51  0.687324  [0.         0.24165708 0.        ]  0.687396   \n",
       "24            51  0.687524     [0.6838932 0.        0.       ]  0.687431   \n",
       "6             51  0.687452  [0.         0.2378753  0.00754717]  0.687471   \n",
       "25            51  0.687532  [0.         0.24214202 0.04411765]  0.687515   \n",
       "5             51  0.687632  [0.68363637 0.16149068 0.00749064]  0.687573   \n",
       "21            51  0.687726     [0.6838932 0.        0.       ]  0.687646   \n",
       "4             51  0.687641  [0.         0.23214287 0.20749278]  0.687686   \n",
       "3             51  0.687720  [0.         0.24221453 0.        ]  0.687713   \n",
       "22            51  0.687728  [0.         0.2436195  0.02985075]  0.687724   \n",
       "23            51  0.687713  [0.         0.23693381 0.03703704]  0.687730   \n",
       "20            51  0.690604     [0.6838932 0.        0.       ]  0.690542   \n",
       "19            51  0.690619     [0.        0.        0.5019608]  0.690562   \n",
       "1             51  0.690802  [0.00992556 0.24333721 0.        ]  0.690806   \n",
       "2             51  0.690791  [0.         0.24165708 0.        ]  0.690816   \n",
       "0             51  0.690867  [0.         0.19815668 0.36441892]  0.690852   \n",
       "18            51  0.690878  [0.13913043 0.23880596 0.01515152]  0.690853   \n",
       "\n",
       "                       val_fbeta_score activation_layer  activity_regularizer  \\\n",
       "30  [0.7363184  0.3448276  0.55284554]             selu                0.0001   \n",
       "34  [0.70454544 0.26190475 0.5409836 ]             selu                0.0001   \n",
       "29     [0.7032967 0.21875   0.6323529]             selu                0.0001   \n",
       "14  [0.70157063 0.2647059  0.5203252 ]             tanh                0.0001   \n",
       "17  [0.6931818  0.32500002 0.5873016 ]             tanh                0.0001   \n",
       "15  [0.6961326  0.32500002 0.5123967 ]             tanh                0.0001   \n",
       "27  [0.74489796 0.28571427 0.5203252 ]             selu                0.0001   \n",
       "10  [0.6984127  0.30769232 0.56250006]             tanh                0.0001   \n",
       "13  [0.6590909  0.27499998 0.53968257]             tanh                0.0001   \n",
       "12  [0.69148934 0.25641027 0.4827586 ]             tanh                0.0001   \n",
       "28  [0.6813187  0.3376623  0.55284554]             selu                0.0001   \n",
       "35  [0.65142864 0.24390246 0.55999994]             selu                0.0001   \n",
       "9   [0.65536726 0.2682927  0.55284554]             tanh                0.0001   \n",
       "31  [0.65555555 0.28571427 0.50450456]             selu                0.0001   \n",
       "32     [0.6242775 0.2197802 0.4915254]             selu                0.0001   \n",
       "11  [0.6703297  0.3255814  0.49122807]             tanh                0.0001   \n",
       "16  [0.71874994 0.16666667 0.46153846]             tanh                0.0001   \n",
       "33  [0.67428577 0.21212122 0.4964539 ]             selu                0.0001   \n",
       "7   [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "8   [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "26  [0.         0.24770641 0.        ]             selu                0.0001   \n",
       "24     [0.6827586 0.        0.       ]             selu                0.0001   \n",
       "6   [0.         0.24884795 0.03030303]             tanh                0.0001   \n",
       "25  [0.         0.25714284 0.08219179]             selu                0.0001   \n",
       "5   [0.6956522  0.19512196 0.        ]             tanh                0.0001   \n",
       "21     [0.6827586 0.        0.       ]             selu                0.0001   \n",
       "4   [0.         0.20408164 0.13793103]             tanh                0.0001   \n",
       "3   [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "22  [0.         0.23255813 0.02941176]             selu                0.0001   \n",
       "23  [0.         0.23255813 0.02941176]             selu                0.0001   \n",
       "20     [0.6827586 0.        0.       ]             selu                0.0001   \n",
       "19     [0.        0.        0.5078125]             selu                0.0001   \n",
       "1   [0.         0.23148148 0.        ]             tanh                0.0001   \n",
       "2   [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "0   [0.         0.2295082  0.42342344]             tanh                0.0001   \n",
       "18  [0.18333334 0.22335027 0.        ]             selu                0.0001   \n",
       "\n",
       "    batc_normalization  batch_size  bias_regularizer  dropout  epochs  \\\n",
       "30                True          64            0.0001        0  100000   \n",
       "34                True          64            0.0001        0  100000   \n",
       "29                True          64            0.0001        0  100000   \n",
       "14                True          64            0.0001        0  100000   \n",
       "17                True          64            0.0001        0  100000   \n",
       "15                True          64            0.0001        0  100000   \n",
       "27                True          64            0.0001        0  100000   \n",
       "10                True          64            0.0001        0  100000   \n",
       "13                True          64            0.0001        0  100000   \n",
       "12                True          64            0.0001        0  100000   \n",
       "28                True          64            0.0001        0  100000   \n",
       "35                True          64            0.0001        0  100000   \n",
       "9                 True          64            0.0001        0  100000   \n",
       "31                True          64            0.0001        0  100000   \n",
       "32                True          64            0.0001        0  100000   \n",
       "11                True          64            0.0001        0  100000   \n",
       "16                True          64            0.0001        0  100000   \n",
       "33                True          64            0.0001        0  100000   \n",
       "7                False          64            0.0001        0  100000   \n",
       "8                False          64            0.0001        0  100000   \n",
       "26               False          64            0.0001        0  100000   \n",
       "24               False          64            0.0001        0  100000   \n",
       "6                False          64            0.0001        0  100000   \n",
       "25               False          64            0.0001        0  100000   \n",
       "5                False          64            0.0001        0  100000   \n",
       "21               False          64            0.0001        0  100000   \n",
       "4                False          64            0.0001        0  100000   \n",
       "3                False          64            0.0001        0  100000   \n",
       "22               False          64            0.0001        0  100000   \n",
       "23               False          64            0.0001        0  100000   \n",
       "20               False          64            0.0001        0  100000   \n",
       "19               False          64            0.0001        0  100000   \n",
       "1                False          64            0.0001        0  100000   \n",
       "2                False          64            0.0001        0  100000   \n",
       "0                False          64            0.0001        0  100000   \n",
       "18               False          64            0.0001        0  100000   \n",
       "\n",
       "    first_neuron  hidden_layers  hidden_neuron kernel_initializer  \\\n",
       "30            55              3             50            uniform   \n",
       "34            55              3             50            uniform   \n",
       "29            55              3             50            uniform   \n",
       "14            55              3             50            uniform   \n",
       "17            55              3             50            uniform   \n",
       "15            55              3             50            uniform   \n",
       "27            55              3             50            uniform   \n",
       "10            55              3             50            uniform   \n",
       "13            55              3             50            uniform   \n",
       "12            55              3             50            uniform   \n",
       "28            55              3             50            uniform   \n",
       "35            55              3             50            uniform   \n",
       "9             55              3             50            uniform   \n",
       "31            55              3             50            uniform   \n",
       "32            55              3             50            uniform   \n",
       "11            55              3             50            uniform   \n",
       "16            55              3             50            uniform   \n",
       "33            55              3             50            uniform   \n",
       "7             55              3             50            uniform   \n",
       "8             55              3             50            uniform   \n",
       "26            55              3             50            uniform   \n",
       "24            55              3             50            uniform   \n",
       "6             55              3             50            uniform   \n",
       "25            55              3             50            uniform   \n",
       "5             55              3             50            uniform   \n",
       "21            55              3             50            uniform   \n",
       "4             55              3             50            uniform   \n",
       "3             55              3             50            uniform   \n",
       "22            55              3             50            uniform   \n",
       "23            55              3             50            uniform   \n",
       "20            55              3             50            uniform   \n",
       "19            55              3             50            uniform   \n",
       "1             55              3             50            uniform   \n",
       "2             55              3             50            uniform   \n",
       "0             55              3             50            uniform   \n",
       "18            55              3             50            uniform   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2 last_activation     lr  \n",
       "30           1.000000e-06           1.000000e-05         sigmoid  0.001  \n",
       "34           1.000000e-07           1.000000e-06         sigmoid  0.001  \n",
       "29           1.000000e-05           1.000000e-07         sigmoid  0.001  \n",
       "14           1.000000e-06           1.000000e-07         sigmoid  0.001  \n",
       "17           1.000000e-07           1.000000e-07         sigmoid  0.001  \n",
       "15           1.000000e-07           1.000000e-05         sigmoid  0.001  \n",
       "27           1.000000e-05           1.000000e-05         sigmoid  0.001  \n",
       "10           1.000000e-05           1.000000e-06         sigmoid  0.001  \n",
       "13           1.000000e-06           1.000000e-06         sigmoid  0.001  \n",
       "12           1.000000e-06           1.000000e-05         sigmoid  0.001  \n",
       "28           1.000000e-05           1.000000e-06         sigmoid  0.001  \n",
       "35           1.000000e-07           1.000000e-07         sigmoid  0.001  \n",
       "9            1.000000e-05           1.000000e-05         sigmoid  0.001  \n",
       "31           1.000000e-06           1.000000e-06         sigmoid  0.001  \n",
       "32           1.000000e-06           1.000000e-07         sigmoid  0.001  \n",
       "11           1.000000e-05           1.000000e-07         sigmoid  0.001  \n",
       "16           1.000000e-07           1.000000e-06         sigmoid  0.001  \n",
       "33           1.000000e-07           1.000000e-05         sigmoid  0.001  \n",
       "7            1.000000e-07           1.000000e-06         sigmoid  0.001  \n",
       "8            1.000000e-07           1.000000e-07         sigmoid  0.001  \n",
       "26           1.000000e-07           1.000000e-07         sigmoid  0.001  \n",
       "24           1.000000e-07           1.000000e-05         sigmoid  0.001  \n",
       "6            1.000000e-07           1.000000e-05         sigmoid  0.001  \n",
       "25           1.000000e-07           1.000000e-06         sigmoid  0.001  \n",
       "5            1.000000e-06           1.000000e-07         sigmoid  0.001  \n",
       "21           1.000000e-06           1.000000e-05         sigmoid  0.001  \n",
       "4            1.000000e-06           1.000000e-06         sigmoid  0.001  \n",
       "3            1.000000e-06           1.000000e-05         sigmoid  0.001  \n",
       "22           1.000000e-06           1.000000e-06         sigmoid  0.001  \n",
       "23           1.000000e-06           1.000000e-07         sigmoid  0.001  \n",
       "20           1.000000e-05           1.000000e-07         sigmoid  0.001  \n",
       "19           1.000000e-05           1.000000e-06         sigmoid  0.001  \n",
       "1            1.000000e-05           1.000000e-06         sigmoid  0.001  \n",
       "2            1.000000e-05           1.000000e-07         sigmoid  0.001  \n",
       "0            1.000000e-05           1.000000e-05         sigmoid  0.001  \n",
       "18           1.000000e-05           1.000000e-05         sigmoid  0.001  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    'kernel_regularizer_l1':[0.000001],\n",
    "    'kernel_regularizer_l2':[0.00001,0.000001,0.0000001],\n",
    "    \n",
    "\n",
    "    'activation_layer':['tanh','selu'],\n",
    " \n",
    "    'batc_normalization':[False,True],\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "472827b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of kernel_regularizer_l1')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_regularizer_l1'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a984031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of kernel_regularizer_l2')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'kernel_regularizer_l2'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ececef6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of activation_layer')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'activation_layer'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b2c10857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batc_normalization')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batc_normalization'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55afbc",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27def762",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3,6],   #Done\n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.000001],#Done\n",
    "    'kernel_regularizer_l2':[0.00001],#Done\n",
    "    'bias_regularizer':[0.0001],#Done\n",
    "    'activity_regularizer':[0.0001],#Done\n",
    "\n",
    "    'dropout': [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['uniform'],#Done\n",
    "\n",
    "    'activation_layer':['tanh'],#Done\n",
    " \n",
    "    'batc_normalization':[True],#Done\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b135db19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E26C048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2271735E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 88.\n",
      "Epoch 00138: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|██                                                                                 | 1/40 [00:10<06:59, 10.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FA09288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226DAEEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 102.\n",
      "Epoch 00152: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|████▏                                                                              | 2/40 [00:22<07:17, 11.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21967EA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADCD5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Epoch 00100: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▏                                                                            | 3/40 [00:34<07:15, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E54ACA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F224803E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 00106: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|████████▎                                                                          | 4/40 [00:47<07:11, 11.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F239988C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226E68828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 121.\n",
      "Epoch 00171: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|██████████▍                                                                        | 5/40 [01:00<07:18, 12.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226E689D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2271D3558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 111.\n",
      "Epoch 00161: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|████████████▍                                                                      | 6/40 [01:15<07:28, 13.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2193F28B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F220CCD4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 108.\n",
      "Epoch 00158: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|██████████████▌                                                                    | 7/40 [01:33<08:07, 14.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F227173E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22CE77CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 140.\n",
      "Epoch 00190: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▌                                                                  | 8/40 [01:52<08:35, 16.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CE77AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADCD5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 180.\n",
      "Epoch 00230: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██████████████████▋                                                                | 9/40 [02:08<08:21, 16.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E51F048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C725DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 244.\n",
      "Epoch 00294: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|████████████████████▌                                                             | 10/40 [02:26<08:26, 16.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FD895E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F221083048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 166.\n",
      "Epoch 00216: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██████████████████████▌                                                           | 11/40 [02:46<08:29, 17.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E51F1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F224BCDA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 231.\n",
      "Epoch 00281: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|████████████████████████▌                                                         | 12/40 [03:10<09:11, 19.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E4F5438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F241475798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 186.\n",
      "Epoch 00236: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|██████████████████████████▋                                                       | 13/40 [03:25<08:16, 18.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E2E2168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E34C828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 252.\n",
      "Epoch 00302: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|████████████████████████████▋                                                     | 14/40 [03:44<07:57, 18.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E34CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2259B7E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 141.\n",
      "Epoch 00191: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|██████████████████████████████▊                                                   | 15/40 [04:01<07:32, 18.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22AE4D5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F224ED8828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████████████████████████████████▊                                                 | 16/40 [04:10<06:03, 15.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.4, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FD89678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226CF01F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 254.\n",
      "Epoch 00304: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|██████████████████████████████████▊                                               | 17/40 [04:28<06:11, 16.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.4, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226CF0F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F224ED8708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 169.\n",
      "Epoch 00219: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████████████████████████████████████▉                                             | 18/40 [04:42<05:42, 15.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.4, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21967E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2271D3558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Epoch 00085: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|██████████████████████████████████████▉                                           | 19/40 [04:52<04:52, 13.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.4, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226CF0EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21F27E5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Epoch 00091: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████████████████████████████████████████                                         | 20/40 [05:03<04:17, 12.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.5, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21F27E8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E4F89D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "Epoch 00118: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|███████████████████████████████████████████                                       | 21/40 [05:14<03:53, 12.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.5, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CE12D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F239698948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 00106: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████████████████████████████████████████████                                     | 22/40 [05:24<03:32, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.5, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CC543A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C725798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|███████████████████████████████████████████████▏                                  | 23/40 [05:34<03:10, 11.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.5, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21967E798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21BFD54C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|█████████████████████████████████████████████████▏                                | 24/40 [05:43<02:49, 10.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.6, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21BFD5048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F219497438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 83.\n",
      "Epoch 00133: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|███████████████████████████████████████████████████▎                              | 25/40 [05:55<02:44, 10.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.6, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226E68D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CDAB678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "Epoch 00125: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|█████████████████████████████████████████████████████▎                            | 26/40 [06:05<02:29, 10.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.6, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B010AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23FC36828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|███████████████████████████████████████████████████████▎                          | 27/40 [06:15<02:16, 10.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.6, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C4945E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2285DD048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 61.\n",
      "Epoch 00111: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|█████████████████████████████████████████████████████████▍                        | 28/40 [06:28<02:12, 11.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.7, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F24271D8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21967E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████████████████████████████████████████████████████████▍                      | 29/40 [06:34<01:44,  9.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.7, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E34CAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E4F8B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Epoch 00078: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 30/40 [06:41<01:29,  8.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.7, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2382C03A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2416545E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Epoch 00077: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████████████████████████████████████████████████████████████▌                  | 31/40 [06:51<01:22,  9.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.7, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E54AE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225905E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Epoch 00094: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 32/40 [07:02<01:16,  9.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.8, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CC543A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2193F21F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Epoch 00076: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|███████████████████████████████████████████████████████████████████▋              | 33/40 [07:09<01:01,  8.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.8, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FC493A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21F436798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 34/40 [07:14<00:47,  7.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.8, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E34C048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2195F74C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 35/40 [07:22<00:39,  7.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.8, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B4589D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2213DDCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 36/40 [07:33<00:35,  8.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.9, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2213DD438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F2335F0708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▊      | 37/40 [07:39<00:23,  7.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.9, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22C725168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22C494948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 38/40 [07:45<00:14,  7.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.9, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F238473CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23E4F8168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|███████████████████████████████████████████████████████████████████████████████▉  | 39/40 [07:52<00:07,  7.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.9, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 6, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "adding layer 4\n",
      "adding layer 5\n",
      "adding layer 6\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F2271ABB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21AFC2A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [08:00<00:00, 12.01s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3dc0b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/051022214536.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5f75ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b1413ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>302</td>\n",
       "      <td>0.553516</td>\n",
       "      <td>[0.7125505  0.46052635 0.5962733 ]</td>\n",
       "      <td>0.582247</td>\n",
       "      <td>[0.74038464 0.32142854 0.5423729 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>158</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>[0.68914956 0.48396504 0.6560636 ]</td>\n",
       "      <td>0.590320</td>\n",
       "      <td>[0.7071823 0.2631579 0.576    ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>236</td>\n",
       "      <td>0.587707</td>\n",
       "      <td>[0.6647888  0.41139242 0.5936255 ]</td>\n",
       "      <td>0.592761</td>\n",
       "      <td>[0.7179488 0.2857143 0.5190839]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>294</td>\n",
       "      <td>0.506819</td>\n",
       "      <td>[0.6978022  0.50491804 0.6707071 ]</td>\n",
       "      <td>0.593632</td>\n",
       "      <td>[0.74111676 0.2962963  0.56488556]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>230</td>\n",
       "      <td>0.539934</td>\n",
       "      <td>[0.6725926  0.44864866 0.6335404 ]</td>\n",
       "      <td>0.594149</td>\n",
       "      <td>[0.6666667  0.26086956 0.59016395]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>161</td>\n",
       "      <td>0.510376</td>\n",
       "      <td>[0.7238913  0.49707603 0.6776181 ]</td>\n",
       "      <td>0.596518</td>\n",
       "      <td>[0.7150259  0.30555555 0.5128205 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171</td>\n",
       "      <td>0.514692</td>\n",
       "      <td>[0.6948905  0.48850572 0.6545454 ]</td>\n",
       "      <td>0.600361</td>\n",
       "      <td>[0.74999994 0.28571427 0.56296295]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>0.594077</td>\n",
       "      <td>[0.6407507  0.47588426 0.50106156]</td>\n",
       "      <td>0.600444</td>\n",
       "      <td>[0.7339449  0.4        0.47706422]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>304</td>\n",
       "      <td>0.618160</td>\n",
       "      <td>[0.71372545 0.34020618 0.57644993]</td>\n",
       "      <td>0.604975</td>\n",
       "      <td>[0.77227724 0.2631579  0.57746476]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "      <td>0.429256</td>\n",
       "      <td>[0.7681564  0.62295085 0.78500986]</td>\n",
       "      <td>0.606845</td>\n",
       "      <td>[0.7179488  0.41095892 0.50877196]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>281</td>\n",
       "      <td>0.563949</td>\n",
       "      <td>[0.69361144 0.4084507  0.6331237 ]</td>\n",
       "      <td>0.615159</td>\n",
       "      <td>[0.7830188  0.29090908 0.53913045]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>190</td>\n",
       "      <td>0.523686</td>\n",
       "      <td>[0.68857145 0.460274   0.57451403]</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>[0.68508285 0.2195122  0.5546218 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>[0.7860963  0.61736333 0.72494674]</td>\n",
       "      <td>0.633351</td>\n",
       "      <td>[0.71578956 0.33333337 0.49122807]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>219</td>\n",
       "      <td>0.646162</td>\n",
       "      <td>[0.6501241  0.3260188  0.32258064]</td>\n",
       "      <td>0.633631</td>\n",
       "      <td>[0.7312775  0.35294116 0.2988506 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>0.421218</td>\n",
       "      <td>[0.8060522  0.61093247 0.7591836 ]</td>\n",
       "      <td>0.634147</td>\n",
       "      <td>[0.6847826  0.28169015 0.5354331 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>191</td>\n",
       "      <td>0.662368</td>\n",
       "      <td>[0.67220896 0.256      0.38990828]</td>\n",
       "      <td>0.644775</td>\n",
       "      <td>[0.74698794 0.15789475 0.2736842 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>0.396457</td>\n",
       "      <td>[0.79012346 0.6344828  0.79371315]</td>\n",
       "      <td>0.648909</td>\n",
       "      <td>[0.6804124  0.26666665 0.4778761 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>106</td>\n",
       "      <td>0.679567</td>\n",
       "      <td>[0.59949625 0.23448277 0.34234235]</td>\n",
       "      <td>0.654376</td>\n",
       "      <td>[0.7509579  0.22222221 0.30588236]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>118</td>\n",
       "      <td>0.674811</td>\n",
       "      <td>[0.6035806  0.22680412 0.36043957]</td>\n",
       "      <td>0.663784</td>\n",
       "      <td>[0.7191012  0.26666665 0.14285713]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>133</td>\n",
       "      <td>0.688383</td>\n",
       "      <td>[0.5483444  0.22073579 0.3206751 ]</td>\n",
       "      <td>0.666740</td>\n",
       "      <td>[0.69718313 0.12903225 0.02985075]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.6</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>125</td>\n",
       "      <td>0.674170</td>\n",
       "      <td>[0.59079283 0.24104235 0.32346243]</td>\n",
       "      <td>0.667361</td>\n",
       "      <td>[0.70758116 0.1935484  0.16216217]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.6</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>111</td>\n",
       "      <td>0.687065</td>\n",
       "      <td>[0.48382556 0.22018349 0.3142857 ]</td>\n",
       "      <td>0.669173</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.6</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>91</td>\n",
       "      <td>0.683031</td>\n",
       "      <td>[0.5326086  0.22560978 0.30603448]</td>\n",
       "      <td>0.673536</td>\n",
       "      <td>[0.70212764 0.13793103 0.08450705]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>85</td>\n",
       "      <td>0.689683</td>\n",
       "      <td>[0.46413502 0.15662651 0.34226805]</td>\n",
       "      <td>0.674278</td>\n",
       "      <td>[0.69964665 0.12903225 0.05882353]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>81</td>\n",
       "      <td>0.691258</td>\n",
       "      <td>[0.4402878  0.21671827 0.3490196 ]</td>\n",
       "      <td>0.675208</td>\n",
       "      <td>[0.67820066 0.         0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.6</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78</td>\n",
       "      <td>0.696392</td>\n",
       "      <td>[0.50341994 0.18497111 0.27937916]</td>\n",
       "      <td>0.675278</td>\n",
       "      <td>[0.68531466 0.06896552 0.02985075]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.7</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>76</td>\n",
       "      <td>0.706559</td>\n",
       "      <td>[0.42120764 0.19819818 0.32170543]</td>\n",
       "      <td>0.676597</td>\n",
       "      <td>[0.6903915 0.        0.1081081]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>77</td>\n",
       "      <td>0.694531</td>\n",
       "      <td>[0.45086703 0.20307693 0.3522505 ]</td>\n",
       "      <td>0.677731</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.7</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>94</td>\n",
       "      <td>0.696944</td>\n",
       "      <td>[0.42962965 0.14005603 0.33064517]</td>\n",
       "      <td>0.679176</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.7</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>98</td>\n",
       "      <td>0.703886</td>\n",
       "      <td>[0.4483776  0.15135136 0.29999998]</td>\n",
       "      <td>0.679613</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>51</td>\n",
       "      <td>0.684518</td>\n",
       "      <td>[0.4978541  0.21752267 0.38955823]</td>\n",
       "      <td>0.684361</td>\n",
       "      <td>[0.66960347 0.31707317 0.05479452]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>51</td>\n",
       "      <td>0.695437</td>\n",
       "      <td>[0.47277936 0.2195122  0.35856575]</td>\n",
       "      <td>0.688227</td>\n",
       "      <td>[0.6953405  0.07142857 0.13333334]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.7</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>51</td>\n",
       "      <td>0.713336</td>\n",
       "      <td>[0.41751826 0.17777778 0.310559  ]</td>\n",
       "      <td>0.688580</td>\n",
       "      <td>[0.6827586 0.        0.       ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51</td>\n",
       "      <td>0.694939</td>\n",
       "      <td>[0.3945783  0.17789757 0.30831644]</td>\n",
       "      <td>0.688867</td>\n",
       "      <td>[0.64840186 0.30434784 0.05633803]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>51</td>\n",
       "      <td>0.694862</td>\n",
       "      <td>[0.42480618 0.18230563 0.32941177]</td>\n",
       "      <td>0.690498</td>\n",
       "      <td>[0.01801802 0.         0.4773663 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>51</td>\n",
       "      <td>0.725939</td>\n",
       "      <td>[0.35255355 0.16120906 0.3167939 ]</td>\n",
       "      <td>0.694754</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>51</td>\n",
       "      <td>0.708126</td>\n",
       "      <td>[0.3797054  0.21307506 0.37301588]</td>\n",
       "      <td>0.696367</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>51</td>\n",
       "      <td>0.703553</td>\n",
       "      <td>[0.3847377  0.18085106 0.3594646 ]</td>\n",
       "      <td>0.696879</td>\n",
       "      <td>[0.03883495 0.24299064 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>51</td>\n",
       "      <td>0.721612</td>\n",
       "      <td>[0.43452382 0.2404692  0.3339806 ]</td>\n",
       "      <td>0.698021</td>\n",
       "      <td>[0.6875     0.07142857 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>51</td>\n",
       "      <td>0.721253</td>\n",
       "      <td>[0.3653846  0.17066668 0.33648393]</td>\n",
       "      <td>0.698635</td>\n",
       "      <td>[0.        0.        0.5078125]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "13           302  0.553516  [0.7125505  0.46052635 0.5962733 ]  0.582247   \n",
       "6            158  0.524369  [0.68914956 0.48396504 0.6560636 ]  0.590320   \n",
       "12           236  0.587707  [0.6647888  0.41139242 0.5936255 ]  0.592761   \n",
       "9            294  0.506819  [0.6978022  0.50491804 0.6707071 ]  0.593632   \n",
       "8            230  0.539934  [0.6725926  0.44864866 0.6335404 ]  0.594149   \n",
       "5            161  0.510376  [0.7238913  0.49707603 0.6776181 ]  0.596518   \n",
       "4            171  0.514692  [0.6948905  0.48850572 0.6545454 ]  0.600361   \n",
       "10           216  0.594077  [0.6407507  0.47588426 0.50106156]  0.600444   \n",
       "16           304  0.618160  [0.71372545 0.34020618 0.57644993]  0.604975   \n",
       "1            152  0.429256  [0.7681564  0.62295085 0.78500986]  0.606845   \n",
       "11           281  0.563949  [0.69361144 0.4084507  0.6331237 ]  0.615159   \n",
       "7            190  0.523686  [0.68857145 0.460274   0.57451403]  0.624437   \n",
       "2            100  0.448912  [0.7860963  0.61736333 0.72494674]  0.633351   \n",
       "17           219  0.646162  [0.6501241  0.3260188  0.32258064]  0.633631   \n",
       "0            138  0.421218  [0.8060522  0.61093247 0.7591836 ]  0.634147   \n",
       "14           191  0.662368  [0.67220896 0.256      0.38990828]  0.644775   \n",
       "3            106  0.396457  [0.79012346 0.6344828  0.79371315]  0.648909   \n",
       "21           106  0.679567  [0.59949625 0.23448277 0.34234235]  0.654376   \n",
       "20           118  0.674811  [0.6035806  0.22680412 0.36043957]  0.663784   \n",
       "24           133  0.688383  [0.5483444  0.22073579 0.3206751 ]  0.666740   \n",
       "25           125  0.674170  [0.59079283 0.24104235 0.32346243]  0.667361   \n",
       "27           111  0.687065  [0.48382556 0.22018349 0.3142857 ]  0.669173   \n",
       "19            91  0.683031  [0.5326086  0.22560978 0.30603448]  0.673536   \n",
       "18            85  0.689683  [0.46413502 0.15662651 0.34226805]  0.674278   \n",
       "26            81  0.691258  [0.4402878  0.21671827 0.3490196 ]  0.675208   \n",
       "29            78  0.696392  [0.50341994 0.18497111 0.27937916]  0.675278   \n",
       "32            76  0.706559  [0.42120764 0.19819818 0.32170543]  0.676597   \n",
       "30            77  0.694531  [0.45086703 0.20307693 0.3522505 ]  0.677731   \n",
       "31            94  0.696944  [0.42962965 0.14005603 0.33064517]  0.679176   \n",
       "35            98  0.703886  [0.4483776  0.15135136 0.29999998]  0.679613   \n",
       "15            51  0.684518  [0.4978541  0.21752267 0.38955823]  0.684361   \n",
       "28            51  0.695437  [0.47277936 0.2195122  0.35856575]  0.688227   \n",
       "34            51  0.713336  [0.41751826 0.17777778 0.310559  ]  0.688580   \n",
       "22            51  0.694939  [0.3945783  0.17789757 0.30831644]  0.688867   \n",
       "23            51  0.694862  [0.42480618 0.18230563 0.32941177]  0.690498   \n",
       "39            51  0.725939  [0.35255355 0.16120906 0.3167939 ]  0.694754   \n",
       "36            51  0.708126  [0.3797054  0.21307506 0.37301588]  0.696367   \n",
       "33            51  0.703553  [0.3847377  0.18085106 0.3594646 ]  0.696879   \n",
       "37            51  0.721612  [0.43452382 0.2404692  0.3339806 ]  0.698021   \n",
       "38            51  0.721253  [0.3653846  0.17066668 0.33648393]  0.698635   \n",
       "\n",
       "                       val_fbeta_score activation_layer  activity_regularizer  \\\n",
       "13  [0.74038464 0.32142854 0.5423729 ]             tanh                0.0001   \n",
       "6      [0.7071823 0.2631579 0.576    ]             tanh                0.0001   \n",
       "12     [0.7179488 0.2857143 0.5190839]             tanh                0.0001   \n",
       "9   [0.74111676 0.2962963  0.56488556]             tanh                0.0001   \n",
       "8   [0.6666667  0.26086956 0.59016395]             tanh                0.0001   \n",
       "5   [0.7150259  0.30555555 0.5128205 ]             tanh                0.0001   \n",
       "4   [0.74999994 0.28571427 0.56296295]             tanh                0.0001   \n",
       "10  [0.7339449  0.4        0.47706422]             tanh                0.0001   \n",
       "16  [0.77227724 0.2631579  0.57746476]             tanh                0.0001   \n",
       "1   [0.7179488  0.41095892 0.50877196]             tanh                0.0001   \n",
       "11  [0.7830188  0.29090908 0.53913045]             tanh                0.0001   \n",
       "7   [0.68508285 0.2195122  0.5546218 ]             tanh                0.0001   \n",
       "2   [0.71578956 0.33333337 0.49122807]             tanh                0.0001   \n",
       "17  [0.7312775  0.35294116 0.2988506 ]             tanh                0.0001   \n",
       "0   [0.6847826  0.28169015 0.5354331 ]             tanh                0.0001   \n",
       "14  [0.74698794 0.15789475 0.2736842 ]             tanh                0.0001   \n",
       "3   [0.6804124  0.26666665 0.4778761 ]             tanh                0.0001   \n",
       "21  [0.7509579  0.22222221 0.30588236]             tanh                0.0001   \n",
       "20  [0.7191012  0.26666665 0.14285713]             tanh                0.0001   \n",
       "24  [0.69718313 0.12903225 0.02985075]             tanh                0.0001   \n",
       "25  [0.70758116 0.1935484  0.16216217]             tanh                0.0001   \n",
       "27     [0.6827586 0.        0.       ]             tanh                0.0001   \n",
       "19  [0.70212764 0.13793103 0.08450705]             tanh                0.0001   \n",
       "18  [0.69964665 0.12903225 0.05882353]             tanh                0.0001   \n",
       "26  [0.67820066 0.         0.        ]             tanh                0.0001   \n",
       "29  [0.68531466 0.06896552 0.02985075]             tanh                0.0001   \n",
       "32     [0.6903915 0.        0.1081081]             tanh                0.0001   \n",
       "30     [0.6827586 0.        0.       ]             tanh                0.0001   \n",
       "31     [0.6827586 0.        0.       ]             tanh                0.0001   \n",
       "35     [0.6827586 0.        0.       ]             tanh                0.0001   \n",
       "15  [0.66960347 0.31707317 0.05479452]             tanh                0.0001   \n",
       "28  [0.6953405  0.07142857 0.13333334]             tanh                0.0001   \n",
       "34     [0.6827586 0.        0.       ]             tanh                0.0001   \n",
       "22  [0.64840186 0.30434784 0.05633803]             tanh                0.0001   \n",
       "23  [0.01801802 0.         0.4773663 ]             tanh                0.0001   \n",
       "39  [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "36  [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "33  [0.03883495 0.24299064 0.        ]             tanh                0.0001   \n",
       "37  [0.6875     0.07142857 0.        ]             tanh                0.0001   \n",
       "38     [0.        0.        0.5078125]             tanh                0.0001   \n",
       "\n",
       "    batc_normalization  batch_size  bias_regularizer  dropout  epochs  \\\n",
       "13                True          64            0.0001      0.3  100000   \n",
       "6                 True          64            0.0001      0.1  100000   \n",
       "12                True          64            0.0001      0.3  100000   \n",
       "9                 True          64            0.0001      0.2  100000   \n",
       "8                 True          64            0.0001      0.2  100000   \n",
       "5                 True          64            0.0001      0.1  100000   \n",
       "4                 True          64            0.0001      0.1  100000   \n",
       "10                True          64            0.0001      0.2  100000   \n",
       "16                True          64            0.0001      0.4  100000   \n",
       "1                 True          64            0.0001      0.0  100000   \n",
       "11                True          64            0.0001      0.2  100000   \n",
       "7                 True          64            0.0001      0.1  100000   \n",
       "2                 True          64            0.0001      0.0  100000   \n",
       "17                True          64            0.0001      0.4  100000   \n",
       "0                 True          64            0.0001      0.0  100000   \n",
       "14                True          64            0.0001      0.3  100000   \n",
       "3                 True          64            0.0001      0.0  100000   \n",
       "21                True          64            0.0001      0.5  100000   \n",
       "20                True          64            0.0001      0.5  100000   \n",
       "24                True          64            0.0001      0.6  100000   \n",
       "25                True          64            0.0001      0.6  100000   \n",
       "27                True          64            0.0001      0.6  100000   \n",
       "19                True          64            0.0001      0.4  100000   \n",
       "18                True          64            0.0001      0.4  100000   \n",
       "26                True          64            0.0001      0.6  100000   \n",
       "29                True          64            0.0001      0.7  100000   \n",
       "32                True          64            0.0001      0.8  100000   \n",
       "30                True          64            0.0001      0.7  100000   \n",
       "31                True          64            0.0001      0.7  100000   \n",
       "35                True          64            0.0001      0.8  100000   \n",
       "15                True          64            0.0001      0.3  100000   \n",
       "28                True          64            0.0001      0.7  100000   \n",
       "34                True          64            0.0001      0.8  100000   \n",
       "22                True          64            0.0001      0.5  100000   \n",
       "23                True          64            0.0001      0.5  100000   \n",
       "39                True          64            0.0001      0.9  100000   \n",
       "36                True          64            0.0001      0.9  100000   \n",
       "33                True          64            0.0001      0.8  100000   \n",
       "37                True          64            0.0001      0.9  100000   \n",
       "38                True          64            0.0001      0.9  100000   \n",
       "\n",
       "    first_neuron  hidden_layers  hidden_neuron kernel_initializer  \\\n",
       "13            55              3             50            uniform   \n",
       "6             55              6             50            uniform   \n",
       "12            55              3             50            uniform   \n",
       "9             55              3             50            uniform   \n",
       "8             55              3             50            uniform   \n",
       "5             55              3             50            uniform   \n",
       "4             55              3             50            uniform   \n",
       "10            55              6             50            uniform   \n",
       "16            55              3             50            uniform   \n",
       "1             55              3             50            uniform   \n",
       "11            55              6             50            uniform   \n",
       "7             55              6             50            uniform   \n",
       "2             55              6             50            uniform   \n",
       "17            55              3             50            uniform   \n",
       "0             55              3             50            uniform   \n",
       "14            55              6             50            uniform   \n",
       "3             55              6             50            uniform   \n",
       "21            55              3             50            uniform   \n",
       "20            55              3             50            uniform   \n",
       "24            55              3             50            uniform   \n",
       "25            55              3             50            uniform   \n",
       "27            55              6             50            uniform   \n",
       "19            55              6             50            uniform   \n",
       "18            55              6             50            uniform   \n",
       "26            55              6             50            uniform   \n",
       "29            55              3             50            uniform   \n",
       "32            55              3             50            uniform   \n",
       "30            55              6             50            uniform   \n",
       "31            55              6             50            uniform   \n",
       "35            55              6             50            uniform   \n",
       "15            55              6             50            uniform   \n",
       "28            55              3             50            uniform   \n",
       "34            55              6             50            uniform   \n",
       "22            55              6             50            uniform   \n",
       "23            55              6             50            uniform   \n",
       "39            55              6             50            uniform   \n",
       "36            55              3             50            uniform   \n",
       "33            55              3             50            uniform   \n",
       "37            55              3             50            uniform   \n",
       "38            55              6             50            uniform   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2 last_activation     lr  \n",
       "13               0.000001                0.00001         sigmoid  0.001  \n",
       "6                0.000001                0.00001         sigmoid  0.001  \n",
       "12               0.000001                0.00001         sigmoid  0.001  \n",
       "9                0.000001                0.00001         sigmoid  0.001  \n",
       "8                0.000001                0.00001         sigmoid  0.001  \n",
       "5                0.000001                0.00001         sigmoid  0.001  \n",
       "4                0.000001                0.00001         sigmoid  0.001  \n",
       "10               0.000001                0.00001         sigmoid  0.001  \n",
       "16               0.000001                0.00001         sigmoid  0.001  \n",
       "1                0.000001                0.00001         sigmoid  0.001  \n",
       "11               0.000001                0.00001         sigmoid  0.001  \n",
       "7                0.000001                0.00001         sigmoid  0.001  \n",
       "2                0.000001                0.00001         sigmoid  0.001  \n",
       "17               0.000001                0.00001         sigmoid  0.001  \n",
       "0                0.000001                0.00001         sigmoid  0.001  \n",
       "14               0.000001                0.00001         sigmoid  0.001  \n",
       "3                0.000001                0.00001         sigmoid  0.001  \n",
       "21               0.000001                0.00001         sigmoid  0.001  \n",
       "20               0.000001                0.00001         sigmoid  0.001  \n",
       "24               0.000001                0.00001         sigmoid  0.001  \n",
       "25               0.000001                0.00001         sigmoid  0.001  \n",
       "27               0.000001                0.00001         sigmoid  0.001  \n",
       "19               0.000001                0.00001         sigmoid  0.001  \n",
       "18               0.000001                0.00001         sigmoid  0.001  \n",
       "26               0.000001                0.00001         sigmoid  0.001  \n",
       "29               0.000001                0.00001         sigmoid  0.001  \n",
       "32               0.000001                0.00001         sigmoid  0.001  \n",
       "30               0.000001                0.00001         sigmoid  0.001  \n",
       "31               0.000001                0.00001         sigmoid  0.001  \n",
       "35               0.000001                0.00001         sigmoid  0.001  \n",
       "15               0.000001                0.00001         sigmoid  0.001  \n",
       "28               0.000001                0.00001         sigmoid  0.001  \n",
       "34               0.000001                0.00001         sigmoid  0.001  \n",
       "22               0.000001                0.00001         sigmoid  0.001  \n",
       "23               0.000001                0.00001         sigmoid  0.001  \n",
       "39               0.000001                0.00001         sigmoid  0.001  \n",
       "36               0.000001                0.00001         sigmoid  0.001  \n",
       "33               0.000001                0.00001         sigmoid  0.001  \n",
       "37               0.000001                0.00001         sigmoid  0.001  \n",
       "38               0.000001                0.00001         sigmoid  0.001  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b63b3168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of dropout')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'dropout'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "599f2e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_layers')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_layers'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18fdf2",
   "metadata": {},
   "source": [
    "## Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "28f94942",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3],   #Done\n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001,0.0001,0.00001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.000001],#Done\n",
    "    'kernel_regularizer_l2':[0.00001],#Done\n",
    "    'bias_regularizer':[0.0001],#Done\n",
    "    'activity_regularizer':[0.0001],#Done\n",
    "\n",
    "    'dropout': [0,0.1,0.2,0.3],\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['uniform'],#Done\n",
    "\n",
    "    'activation_layer':['tanh'],#Done\n",
    " \n",
    "    'batc_normalization':[True,False],#Done\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d6b19bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/24 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21AF75DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226C648B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 00107: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|███▍                                                                               | 1/24 [00:08<03:04,  8.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22B12ACA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23B0F84C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▉                                                                            | 2/24 [00:13<02:22,  6.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CD4AB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21F2F5798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|██████████▍                                                                        | 3/24 [00:18<02:05,  6.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F20D6FB948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DEC4EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 147.\n",
      "Epoch 00197: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█████████████▊                                                                     | 4/24 [00:32<03:01,  9.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22DEAEC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CB4D678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|█████████████████▎                                                                 | 5/24 [00:40<02:46,  8.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23FD89D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CB4D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|████████████████████▊                                                              | 6/24 [00:46<02:19,  7.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22AE4DA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DEAE5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 187.\n",
      "Epoch 00237: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|████████████████████████▏                                                          | 7/24 [01:02<02:56, 10.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23B0F8438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DEAE948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███████████████████████████▋                                                       | 8/24 [01:09<02:29,  9.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22CD4A048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F220901438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███████████████████████████████▏                                                   | 9/24 [01:15<02:02,  8.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F219596948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226CF04C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 263.\n",
      "Epoch 00313: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|██████████████████████████████████▏                                               | 10/24 [01:34<02:42, 11.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E26CEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21AF75C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|█████████████████████████████████████▌                                            | 11/24 [01:40<02:08,  9.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22DEAEF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F220901D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████████████████████████████████████████                                         | 12/24 [01:47<01:47,  8.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E4F8EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CB4DD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|████████████████████████████████████████████▍                                     | 13/24 [01:52<01:25,  7.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21F27E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DFB1E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|███████████████████████████████████████████████▊                                  | 14/24 [01:56<01:06,  6.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22ADCDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21C65EC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|███████████████████████████████████████████████████▎                              | 15/24 [02:00<00:53,  5.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CB4D708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21F2F5D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████████████████████████████▋                           | 16/24 [02:05<00:44,  5.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226DC7558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22ADCD048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|██████████████████████████████████████████████████████████                        | 17/24 [02:10<00:38,  5.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F226CF0E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226DC7C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 18/24 [02:15<00:31,  5.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F228666828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F226DC70D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|████████████████████████████████████████████████████████████████▉                 | 19/24 [02:19<00:25,  5.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23CB4D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F225901678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 20/24 [02:25<00:20,  5.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23C73CDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DEC4318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 21/24 [02:30<00:15,  5.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23E34C438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F23CB4DCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 22/24 [02:35<00:10,  5.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.0001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21AFC2E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F21EDA7948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|██████████████████████████████████████████████████████████████████████████████▌   | 23/24 [02:39<00:04,  4.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': False, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 1e-05}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F22ADCDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F22DEC4288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [02:43<00:00,  6.83s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f9257b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/051022215718.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "631754a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8a281d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>313</td>\n",
       "      <td>0.542596</td>\n",
       "      <td>[0.6311239  0.44574782 0.62068963]</td>\n",
       "      <td>0.581670</td>\n",
       "      <td>[0.7216495 0.3928571 0.6060606]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197</td>\n",
       "      <td>0.478695</td>\n",
       "      <td>[0.7204611  0.51810586 0.711579  ]</td>\n",
       "      <td>0.595548</td>\n",
       "      <td>[0.6989247  0.27160493 0.59130436]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>237</td>\n",
       "      <td>0.533664</td>\n",
       "      <td>[0.6647145  0.43915343 0.6081371 ]</td>\n",
       "      <td>0.612575</td>\n",
       "      <td>[0.73846155 0.3157895  0.5405406 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>0.476207</td>\n",
       "      <td>[0.7733711  0.57313436 0.69815195]</td>\n",
       "      <td>0.649659</td>\n",
       "      <td>[0.6380368  0.24242425 0.5166667 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98</td>\n",
       "      <td>0.693698</td>\n",
       "      <td>[0.4992658  0.22429909 0.3954373 ]</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>[0.6344086  0.30508474 0.49635035]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>79</td>\n",
       "      <td>0.695831</td>\n",
       "      <td>[0.45588234 0.1882353  0.3661417 ]</td>\n",
       "      <td>0.681197</td>\n",
       "      <td>[0.595122  0.2       0.3649635]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>0.686544</td>\n",
       "      <td>[0.46085674 0.29120877 0.34086242]</td>\n",
       "      <td>0.686683</td>\n",
       "      <td>[0.60098517 0.22916666 0.26506025]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687644</td>\n",
       "      <td>[0.17391303 0.21368949 0.33035713]</td>\n",
       "      <td>0.687622</td>\n",
       "      <td>[0.        0.2121212 0.2682927]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687727</td>\n",
       "      <td>[0.20841685 0.25454545 0.14649682]</td>\n",
       "      <td>0.687736</td>\n",
       "      <td>[0.01960784 0.24186045 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687783</td>\n",
       "      <td>[0.02463054 0.24292454 0.04379562]</td>\n",
       "      <td>0.687811</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>51</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>[0.         0.24000001 0.47682115]</td>\n",
       "      <td>0.687843</td>\n",
       "      <td>[0.         0.33898303 0.5       ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51</td>\n",
       "      <td>0.690344</td>\n",
       "      <td>[0.5815424  0.23846152 0.38155138]</td>\n",
       "      <td>0.689275</td>\n",
       "      <td>[0.65753424 0.16949151 0.28846154]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>0.700974</td>\n",
       "      <td>[0.32279536 0.17789757 0.3597122 ]</td>\n",
       "      <td>0.690985</td>\n",
       "      <td>[0.3311258  0.18918918 0.42038217]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>[0.598509   0.02649007 0.27853882]</td>\n",
       "      <td>0.693259</td>\n",
       "      <td>[0.68070173 0.         0.08571429]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51</td>\n",
       "      <td>0.693288</td>\n",
       "      <td>[0.603538   0.04615384 0.27459955]</td>\n",
       "      <td>0.693268</td>\n",
       "      <td>[0.6520147  0.         0.12195123]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>51</td>\n",
       "      <td>0.693276</td>\n",
       "      <td>[0.6222707 0.        0.3562753]</td>\n",
       "      <td>0.693271</td>\n",
       "      <td>[0.63025206 0.         0.27586207]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>51</td>\n",
       "      <td>0.693289</td>\n",
       "      <td>[0.6574803  0.10884354 0.24657536]</td>\n",
       "      <td>0.693284</td>\n",
       "      <td>[0.6504065  0.25       0.13636364]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51</td>\n",
       "      <td>0.693863</td>\n",
       "      <td>[0.42047027 0.10810811 0.3561644 ]</td>\n",
       "      <td>0.693855</td>\n",
       "      <td>[0.5326633  0.10909091 0.37209305]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>51</td>\n",
       "      <td>0.693849</td>\n",
       "      <td>[0.03406326 0.24099381 0.14102563]</td>\n",
       "      <td>0.693858</td>\n",
       "      <td>[0.01960784 0.25510204 0.16666667]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>51</td>\n",
       "      <td>0.693881</td>\n",
       "      <td>[0.36092713 0.15426998 0.40641713]</td>\n",
       "      <td>0.693864</td>\n",
       "      <td>[0.29931974 0.18181819 0.30434784]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>0.693863</td>\n",
       "      <td>[0.12026726 0.19631903 0.41962773]</td>\n",
       "      <td>0.693868</td>\n",
       "      <td>[0.09259259 0.25714287 0.43283582]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51</td>\n",
       "      <td>0.698075</td>\n",
       "      <td>[0.49062046 0.21714287 0.37113398]</td>\n",
       "      <td>0.695639</td>\n",
       "      <td>[0.5154639  0.2197802  0.22680414]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>51</td>\n",
       "      <td>0.704336</td>\n",
       "      <td>[0.39171976 0.14285715 0.31496063]</td>\n",
       "      <td>0.696090</td>\n",
       "      <td>[0.27941176 0.18181819 0.17283951]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51</td>\n",
       "      <td>0.699622</td>\n",
       "      <td>[0.38810644 0.18508996 0.344     ]</td>\n",
       "      <td>0.698438</td>\n",
       "      <td>[0.2837838  0.20833334 0.33333334]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "9            313  0.542596  [0.6311239  0.44574782 0.62068963]  0.581670   \n",
       "3            197  0.478695  [0.7204611  0.51810586 0.711579  ]  0.595548   \n",
       "6            237  0.533664  [0.6647145  0.43915343 0.6081371 ]  0.612575   \n",
       "0            107  0.476207  [0.7733711  0.57313436 0.69815195]  0.649659   \n",
       "4             98  0.693698  [0.4992658  0.22429909 0.3954373 ]  0.679700   \n",
       "7             79  0.695831  [0.45588234 0.1882353  0.3661417 ]  0.681197   \n",
       "1             51  0.686544  [0.46085674 0.29120877 0.34086242]  0.686683   \n",
       "15            51  0.687644  [0.17391303 0.21368949 0.33035713]  0.687622   \n",
       "18            51  0.687727  [0.20841685 0.25454545 0.14649682]  0.687736   \n",
       "21            51  0.687783  [0.02463054 0.24292454 0.04379562]  0.687811   \n",
       "12            51  0.687831  [0.         0.24000001 0.47682115]  0.687843   \n",
       "5             51  0.690344  [0.5815424  0.23846152 0.38155138]  0.689275   \n",
       "2             51  0.700974  [0.32279536 0.17789757 0.3597122 ]  0.690985   \n",
       "22            51  0.693300  [0.598509   0.02649007 0.27853882]  0.693259   \n",
       "19            51  0.693288  [0.603538   0.04615384 0.27459955]  0.693268   \n",
       "16            51  0.693276     [0.6222707 0.        0.3562753]  0.693271   \n",
       "13            51  0.693289  [0.6574803  0.10884354 0.24657536]  0.693284   \n",
       "20            51  0.693863  [0.42047027 0.10810811 0.3561644 ]  0.693855   \n",
       "14            51  0.693849  [0.03406326 0.24099381 0.14102563]  0.693858   \n",
       "23            51  0.693881  [0.36092713 0.15426998 0.40641713]  0.693864   \n",
       "17            51  0.693863  [0.12026726 0.19631903 0.41962773]  0.693868   \n",
       "10            51  0.698075  [0.49062046 0.21714287 0.37113398]  0.695639   \n",
       "11            51  0.704336  [0.39171976 0.14285715 0.31496063]  0.696090   \n",
       "8             51  0.699622  [0.38810644 0.18508996 0.344     ]  0.698438   \n",
       "\n",
       "                       val_fbeta_score activation_layer  activity_regularizer  \\\n",
       "9      [0.7216495 0.3928571 0.6060606]             tanh                0.0001   \n",
       "3   [0.6989247  0.27160493 0.59130436]             tanh                0.0001   \n",
       "6   [0.73846155 0.3157895  0.5405406 ]             tanh                0.0001   \n",
       "0   [0.6380368  0.24242425 0.5166667 ]             tanh                0.0001   \n",
       "4   [0.6344086  0.30508474 0.49635035]             tanh                0.0001   \n",
       "7      [0.595122  0.2       0.3649635]             tanh                0.0001   \n",
       "1   [0.60098517 0.22916666 0.26506025]             tanh                0.0001   \n",
       "15     [0.        0.2121212 0.2682927]             tanh                0.0001   \n",
       "18  [0.01960784 0.24186045 0.        ]             tanh                0.0001   \n",
       "21  [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "12  [0.         0.33898303 0.5       ]             tanh                0.0001   \n",
       "5   [0.65753424 0.16949151 0.28846154]             tanh                0.0001   \n",
       "2   [0.3311258  0.18918918 0.42038217]             tanh                0.0001   \n",
       "22  [0.68070173 0.         0.08571429]             tanh                0.0001   \n",
       "19  [0.6520147  0.         0.12195123]             tanh                0.0001   \n",
       "16  [0.63025206 0.         0.27586207]             tanh                0.0001   \n",
       "13  [0.6504065  0.25       0.13636364]             tanh                0.0001   \n",
       "20  [0.5326633  0.10909091 0.37209305]             tanh                0.0001   \n",
       "14  [0.01960784 0.25510204 0.16666667]             tanh                0.0001   \n",
       "23  [0.29931974 0.18181819 0.30434784]             tanh                0.0001   \n",
       "17  [0.09259259 0.25714287 0.43283582]             tanh                0.0001   \n",
       "10  [0.5154639  0.2197802  0.22680414]             tanh                0.0001   \n",
       "11  [0.27941176 0.18181819 0.17283951]             tanh                0.0001   \n",
       "8   [0.2837838  0.20833334 0.33333334]             tanh                0.0001   \n",
       "\n",
       "    batc_normalization  batch_size  bias_regularizer  dropout  epochs  \\\n",
       "9                 True          64            0.0001      0.3  100000   \n",
       "3                 True          64            0.0001      0.1  100000   \n",
       "6                 True          64            0.0001      0.2  100000   \n",
       "0                 True          64            0.0001      0.0  100000   \n",
       "4                 True          64            0.0001      0.1  100000   \n",
       "7                 True          64            0.0001      0.2  100000   \n",
       "1                 True          64            0.0001      0.0  100000   \n",
       "15               False          64            0.0001      0.1  100000   \n",
       "18               False          64            0.0001      0.2  100000   \n",
       "21               False          64            0.0001      0.3  100000   \n",
       "12               False          64            0.0001      0.0  100000   \n",
       "5                 True          64            0.0001      0.1  100000   \n",
       "2                 True          64            0.0001      0.0  100000   \n",
       "22               False          64            0.0001      0.3  100000   \n",
       "19               False          64            0.0001      0.2  100000   \n",
       "16               False          64            0.0001      0.1  100000   \n",
       "13               False          64            0.0001      0.0  100000   \n",
       "20               False          64            0.0001      0.2  100000   \n",
       "14               False          64            0.0001      0.0  100000   \n",
       "23               False          64            0.0001      0.3  100000   \n",
       "17               False          64            0.0001      0.1  100000   \n",
       "10                True          64            0.0001      0.3  100000   \n",
       "11                True          64            0.0001      0.3  100000   \n",
       "8                 True          64            0.0001      0.2  100000   \n",
       "\n",
       "    first_neuron  hidden_layers  hidden_neuron kernel_initializer  \\\n",
       "9             55              3             50            uniform   \n",
       "3             55              3             50            uniform   \n",
       "6             55              3             50            uniform   \n",
       "0             55              3             50            uniform   \n",
       "4             55              3             50            uniform   \n",
       "7             55              3             50            uniform   \n",
       "1             55              3             50            uniform   \n",
       "15            55              3             50            uniform   \n",
       "18            55              3             50            uniform   \n",
       "21            55              3             50            uniform   \n",
       "12            55              3             50            uniform   \n",
       "5             55              3             50            uniform   \n",
       "2             55              3             50            uniform   \n",
       "22            55              3             50            uniform   \n",
       "19            55              3             50            uniform   \n",
       "16            55              3             50            uniform   \n",
       "13            55              3             50            uniform   \n",
       "20            55              3             50            uniform   \n",
       "14            55              3             50            uniform   \n",
       "23            55              3             50            uniform   \n",
       "17            55              3             50            uniform   \n",
       "10            55              3             50            uniform   \n",
       "11            55              3             50            uniform   \n",
       "8             55              3             50            uniform   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2 last_activation       lr  \n",
       "9                0.000001                0.00001         sigmoid  0.00100  \n",
       "3                0.000001                0.00001         sigmoid  0.00100  \n",
       "6                0.000001                0.00001         sigmoid  0.00100  \n",
       "0                0.000001                0.00001         sigmoid  0.00100  \n",
       "4                0.000001                0.00001         sigmoid  0.00010  \n",
       "7                0.000001                0.00001         sigmoid  0.00010  \n",
       "1                0.000001                0.00001         sigmoid  0.00010  \n",
       "15               0.000001                0.00001         sigmoid  0.00100  \n",
       "18               0.000001                0.00001         sigmoid  0.00100  \n",
       "21               0.000001                0.00001         sigmoid  0.00100  \n",
       "12               0.000001                0.00001         sigmoid  0.00100  \n",
       "5                0.000001                0.00001         sigmoid  0.00001  \n",
       "2                0.000001                0.00001         sigmoid  0.00001  \n",
       "22               0.000001                0.00001         sigmoid  0.00010  \n",
       "19               0.000001                0.00001         sigmoid  0.00010  \n",
       "16               0.000001                0.00001         sigmoid  0.00010  \n",
       "13               0.000001                0.00001         sigmoid  0.00010  \n",
       "20               0.000001                0.00001         sigmoid  0.00001  \n",
       "14               0.000001                0.00001         sigmoid  0.00001  \n",
       "23               0.000001                0.00001         sigmoid  0.00001  \n",
       "17               0.000001                0.00001         sigmoid  0.00001  \n",
       "10               0.000001                0.00001         sigmoid  0.00010  \n",
       "11               0.000001                0.00001         sigmoid  0.00001  \n",
       "8                0.000001                0.00001         sigmoid  0.00001  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8f3a4548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "round_epochs                                            313\n",
       "loss                                               0.542596\n",
       "fbeta_score              [0.6311239  0.44574782 0.62068963]\n",
       "val_loss                                            0.58167\n",
       "val_fbeta_score             [0.7216495 0.3928571 0.6060606]\n",
       "activation_layer                                       tanh\n",
       "activity_regularizer                                 0.0001\n",
       "batc_normalization                                     True\n",
       "batch_size                                               64\n",
       "bias_regularizer                                     0.0001\n",
       "dropout                                                 0.3\n",
       "epochs                                               100000\n",
       "first_neuron                                             55\n",
       "hidden_layers                                             3\n",
       "hidden_neuron                                            50\n",
       "kernel_initializer                                  uniform\n",
       "kernel_regularizer_l1                              0.000001\n",
       "kernel_regularizer_l2                               0.00001\n",
       "last_activation                                     sigmoid\n",
       "lr                                                    0.001\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3],   #Done\n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001,0.0001,0.00001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.000001],#Done\n",
    "    'kernel_regularizer_l2':[0.00001],#Done\n",
    "    'bias_regularizer':[0.0001],#Done\n",
    "    'activity_regularizer':[0.0001],#Done\n",
    "\n",
    "    'dropout': [0,0.1,0.2,0.3],\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['uniform'],#Done\n",
    "\n",
    "    'activation_layer':['tanh'],#Done\n",
    " \n",
    "    'batc_normalization':[True,False],#Done\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a41df40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of lr')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'lr'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aa54506e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of dropout')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'dropout'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ffdc974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.65, 0.7)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'dropout'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n",
    "ax.set_ylim(0.65,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b1801e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batc_normalization')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batc_normalization'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "521e51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3],   #Done\n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.000001],#Done\n",
    "    'kernel_regularizer_l2':[0.00001],#Done\n",
    "    'bias_regularizer':[0.0001],#Done\n",
    "    'activity_regularizer':[0.0001],#Done\n",
    "\n",
    "    'dropout': [0.1,0.3],\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['uniform'],#Done\n",
    "\n",
    "    'activation_layer':['tanh'],#Done\n",
    " \n",
    "    'batc_normalization':[True],#Done\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bdf5f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F23F8015E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F220BAB9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 124.\n",
      "Epoch 00174: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:12<00:12, 12.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F21F27E048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F224ED88B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 211.\n",
      "Epoch 00261: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:30<00:00, 15.18s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "70b2c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/051022220423.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "82111fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "67902ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>0.508595</td>\n",
       "      <td>[0.71692747 0.4809384  0.6859504 ]</td>\n",
       "      <td>0.593438</td>\n",
       "      <td>[0.7329843  0.26865673 0.516129  ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>261</td>\n",
       "      <td>0.554234</td>\n",
       "      <td>[0.6703601 0.4471299 0.5978948]</td>\n",
       "      <td>0.604637</td>\n",
       "      <td>[0.7173913 0.3448276 0.5765766]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "0           174  0.508595  [0.71692747 0.4809384  0.6859504 ]  0.593438   \n",
       "1           261  0.554234     [0.6703601 0.4471299 0.5978948]  0.604637   \n",
       "\n",
       "                      val_fbeta_score activation_layer  activity_regularizer  \\\n",
       "0  [0.7329843  0.26865673 0.516129  ]             tanh                0.0001   \n",
       "1     [0.7173913 0.3448276 0.5765766]             tanh                0.0001   \n",
       "\n",
       "   batc_normalization  batch_size  bias_regularizer  dropout  epochs  \\\n",
       "0                True          64            0.0001      0.1  100000   \n",
       "1                True          64            0.0001      0.3  100000   \n",
       "\n",
       "   first_neuron  hidden_layers  hidden_neuron kernel_initializer  \\\n",
       "0            55              3             50            uniform   \n",
       "1            55              3             50            uniform   \n",
       "\n",
       "   kernel_regularizer_l1  kernel_regularizer_l2 last_activation     lr  \n",
       "0               0.000001                0.00001         sigmoid  0.001  \n",
       "1               0.000001                0.00001         sigmoid  0.001  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Staty finalowej sieci neuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5449b32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "round_epochs                                            313\n",
       "loss                                               0.542596\n",
       "fbeta_score              [0.6311239  0.44574782 0.62068963]\n",
       "val_loss                                            0.58167\n",
       "val_fbeta_score             [0.7216495 0.3928571 0.6060606]\n",
       "activation_layer                                       tanh\n",
       "activity_regularizer                                 0.0001\n",
       "batc_normalization                                     True\n",
       "batch_size                                               64\n",
       "bias_regularizer                                     0.0001\n",
       "dropout                                                 0.3\n",
       "epochs                                               100000\n",
       "first_neuron                                             55\n",
       "hidden_layers                                             3\n",
       "hidden_neuron                                            50\n",
       "kernel_initializer                                  uniform\n",
       "kernel_regularizer_l1                              0.000001\n",
       "kernel_regularizer_l2                               0.00001\n",
       "last_activation                                     sigmoid\n",
       "lr                                                    0.001\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae52ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ostatnie podejscie - rozne dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "744657bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_rate=[]\n",
    "for x in range(0,101):\n",
    "    dr_rate.append(x*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f0ec6710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.01,\n",
       " 0.02,\n",
       " 0.03,\n",
       " 0.04,\n",
       " 0.05,\n",
       " 0.06,\n",
       " 0.07,\n",
       " 0.08,\n",
       " 0.09,\n",
       " 0.1,\n",
       " 0.11,\n",
       " 0.12,\n",
       " 0.13,\n",
       " 0.14,\n",
       " 0.15,\n",
       " 0.16,\n",
       " 0.17,\n",
       " 0.18,\n",
       " 0.19,\n",
       " 0.2,\n",
       " 0.21,\n",
       " 0.22,\n",
       " 0.23,\n",
       " 0.24,\n",
       " 0.25,\n",
       " 0.26,\n",
       " 0.27,\n",
       " 0.28,\n",
       " 0.29,\n",
       " 0.3,\n",
       " 0.31,\n",
       " 0.32,\n",
       " 0.33,\n",
       " 0.34,\n",
       " 0.35000000000000003,\n",
       " 0.36,\n",
       " 0.37,\n",
       " 0.38,\n",
       " 0.39,\n",
       " 0.4,\n",
       " 0.41000000000000003,\n",
       " 0.42,\n",
       " 0.43,\n",
       " 0.44,\n",
       " 0.45,\n",
       " 0.46,\n",
       " 0.47000000000000003,\n",
       " 0.48,\n",
       " 0.49,\n",
       " 0.5,\n",
       " 0.51,\n",
       " 0.52,\n",
       " 0.53,\n",
       " 0.54,\n",
       " 0.55,\n",
       " 0.56,\n",
       " 0.5700000000000001,\n",
       " 0.58,\n",
       " 0.59,\n",
       " 0.6,\n",
       " 0.61,\n",
       " 0.62,\n",
       " 0.63,\n",
       " 0.64,\n",
       " 0.65,\n",
       " 0.66,\n",
       " 0.67,\n",
       " 0.68,\n",
       " 0.6900000000000001,\n",
       " 0.7000000000000001,\n",
       " 0.71,\n",
       " 0.72,\n",
       " 0.73,\n",
       " 0.74,\n",
       " 0.75,\n",
       " 0.76,\n",
       " 0.77,\n",
       " 0.78,\n",
       " 0.79,\n",
       " 0.8,\n",
       " 0.81,\n",
       " 0.8200000000000001,\n",
       " 0.8300000000000001,\n",
       " 0.84,\n",
       " 0.85,\n",
       " 0.86,\n",
       " 0.87,\n",
       " 0.88,\n",
       " 0.89,\n",
       " 0.9,\n",
       " 0.91,\n",
       " 0.92,\n",
       " 0.93,\n",
       " 0.9400000000000001,\n",
       " 0.9500000000000001,\n",
       " 0.96,\n",
       " 0.97,\n",
       " 0.98,\n",
       " 0.99,\n",
       " 1.0]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0c6e200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':[55],  #Done\n",
    "     'hidden_neuron':[50], #Done\n",
    "\n",
    "     'hidden_layers':[3],   #Done\n",
    "\n",
    "     \n",
    "    'epochs': [100000], # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': ['sigmoid'], #never touch it\n",
    "\n",
    "\n",
    "    'batch_size': [64], #Done\n",
    "\n",
    "    'lr':[0.001],\n",
    "    \n",
    "    'kernel_regularizer_l1':[0.000001],#Done\n",
    "    'kernel_regularizer_l2':[0.00001],#Done\n",
    "    'bias_regularizer':[0.0001],#Done\n",
    "    'activity_regularizer':[0.0001],#Done\n",
    "\n",
    "    'dropout': dr_rate,\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': ['uniform'],#Done\n",
    "\n",
    "    'activation_layer':['tanh'],#Done\n",
    " \n",
    "    'batc_normalization':[True],#Done\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e578c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(3, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adagrad(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=3,beta=1.0)])\n",
    "    \n",
    "  \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        class_weight={0 : 0.64147775,\n",
    "                                      1 : 2.42539683,\n",
    "                                      2 : 0.97201018},\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b075ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38A388558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38BCEA678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 314.\n",
      "Epoch 00364: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                                 | 1/101 [00:19<32:33, 19.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.01, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38A396E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942AEC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 314.\n",
      "Epoch 00364: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▌                                                                                | 2/101 [00:41<34:44, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.02, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3942AEDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3929D6678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 354.\n",
      "Epoch 00404: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▍                                                                               | 3/101 [01:06<36:57, 22.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.03, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A388C8DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3929D6798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 398.\n",
      "Epoch 00448: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▏                                                                              | 4/101 [01:32<39:02, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.04, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38BCEAD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A396EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 415.\n",
      "Epoch 00465: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████                                                                              | 5/101 [01:59<40:29, 25.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.05, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3905C0DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A388DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 439.\n",
      "Epoch 00489: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▊                                                                             | 6/101 [02:29<42:13, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.06, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395953A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A396438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 385.\n",
      "Epoch 00435: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▋                                                                            | 7/101 [02:55<41:22, 26.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.07, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3905763A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38BCEA948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 389.\n",
      "Epoch 00439: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▍                                                                           | 8/101 [03:21<40:46, 26.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.08, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A387130CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B57FAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 489.\n",
      "Epoch 00539: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▎                                                                          | 9/101 [03:54<43:29, 28.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.09, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B57F798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3905C0558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 327.\n",
      "Epoch 00377: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████                                                                         | 10/101 [04:18<40:57, 27.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.1, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38A388EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38D0540D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 328.\n",
      "Epoch 00378: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▊                                                                        | 11/101 [04:42<39:16, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.11, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A04CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A16D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 394.\n",
      "Epoch 00444: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▌                                                                       | 12/101 [05:09<39:19, 26.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.12, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FE298B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B4328B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 465.\n",
      "Epoch 00515: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▍                                                                      | 13/101 [05:40<40:43, 27.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.13, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B432B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FC53168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 413.\n",
      "Epoch 00463: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▏                                                                     | 14/101 [06:09<41:00, 28.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.14, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A04C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A396798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 377.\n",
      "Epoch 00427: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████                                                                     | 15/101 [06:36<39:58, 27.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.15, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E400048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942AEB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 430.\n",
      "Epoch 00480: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████▊                                                                    | 16/101 [07:06<40:07, 28.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.16, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3942AEDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A394323678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 339.\n",
      "Epoch 00389: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▋                                                                   | 17/101 [07:30<38:02, 27.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.17, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38BCEAA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942AE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 422.\n",
      "Epoch 00472: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▍                                                                  | 18/101 [08:00<38:37, 27.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.18, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A39434E288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B57FD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 446.\n",
      "Epoch 00496: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████▏                                                                 | 19/101 [08:30<39:05, 28.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.19, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A387130C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B57FF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 400.\n",
      "Epoch 00450: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████                                                                 | 20/101 [08:57<37:56, 28.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.2, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B4328B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3905C0828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 472.\n",
      "Epoch 00522: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████▊                                                                | 21/101 [09:28<38:39, 28.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.21, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FE29C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A394323438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 470.\n",
      "Epoch 00520: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████▋                                                               | 22/101 [09:58<38:43, 29.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.22, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395953558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B432678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 486.\n",
      "Epoch 00536: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████▍                                                              | 23/101 [10:30<39:02, 30.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.23, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FE7BAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A396EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 405.\n",
      "Epoch 00455: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▏                                                             | 24/101 [10:57<37:28, 29.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.24, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38672CA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942AEC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 516.\n",
      "Epoch 00566: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████                                                             | 25/101 [11:31<38:47, 30.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.25, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3959533A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FE7B948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 437.\n",
      "Epoch 00487: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|████████████████████▊                                                            | 26/101 [12:00<37:34, 30.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.26, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A396C8A558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A396948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 412.\n",
      "Epoch 00462: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|█████████████████████▋                                                           | 27/101 [12:27<36:05, 29.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.27, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A390576678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38729DAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 478.\n",
      "Epoch 00528: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████▍                                                          | 28/101 [12:58<36:13, 29.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.28, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3F9E94AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FE29F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 444.\n",
      "Epoch 00494: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|███████████████████████▎                                                         | 29/101 [13:27<35:28, 29.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.29, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3902C4708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A3965E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 421.\n",
      "Epoch 00471: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████                                                         | 30/101 [13:55<34:22, 29.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.3, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395953288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A396DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 453.\n",
      "Epoch 00503: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|████████████████████████▊                                                        | 31/101 [14:29<35:29, 30.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.31, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38A388708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38E47C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 451.\n",
      "Epoch 00501: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████████████████▋                                                       | 32/101 [15:03<36:28, 31.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.32, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38A266E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A395953558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 467.\n",
      "Epoch 00517: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████▍                                                      | 33/101 [15:39<37:13, 32.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.33, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A39045B828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942AE828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 405.\n",
      "Epoch 00455: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████▎                                                     | 34/101 [16:12<36:49, 32.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.34, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FC53708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A266318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 424.\n",
      "Epoch 00474: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████                                                     | 35/101 [16:47<36:58, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.35000000000000003, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3942AE4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A39045B3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 401.\n",
      "Epoch 00451: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|████████████████████████████▊                                                    | 36/101 [17:20<35:57, 33.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.36, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A381841828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A39062FDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 367.\n",
      "Epoch 00417: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|█████████████████████████████▋                                                   | 37/101 [17:47<33:26, 31.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.37, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A390576828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A39295D438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 500.\n",
      "Epoch 00550: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████████████████▍                                                  | 38/101 [18:22<34:17, 32.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.38, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B637F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FC53558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 502.\n",
      "Epoch 00552: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███████████████████████████████▎                                                 | 39/101 [18:57<34:31, 33.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.39, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38152ECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3943230D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 441.\n",
      "Epoch 00491: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████                                                 | 40/101 [19:25<32:13, 31.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.4, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FE29948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A39295D708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 403.\n",
      "Epoch 00453: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████████████████████▉                                                | 41/101 [19:51<29:51, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.41000000000000003, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3905C0948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A39062FEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 524.\n",
      "Epoch 00574: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████████████████████▋                                               | 42/101 [20:24<30:24, 30.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.42, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A39062F9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A394323EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 541.\n",
      "Epoch 00591: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|██████████████████████████████████▍                                              | 43/101 [20:57<30:34, 31.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.43, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A39434EB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FE298B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 547.\n",
      "Epoch 00597: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████▎                                             | 44/101 [21:29<30:02, 31.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.44, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3905C0B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A396168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 469.\n",
      "Epoch 00519: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████████████████████                                             | 45/101 [21:57<28:26, 30.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.45, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E8C7048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B6C59D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 483.\n",
      "Epoch 00533: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████████████████████████████████████▉                                            | 46/101 [22:25<27:24, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.46, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A39295D0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3902C49D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 477.\n",
      "Epoch 00527: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|█████████████████████████████████████▋                                           | 47/101 [22:54<26:28, 29.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.47000000000000003, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A394323D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A39434E168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 94.\n",
      "Epoch 00144: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|██████████████████████████████████████▍                                          | 48/101 [23:03<20:45, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.48, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38A396AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3902C4318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 584.\n",
      "Epoch 00634: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|███████████████████████████████████████▎                                         | 49/101 [23:38<23:19, 26.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.49, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A390576B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A394323DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 557.\n",
      "Epoch 00607: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████                                         | 50/101 [24:11<24:26, 28.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.5, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3901AFB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A39434EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 608.\n",
      "Epoch 00658: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████▉                                        | 51/101 [24:47<25:41, 30.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.51, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E400438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3902C41F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 473.\n",
      "Epoch 00523: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████████████████████████████████████████▋                                       | 52/101 [25:16<24:44, 30.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.52, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A396C8A288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A394323D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Epoch 00087: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████▌                                      | 53/101 [25:23<18:40, 23.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.53, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3905C09D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3901AF1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 266.\n",
      "Epoch 00316: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████▎                                     | 54/101 [25:42<17:09, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.54, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FE7BCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B57FA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 141.\n",
      "Epoch 00191: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|████████████████████████████████████████████                                     | 55/101 [25:54<14:36, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.55, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38D2A5B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38E400CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 88.\n",
      "Epoch 00138: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|████████████████████████████████████████████▉                                    | 56/101 [26:04<12:12, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.56, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B4CC048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FE7B708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 54.\n",
      "Epoch 00104: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████████████████████████████████████████████▋                                   | 57/101 [26:12<10:07, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.5700000000000001, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E9E34C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38D3DD288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 62.\n",
      "Epoch 00112: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████▌                                  | 58/101 [26:20<08:44, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.58, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FC53828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A390378E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 00110: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▎                                 | 59/101 [26:29<07:42, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.59, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B432DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3943235E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "Epoch 00114: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████                                 | 60/101 [26:37<07:01, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.6, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B6C5E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38D2A58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 00089: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████▉                                | 61/101 [26:44<06:14,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.61, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E5D6708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38E9E3678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 00120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|█████████████████████████████████████████████████▋                               | 62/101 [26:53<05:58,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.62, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E400288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3903781F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 00097: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████████████████████████████▌                              | 63/101 [27:01<05:31,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.63, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3901AF5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A388E103A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 00106: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████▎                             | 64/101 [27:09<05:16,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.64, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3942AE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3902C4438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 00086: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████▏                            | 65/101 [27:16<04:52,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.65, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E8C74C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3900DE8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Epoch 00099: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|████████████████████████████████████████████████████▉                            | 66/101 [27:24<04:39,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.66, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3901AF438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A39434E5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|█████████████████████████████████████████████████████▋                           | 67/101 [27:32<04:39,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.67, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3905C03A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942B61F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████▌                          | 68/101 [27:39<04:17,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.68, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A04E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38E5D6678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 63.\n",
      "Epoch 00113: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████████▎                         | 69/101 [27:48<04:16,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.6900000000000001, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38152ECA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3905C0E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 00106: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████████▏                        | 70/101 [27:56<04:09,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.7000000000000001, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38152E708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38D3DD558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Epoch 00074: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████████▉                        | 71/101 [28:02<03:47,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.71, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3902E8F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38729D9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Epoch 00077: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████████▋                       | 72/101 [28:09<03:32,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.72, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A387817318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38A388558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 85.\n",
      "Epoch 00135: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████▌                      | 73/101 [28:19<03:43,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.73, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B53C318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38CE59948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████▎                     | 74/101 [28:26<03:32,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.74, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B6C5168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38D2C4B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 55.\n",
      "Epoch 00105: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▏                    | 75/101 [28:34<03:26,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.75, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A048B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FC53948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 00075: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|████████████████████████████████████████████████████████████▉                    | 76/101 [28:41<03:08,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.76, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3902C49D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38E9E34C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Epoch 00078: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|█████████████████████████████████████████████████████████████▊                   | 77/101 [28:48<02:54,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.77, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E9E3048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A390314B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|██████████████████████████████████████████████████████████████▌                  | 78/101 [28:53<02:34,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.78, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38E400828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FE7B048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 61.\n",
      "Epoch 00111: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████████████████████████████████████████████████████████████▎                 | 79/101 [29:01<02:38,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.79, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FE7B5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A388E10558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Epoch 00083: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▏                | 80/101 [29:08<02:29,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.8, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3942B6798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A388E101F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 00086: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████▉                | 81/101 [29:15<02:22,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.81, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3905C0798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38FE7BD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Epoch 00083: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████▊               | 82/101 [29:22<02:13,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.8200000000000001, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38A3963A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942AEA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Epoch 00072: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████████▌              | 83/101 [29:29<02:03,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.8300000000000001, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38D2A5708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3900DE318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 00070: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████▎             | 84/101 [29:35<01:53,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.84, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A39295D558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38E8C7288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Epoch 00069: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████████████████████████████████████████████████████████████████▏            | 85/101 [29:41<01:44,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.85, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A049D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942AE5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████████████████████████████████████████████████████████████████▉            | 86/101 [29:49<01:43,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.86, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A396C8A0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A396D17168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|█████████████████████████████████████████████████████████████████████▊           | 87/101 [29:55<01:33,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.87, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B53C9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A388E10318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|██████████████████████████████████████████████████████████████████████▌          | 88/101 [30:02<01:27,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.88, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A04EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3942B6EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|███████████████████████████████████████████████████████████████████████▍         | 89/101 [30:07<01:15,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.89, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3F9CD0F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A388E10AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████████████████████████████████████████████████████████████████████▏        | 90/101 [30:12<01:05,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.9, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FC533A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A389169318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████▉        | 91/101 [30:18<00:57,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.91, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38B53C9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A389119798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 92/101 [30:23<00:50,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.92, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FE7B9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A389119438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|██████████████████████████████████████████████████████████████████████████▌      | 93/101 [30:28<00:44,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.93, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3901AFEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A390314D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 00063: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████████████████████████▍     | 94/101 [30:34<00:39,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.9400000000000001, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A394323AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A381841AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|████████████████████████████████████████████████████████████████████████████▏    | 95/101 [30:39<00:33,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.9500000000000001, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38D2A5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A389119B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|████████████████████████████████████████████████████████████████████████████▉    | 96/101 [30:45<00:27,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.96, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38BCEA168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A389169AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 00075: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████████████████████████████████████████████████████████████████████████▊   | 97/101 [30:51<00:23,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.97, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FE29678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B6C5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Epoch 00092: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|██████████████████████████████████████████████████████████████████████████████▌  | 98/101 [30:59<00:18,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.98, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38D2A5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B6C5708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Epoch 00087: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|███████████████████████████████████████████████████████████████████████████████▍ | 99/101 [31:06<00:13,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 0.99, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A04D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38B53CC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 00097: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|███████████████████████████████████████████████████████████████████████████████▏| 100/101 [31:14<00:06,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'tanh', 'activity_regularizer': 0.0001, 'batc_normalization': True, 'batch_size': 64, 'bias_regularizer': 0.0001, 'dropout': 1.0, 'epochs': 100000, 'first_neuron': 55, 'hidden_layers': 3, 'hidden_neuron': 50, 'kernel_initializer': 'uniform', 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'last_activation': 'sigmoid', 'lr': 0.001}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38BCEA3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"dropout\" (type Dropout).\n\n`rate` must be a scalar tensor or a float in the range [0, 1). Received: rate=1.0\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 55), dtype=float32)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-ab7150996d91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumerai_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# start runtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscan_run\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mscan_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\talos\\scan\\scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# otherwise proceed with next permutation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscan_round\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\talos\\scan\\scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mingest_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\talos\\model\\ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                       self.round_params)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-120-49e264190334>\u001b[0m in \u001b[0;36mnumerai_model\u001b[1;34m(x_train, y_train, x_val, y_val, params)\u001b[0m\n\u001b[0;32m     64\u001b[0m                                         \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                                         \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                                                       restore_best_weights=True)\n\u001b[0m\u001b[0;32m     67\u001b[0m                                     ] #,ta.live(),\n\u001b[0;32m     68\u001b[0m                         )\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"dropout\" (type Dropout).\n\n`rate` must be a scalar tensor or a float in the range [0, 1). Received: rate=1.0\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 55), dtype=float32)\n  • training=True"
     ]
    }
   ],
   "source": [
    "\n",
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,  \n",
    "            experiment_name='Predykcja klasy T - Weighted binary cross-entropy (softmax)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5476f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/T/Predykcja klasy T - Weighted binary cross-entropy (softmax)/051222131540.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9035340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df.sort_values('val_loss',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fa055779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>activity_regularizer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias_regularizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>kernel_regularizer_l1</th>\n",
       "      <th>kernel_regularizer_l2</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>539</td>\n",
       "      <td>0.312565</td>\n",
       "      <td>[0.84648186 0.67455626 0.81072557]</td>\n",
       "      <td>0.362793</td>\n",
       "      <td>[0.8497409 0.6557377 0.796875 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.08</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>465</td>\n",
       "      <td>0.288222</td>\n",
       "      <td>[0.86772484 0.7020649  0.8274761 ]</td>\n",
       "      <td>0.363441</td>\n",
       "      <td>[0.83902436 0.69230765 0.768     ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.04</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>448</td>\n",
       "      <td>0.283250</td>\n",
       "      <td>[0.87830687 0.7108433  0.83412325]</td>\n",
       "      <td>0.374417</td>\n",
       "      <td>[0.84313726 0.7083333  0.8000001 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>489</td>\n",
       "      <td>0.307949</td>\n",
       "      <td>[0.8400435  0.70520234 0.8248062 ]</td>\n",
       "      <td>0.380290</td>\n",
       "      <td>[0.82901555 0.73015875 0.7777778 ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>404</td>\n",
       "      <td>0.284982</td>\n",
       "      <td>[0.86137563 0.72303206 0.84565926]</td>\n",
       "      <td>0.380841</td>\n",
       "      <td>[0.83505154 0.6027397  0.81739134]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>51</td>\n",
       "      <td>0.713793</td>\n",
       "      <td>[0.34838712 0.18666665 0.3532847 ]</td>\n",
       "      <td>0.687339</td>\n",
       "      <td>[0.        0.        0.5078125]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.89</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>51</td>\n",
       "      <td>0.737424</td>\n",
       "      <td>[0.40145105 0.16227181 0.2881356 ]</td>\n",
       "      <td>0.688125</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.94</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>51</td>\n",
       "      <td>0.710347</td>\n",
       "      <td>[0.3710292  0.20366597 0.30696204]</td>\n",
       "      <td>0.688449</td>\n",
       "      <td>[0.20979021 0.18390803 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.92</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>51</td>\n",
       "      <td>0.709621</td>\n",
       "      <td>[0.36363637 0.21906693 0.30514097]</td>\n",
       "      <td>0.688935</td>\n",
       "      <td>[0.6836364  0.23809524 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.90</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>51</td>\n",
       "      <td>0.715127</td>\n",
       "      <td>[0.34666663 0.22222224 0.30842608]</td>\n",
       "      <td>0.694244</td>\n",
       "      <td>[0.         0.24770641 0.        ]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.91</td>\n",
       "      <td>100000</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      loss                         fbeta_score  val_loss  \\\n",
       "8            539  0.312565  [0.84648186 0.67455626 0.81072557]  0.362793   \n",
       "4            465  0.288222  [0.86772484 0.7020649  0.8274761 ]  0.363441   \n",
       "3            448  0.283250  [0.87830687 0.7108433  0.83412325]  0.374417   \n",
       "5            489  0.307949  [0.8400435  0.70520234 0.8248062 ]  0.380290   \n",
       "2            404  0.284982  [0.86137563 0.72303206 0.84565926]  0.380841   \n",
       "..           ...       ...                                 ...       ...   \n",
       "89            51  0.713793  [0.34838712 0.18666665 0.3532847 ]  0.687339   \n",
       "94            51  0.737424  [0.40145105 0.16227181 0.2881356 ]  0.688125   \n",
       "92            51  0.710347  [0.3710292  0.20366597 0.30696204]  0.688449   \n",
       "90            51  0.709621  [0.36363637 0.21906693 0.30514097]  0.688935   \n",
       "91            51  0.715127  [0.34666663 0.22222224 0.30842608]  0.694244   \n",
       "\n",
       "                       val_fbeta_score activation_layer  activity_regularizer  \\\n",
       "8      [0.8497409 0.6557377 0.796875 ]             tanh                0.0001   \n",
       "4   [0.83902436 0.69230765 0.768     ]             tanh                0.0001   \n",
       "3   [0.84313726 0.7083333  0.8000001 ]             tanh                0.0001   \n",
       "5   [0.82901555 0.73015875 0.7777778 ]             tanh                0.0001   \n",
       "2   [0.83505154 0.6027397  0.81739134]             tanh                0.0001   \n",
       "..                                 ...              ...                   ...   \n",
       "89     [0.        0.        0.5078125]             tanh                0.0001   \n",
       "94  [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "92  [0.20979021 0.18390803 0.        ]             tanh                0.0001   \n",
       "90  [0.6836364  0.23809524 0.        ]             tanh                0.0001   \n",
       "91  [0.         0.24770641 0.        ]             tanh                0.0001   \n",
       "\n",
       "    batc_normalization  batch_size  bias_regularizer  dropout  epochs  \\\n",
       "8                 True          64            0.0001     0.08  100000   \n",
       "4                 True          64            0.0001     0.04  100000   \n",
       "3                 True          64            0.0001     0.03  100000   \n",
       "5                 True          64            0.0001     0.05  100000   \n",
       "2                 True          64            0.0001     0.02  100000   \n",
       "..                 ...         ...               ...      ...     ...   \n",
       "89                True          64            0.0001     0.89  100000   \n",
       "94                True          64            0.0001     0.94  100000   \n",
       "92                True          64            0.0001     0.92  100000   \n",
       "90                True          64            0.0001     0.90  100000   \n",
       "91                True          64            0.0001     0.91  100000   \n",
       "\n",
       "    first_neuron  hidden_layers  hidden_neuron kernel_initializer  \\\n",
       "8             55              3             50            uniform   \n",
       "4             55              3             50            uniform   \n",
       "3             55              3             50            uniform   \n",
       "5             55              3             50            uniform   \n",
       "2             55              3             50            uniform   \n",
       "..           ...            ...            ...                ...   \n",
       "89            55              3             50            uniform   \n",
       "94            55              3             50            uniform   \n",
       "92            55              3             50            uniform   \n",
       "90            55              3             50            uniform   \n",
       "91            55              3             50            uniform   \n",
       "\n",
       "    kernel_regularizer_l1  kernel_regularizer_l2 last_activation     lr  \n",
       "8                0.000001                0.00001         sigmoid  0.001  \n",
       "4                0.000001                0.00001         sigmoid  0.001  \n",
       "3                0.000001                0.00001         sigmoid  0.001  \n",
       "5                0.000001                0.00001         sigmoid  0.001  \n",
       "2                0.000001                0.00001         sigmoid  0.001  \n",
       "..                    ...                    ...             ...    ...  \n",
       "89               0.000001                0.00001         sigmoid  0.001  \n",
       "94               0.000001                0.00001         sigmoid  0.001  \n",
       "92               0.000001                0.00001         sigmoid  0.001  \n",
       "90               0.000001                0.00001         sigmoid  0.001  \n",
       "91               0.000001                0.00001         sigmoid  0.001  \n",
       "\n",
       "[100 rows x 20 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448040d",
   "metadata": {},
   "source": [
    "## Cross-walidacja NN klasy T!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cb775",
   "metadata": {},
   "source": [
    "# dr = 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3428a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':55,  #Done\n",
    "     'hidden_neuron':50, #Done\n",
    "\n",
    "     'hidden_layers':3,   #Done\n",
    "\n",
    "     \n",
    "    'epochs': 100000, # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': 'sigmoid', #never touch it\n",
    "\n",
    "\n",
    "    'batch_size':64, #Done\n",
    "\n",
    "    'lr':0.001,\n",
    "    \n",
    "    'kernel_regularizer_l1':0.000001,#Done\n",
    "    'kernel_regularizer_l2':0.00001,#Done\n",
    "    'bias_regularizer':0.0001,#Done\n",
    "    'activity_regularizer':0.0001,#Done\n",
    "\n",
    "    'dropout': 0.08,\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': 'uniform',#Done\n",
    "\n",
    "    'activation_layer':'tanh',#Done\n",
    " \n",
    "    'batc_normalization':True#Done\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c58e0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(3, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adagrad(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=3,beta=1.0)])\n",
    "    \n",
    "  \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        class_weight={0 : 0.64147775,\n",
    "                                      1 : 2.42539683,\n",
    "                                      2 : 0.97201018},\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10f2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c364f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b3c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57abb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28268bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86012f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98646e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sprobowac zarowno na dropoucie 0,1 jak i 0.3 (0.1 raczj lepszy bo nizsze val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61b8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "052937da",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron':55,  #Done\n",
    "     'hidden_neuron':50, #Done\n",
    "\n",
    "     'hidden_layers':3,   #Done\n",
    "\n",
    "     \n",
    "    'epochs': 100000, # never touch it\n",
    "\n",
    "\n",
    "    'last_activation': 'sigmoid', #never touch it\n",
    "\n",
    "\n",
    "    'batch_size':64, #Done\n",
    "\n",
    "    'lr':0.001,\n",
    "    \n",
    "    'kernel_regularizer_l1':0.000001,#Done\n",
    "    'kernel_regularizer_l2':0.00001,#Done\n",
    "    'bias_regularizer':0.0001,#Done\n",
    "    'activity_regularizer':0.0001,#Done\n",
    "\n",
    "    'dropout': 0.3,\n",
    "    \n",
    "  \n",
    "    'kernel_initializer': 'uniform',#Done\n",
    "\n",
    "    'activation_layer':'tanh',#Done\n",
    " \n",
    "    'batc_normalization':True#Done\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6f2f1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "               \n",
    "                    kernel_initializer = params['kernel_initializer'],\n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if params['dropout']!=0:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                        kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                                    l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                       ))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if params['dropout']!=0:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(3, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer'],\n",
    "                   kernel_regularizer=keras.regularizers.l1_l2(l1=params['kernel_regularizer_l1'],\n",
    "                                                               l2=params['kernel_regularizer_l2']),\n",
    "                    bias_regularizer=keras.regularizers.l2(params['bias_regularizer']),\n",
    "                    activity_regularizer=keras.regularizers.l2(params['activity_regularizer'])\n",
    "                   ))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adagrad(learning_rate=params['lr']),\n",
    "                  metrics=[tfa.metrics.FBetaScore(num_classes=3,beta=1.0)])\n",
    "    \n",
    "  \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        class_weight={0 : 0.64147775,\n",
    "                                      1 : 2.42539683,\n",
    "                                      2 : 0.97201018},\n",
    "                        callbacks = [\n",
    "                                     EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0.01,\n",
    "                                        patience=50, mode='min',verbose=1,\n",
    "                                                      restore_best_weights=True)\n",
    "                                    ] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e2a04678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.08, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A048B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38D395E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 77.\n",
      "Epoch 00127: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A38747D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.08, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A392BCB558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A397051DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 123.\n",
      "Epoch 00173: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A39734D678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.08, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38FD4A798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A396D17C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 102.\n",
      "Epoch 00152: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A387817708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.08, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A391800A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3875FF5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "Epoch 00126: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A38D289C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.08, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A39753DCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A3801DE708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 00051: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A389115AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "dane_do_zapisu=pd.DataFrame()\n",
    "licznik=1\n",
    "\n",
    "for train, test in kfold.split(caly_df,train_label_2): # train 80, test 20 %\n",
    "    train, val = train_test_split(train, test_size=0.1, random_state=42) # train 80 % test 10% val 10 %\n",
    "    \n",
    "    returny=numerai_model(caly_df[train], caly_label[train], caly_df[val],caly_label[val], p)\n",
    "    \n",
    "    model=returny[1]\n",
    "    # evaluate the model\n",
    "\n",
    "    \n",
    "    predictions = model.predict(caly_df[test])\n",
    "    rezults=[]\n",
    "    w_0=[]\n",
    "    w_1=[]\n",
    "    w_2=[]\n",
    "    for z in predictions:\n",
    "        rezults.append(np.argmax(z))\n",
    "        w_0.append(z[0])\n",
    "        w_1.append(z[1])\n",
    "        w_2.append(z[2])\n",
    "    \n",
    "\n",
    "\n",
    "    scores=f1_score(train_label_2[test], rezults,average='macro')\n",
    "    \n",
    "    dane_folda=pd.DataFrame({\"pred\": list(rezults),\n",
    "                                \"obs\": train_label_2[test].tolist(),\n",
    "                                 \"probability_0\":list(w_0),\n",
    "                                 \"probability_1\":list(w_1),\n",
    "                                 \"probability_2\":list(w_2),\n",
    "                                \"Fold\":licznik})\n",
    "    \n",
    "    dane_do_zapisu=pd.concat([dane_do_zapisu,dane_folda])\n",
    "    cvscores.append(scores)\n",
    "    licznik+=1\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9722da35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4679096045197739,\n",
       " 0.4600454112649235,\n",
       " 0.5459252649794505,\n",
       " 0.45139542996685855,\n",
       " 0.17120622568093383]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "427a0799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41929638728238805"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cvscores)/len(cvscores) #0.032977654217233775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e8a3e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(dane_do_zapisu['obs'], dane_do_zapisu['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "88158b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='g')\n",
    "\n",
    "ax.set_title('Dane pierwotne - Klasa T - NN')\n",
    "plt.ylabel('Rzeczywiste')\n",
    "plt.xlabel('Predykcja')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba599ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'],  average='macro')\n",
    "recall = recall_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'],  average='macro')\n",
    "f1score=f1_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'], average='macro')\n",
    "print('Recall: %.3f' % recall)\n",
    "print('Precision: %.3f' % precision)\n",
    "print('F1score: %.3f' % f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cb821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ac46a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/Dane_do_uczenia_T.csv\", encoding=\"utf-8\")\n",
    "del train_df['Unnamed: 0']\n",
    "train_df['scale'].loc[(train_df['scale'] == 'T1')] = 0\n",
    "train_df['scale'].loc[(train_df['scale'] == 'T2')] = 1\n",
    "train_df['scale'].loc[(train_df['scale'] == 'T3')] = 2\n",
    "train_label=train_df['scale']\n",
    "del train_df['scale']\n",
    "scaler = StandardScaler()\n",
    "train_df=scaler.fit_transform(train_df)\n",
    "train_label=np.asarray(train_label).astype(np.int)\n",
    "train_label_2=train_label\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_label)\n",
    "train_label = encoder.transform(train_label)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "\n",
    "train_label = np_utils.to_categorical(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6415f074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15545413, -0.19268726, -0.18007555, ..., -0.11325408,\n",
       "        -0.1310684 , -0.14934876],\n",
       "       [-0.1740803 , -0.20813705, -0.23130506, ..., -0.20152997,\n",
       "        -0.17992108, -0.14934876],\n",
       "       [-0.19408164, -0.21062068, -0.23130506, ..., -0.20152997,\n",
       "        -0.17992108, -0.14934876],\n",
       "       ...,\n",
       "       [-0.21070965, -0.21834049, -0.23130506, ..., -0.20152997,\n",
       "        -0.17992108, -0.14934876],\n",
       "       [ 0.00099515, -0.02176597, -0.23130506, ..., -0.20152997,\n",
       "        -0.17992108, -0.14934876],\n",
       "       [ 0.03374514, -0.21834049, -0.23130506, ..., -0.20152997,\n",
       "        -0.08266341, -0.14934876]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a7d17e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(955, 105)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b2776564",
   "metadata": {},
   "outputs": [],
   "source": [
    "caly_df=train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "28f8e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "caly_label=train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e7de85a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2ad52e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4abcdf1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.3, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A39010A048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38152E4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 184.\n",
      "Epoch 00234: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A38E723A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.3, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A38D322438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38CE51318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 86.\n",
      "Epoch 00136: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A38152E0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.3, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A392B63288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38D2364C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 172.\n",
      "Epoch 00222: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A38FD949D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.3, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3902E8A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A395A04558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 177.\n",
      "Epoch 00227: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A395A5CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'first_neuron': 55, 'hidden_neuron': 50, 'hidden_layers': 3, 'epochs': 100000, 'last_activation': 'sigmoid', 'batch_size': 64, 'lr': 0.001, 'kernel_regularizer_l1': 1e-06, 'kernel_regularizer_l2': 1e-05, 'bias_regularizer': 0.0001, 'activity_regularizer': 0.0001, 'dropout': 0.3, 'kernel_initializer': 'uniform', 'activation_layer': 'tanh', 'batc_normalization': True}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "adding layer 3\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A395A044C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001A38152EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 00088: early stopping\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A3801DEAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "dane_do_zapisu=pd.DataFrame()\n",
    "licznik=1\n",
    "\n",
    "for train, test in kfold.split(caly_df,train_label_2): # train 80, test 20 %\n",
    "    train, val = train_test_split(train, test_size=0.1, random_state=42) # train 80 % test 10% val 10 %\n",
    "    \n",
    "    returny=numerai_model(caly_df[train], caly_label[train], caly_df[val],caly_label[val], p)\n",
    "    \n",
    "    model=returny[1]\n",
    "    # evaluate the model\n",
    "\n",
    "    \n",
    "    predictions = model.predict(caly_df[test])\n",
    "    rezults=[]\n",
    "    w_0=[]\n",
    "    w_1=[]\n",
    "    w_2=[]\n",
    "    for z in predictions:\n",
    "        rezults.append(np.argmax(z))\n",
    "        w_0.append(z[0])\n",
    "        w_1.append(z[1])\n",
    "        w_2.append(z[2])\n",
    "    \n",
    "\n",
    "\n",
    "    scores=f1_score(train_label_2[test], rezults,average='macro')\n",
    "    \n",
    "    dane_folda=pd.DataFrame({\"pred\": list(rezults),\n",
    "                                \"obs\": train_label_2[test].tolist(),\n",
    "                                 \"probability_0\":list(w_0),\n",
    "                                 \"probability_1\":list(w_1),\n",
    "                                 \"probability_2\":list(w_2),\n",
    "                                \"Fold\":licznik})\n",
    "    \n",
    "    dane_do_zapisu=pd.concat([dane_do_zapisu,dane_folda])\n",
    "    cvscores.append(scores)\n",
    "    licznik+=1\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b8fa880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dane_do_zapisu.to_csv('D:\\\\STUDIA\\\\ROK_II\\\\Magisterka\\\\Modele\\\\Dane pierwotne\\\\Dane z cross-walidacji\\\\Klasa_T_NN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "959c15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b28e7b5",
   "metadata": {},
   "source": [
    "## dr 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5df1168d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4730920393779641,\n",
       " 0.4403567787349129,\n",
       " 0.5193394371175799,\n",
       " 0.4537548931044866,\n",
       " 0.4958286848050628]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a7197ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47647436662800124"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cvscores)/len(cvscores) #0.032977654217233775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6693d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(dane_do_zapisu['obs'], dane_do_zapisu['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c2d01911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEWCAYAAABG030jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqjUlEQVR4nO3deZgU1fn28e8NAwiCQQUEAXGDqLjgvhs3xPWHSdQX40LUBE3QmGjivkYxJnGPKy7RGJSguOAuLrjviqCigoqAgICKLBpkZp73j6rBFmd6GpmenurcH6+6pvrUqaqnZ/Dp06dOnVJEYGZm2dGs1AGYmdmyceI2M8sYJ24zs4xx4jYzyxgnbjOzjHHiNjPLGCfu/0GSrpV0ZqnjaKokrSkpJFWUOhaz2jhxNxBJkyV9LWm+pLmSnpd0jKQm9zuOiGMi4rxSxiBpjKRflfD8kyXtnvN6gKQvJP2kBLFcK2lBunwjaXHO64d+4DFD0vjcf3+Szpd0c7pe8+H0wFL7/VvSOcvzfqz4mlxSybj9IqId0AO4EDgZuLG0ITWccm2BShoIXAXsExFPNfb50w/SthHRFrgA+E/N64jYazkOvTowoJ4620jafjnOYSXgxF0EEfFlRIwC/h8wUNKGAJL2kfSGpHmSpua2bHJaQAMlTZE0R9LpOdubSTpF0geSPpM0QtIqtZ1f0s6Spkk6LT3OZEmH5Gy/WdL5Oa/3lTQ255vCxjnbJks6WdI4YKGkX0u6L2f7JEkjcl5PldQnXd9O0iuSvkx/bpeWDwF2BK5MW5VXpuWRfkuZmLZ+r5KknGMfKWlCuu0RST2W+Y/z/d/VIOBioF9EPF9HnSPS886X9KGko3O2dZB0f/q7+1zSMzWt3Jy/13xJ70j66fLGu4z+Bpxbzwfu34Dz82y3pigivDTAAkwGdq+lfArwm3R9Z2Ajkg/MjYFPgf3TbWsCAVwPtAY2ARYB66fbfw+8CHQDWgHXAbfXEcvOQCVwSVr3J8BC4Mfp9puB89P1zYBZwNZAc2Bg+l5a5byvsUD3NK61gbnpe+gCfAx8ktZdG/gi3bZKun4YUAEcnL5eNa07BvjVUnEHcD/QHlgDmA3smW7bH5gErJ8e7wzg+eX8e41M/wabLLWt5m9Rkb7eB1gHUPq7/ArYLN32F+BaoEW67Ago3XYgSau3GcmH+EKgSz1xnQP8uwH+PQbQE3it5vdMkqBvXuo9tgU+If23C/wbOKfU/z95yb+4xV1800mSGBExJiLGR0R1RIwDbidJBLnOjYivI+JN4E2SBA5wNHB6REyLiEUk/4MfUE9r6syIWBTJ1/8HgINqqfNr4LqIeCkiqiLiFpIPjG1y6lwREVPTuD4E5gN90tgfAT6RtF76+pmIqCZJdhMj4taIqIyI24F3gf3q+X1dGBFzI2IK8GR6npr3/5eImBARlSRdCn2Ws9Xdl+TDcHy+ShHxQER8EImngEdJEjTAYpIPsB4RsTginolIM2fEHRExPf17/weYCGy1HPEuqwDOBM6S1KqOOv8FhuBWd6Y4cRdfV+BzAElbS3pS0mxJXwLHAB2Wqj8zZ/0rkhYRJP3md6dfyecCE4AqYLU6zvtFRCzMef0xSetvaT2AE2uOmx67+1J1py61z1Mkrfqd0vUxJEn7J+lr0v0/Xmq/j0l+H/nke/+X58T4OUkL+HvHW+pi32l5znUM0Au4IbdLppbj7SXpxbQrZC6wN9/+3f5O8k3g0bQb5ZSc/Q7P6YKaC2zI9//ey0zSQznv75B8dSPiQZJvfYPyVLseWE1SfR+q1kQ4cReRpC1JEsuzadFtwCige0T8iOQrdp0JYylTgb0ion3OskJEfFJH/ZUlrZjzeg2S1n9txx2y1HHbpC3kGktPIVmTuHdM15/i+4l7OkmyzbUGydfy2o5Zn6nA0UvF2Tpq6ZeOnIt9EXFBnmPOAnZL38fVtVVIW6ojgYuA1SKiPfAg6d8tIuZHxIkRsTbJt4kTJO2WfhO4HjiWpHuoPfAWhf+96xQRe+W8v2EF7HIGcDrQpo7jLQbOBc5riPis+Jy4i0DSSpL2BYaT9FfWfBVvB3weEf+VtBXwi2U47LXAkJquAUkdJfWvZ59zJbWUtCOwL3BHLXWuB45Jvw1I0opKLqK2y3Pcp4BdgNYRMQ14BtgTWBV4I63zINBL0i8kVUj6f8AGJH3YkPQtr13vu/7WtcCpknoDSPqRpAOXYf9aRcR0YFdgT0mX1lKlJcl1gtlApaS9gD1qNqYXdtdNW+zzSL4FVQErknw4zU7rHUHS4m50ETGGpDtoYJ5qt5K8zz0bIyZbPk7cDes+SfNJWoenk1wcPCJn+2+BP6d1zgJGfP8QdbqcpLX+aLr/iyQXFOsyk+Ri4HRgGHBMRLy7dKWIeJWkn/vKtP4k4Jf5AomI94EFJAmbiJgHfAg8FxFVadlnJB8WJwKfAScB+0bEnJz3c0A6QuSK+t58RNwN/BUYLmkeSet1eYbK5R57KknyPkDSX5baNh/4Hcnf6guSD9tROVV6Ao+R/D5eAK5Or2W8QzJa5QWSD6mNgOcaIt4f6AzSay21Sf9uZ+erY01HzdVvKyOSdiZp6XcrcShmVgRucZuZZYwTt5lZxrirxMwsY9ziNjPLmCY7adDiOR/6q0CRtV9j11KHUPZ6t1/u6VSsAC9Pf2q5x58vS85p0WHtko53d4vbzCxjmmyL28ysUVVXlTqCgjlxm5kBVFWWOoKCOXGbmQHJpJbZ4MRtZgZQ7cRtZpYtbnGbmWWML06amWWMW9xmZtkSHlViZpYxvjhpZpYx7ioxM8sYX5w0M8sYt7jNzDLGFyfNzDLGFyfNzLIledB9Njhxm5mB+7jNzDLHXSVmZhnjFreZWcZULS51BAVz4jYzg0x1lfhhwWZmkHSVFLrkIWkFSS9LelPS25LOTcvPkfSJpLHpsnfOPqdKmiTpPUn96gvVLW4zM2jIFvciYNeIWCCpBfCspIfSbZdGxEW5lSVtAAwAegOrA49J6hV5xie6xW1mBkniLnTJIxIL0pct0iXy7NIfGB4RiyLiI2ASsFW+czhxm5kBUbW44EXSIEmv5iyDco8lqbmkscAsYHREvJRuOlbSOEk3SVo5LesKTM3ZfVpaVicnbjMzWKY+7ogYGhFb5CxDv3OoiKqI6AN0A7aStCFwDbAO0AeYAVycVldt0eQL1YnbzAwarKskV0TMBcYAe0bEp2lCrwau59vukGlA95zdugHT8x3XidvMDBpyVElHSe3T9dbA7sC7krrkVPsp8Fa6PgoYIKmVpLWAnsDL+c7hUSVmZtCQo0q6ALdIak7SOB4REfdLulVSH5JukMnA0QAR8bakEcA7QCUwON+IEnDiNjNLNNAt7xExDti0lvLD8uwzBBhS6DmcuM3MACr9IIWys2jRNwwc/Ce+WbyYqsoq+u6yA8f+6jDenfgh5/39H3z19X9ZvUsn/nr2SbRdcUWef/l1Lrv2nyxeXEmLFhWcOPgott68T6nfRma0atWKR0f/h1YtW9G8ojn33PMQQ86/lJVX/hH/+teVrNGjG1M+nsZhhw1m7tx5pQ43s9ZYpzsXXHv2kterr7E6Q/9+E8NvuBOAQ475fxx/1m/pu+H/8eXnX5YqzMbhSabKT8uWLbjpigtp06Y1iysrOfw3f2THbbbggkuv4Y/H/ootN92Yu+5/hH8OG8lxgw5n5fYrceVfz6FTx1WZ+OFkjv7DGTxx779L/TYyY9GiRey91y9YuPArKioqeOzxO3n0kTH079+PMWOe5+KLr+HEE3/DiSf+ljPPvLDU4WbWlA+mcmjfXwHQrFkzHnj9TsY89AwAnVbvyNY7bcGMaTNLGWLj8Vwl5UcSbdq0BqCyspLKykokMXnKNLbosxEA2265GaOfehaA9XutS6eOqwKw7lo9WPTNN3zzzTelCT6jFi78CoAWLSpo0aKCINhn374MG5a0BocNu5N99+tbyhDLypY7bsa0j6cz85NPAfjDOcfyj/OvJSLvkOLy0UCjShqDE/cyqKqq4ucDB7PTvgez7ZabsnHv9Vh37TV58tkXAXj0yWeY+emc7+03esyzrN9rHVq2bNnYIWdas2bNeOHFB5n88Ws88fizvPrKWDp16sjMmbMBmDlzNh07dihxlOWjb//dePSexwHYcY/tmD1zDhPf+aDEUTWiIozjLpaiJW5J60k6WdIVki5P19cv1vkaQ/PmzRl5y1U8fvetjH/nfSZ+OJnzTvsDt4+8j4OOPI6FX31Nixbf7X2a9OHHXHL1TZz1p+NKFHV2VVdXs+02e9Or57ZsvsUmbLBBr1KHVLYqWlSw0x7b8fh9Y2jVuhVH/O4wrvv7TaUOq3H9r7e4JZ0MDCe5lfNl4JV0/XZJp+TZb8n9/zf86/ZihNYgVmrXli0325hnX3yVtXt05/rLLmDETf9g791/Qveu346xnzlrNsefdh4XnPlH1ui2egkjzrYvv5zHM8+8SN++P2HWrNl07twRgM6dOzJ79ve/4diy227XrXl3/EQ+n/MF3Xp0ZfU1ujDssRu556XhdOrSkVsfuZ5VO65S6jCLq7Ky8KXEinVx8iigd0R855ESki4B3gZqvZqU3u8/FGDxnA+bVMfa51/MpaKigpXateW/ixbx4itvcOShB/LZF3NZdeX2VFdXc90twzlo/2SK3XnzF/DbP53N74/+JZtt3LvE0WdPhw6rsHhxJV9+OY8VVmjFLrtszyWXXMuDDzzGIYccwMUXX8MhhxzAA/ePLnWoZWGP/b/tJvng3Q/Zc+P9l2y756XhDNzr6P+BUSVNKuXkVazEXU0yr+zHS5V3SbdlzuzPvuD08y+iqrqaqA767bojO2+/NbeOuIfhd90PwO4/2Y6f7rMHALePvI+p06Zz7c23c+3NybeHoZcNYdWV25fqLWRK586dGHr9xTRv1oxmzZox8q4HePihJ3j5pde59darOHzgQUybOp1DD/1tqUPNvFatW7H1jlvwl5Murr9yOWsCfdeFUjGuGEvaE7gSmMi30xWuAawLHBsRD9d3jKbW4i5H7dfYtdQhlL3e7XuUOoT/CS9Pf6q2GfaWydfDziw457Q+5LzlPt/yKEqLOyIeltSLZParriT929OAV+q7B9/MrCSawEXHQhXtBpx06sIXi3V8M7MGVZWdNqXvnDQzg0z1cTtxm5mBE7eZWea4j9vMLFuiOjsD2Zy4zczAXSVmZpnjUSVmZhnjFreZWcZkKHF7Pm4zM0gmmSp0yUPSCpJelvSmpLclnZuWryJptKSJ6c+Vc/Y5VdIkSe9J6ldfqE7cZmbQkA9SWATsGhGbAH2APSVtA5wCPB4RPYHH09dI2gAYAPQG9gSultQ83wmcuM3MAKqj8CWPSCxIX7ZIlwD6A7ek5bcA+6fr/YHhEbEoIj4CJpHM81QnJ24zM0hGlRS45D70JV0G5R5KUnNJY4FZwOiIeAlYLSJmAKQ/O6XVu/LtLKqQTMjXNV+ovjhpZgbEMlyczH3oSx3bq4A+ktoDd0vaMM/hapsiNm+z3i1uMzNosK6SXBExFxhD0nf9qaQuAOnPWWm1aUD3nN26AdPzHdeJ28wMGuxhwZI6pi1tJLUGdgfeBUYBA9NqA4F70/VRwABJrSStBfQkeVZvndxVYmYGy9SSrkcX4JZ0ZEgzYERE3C/pBWCEpKOAKcCBABHxtqQRwDtAJTC4vgfOOHGbmQFUNswt7xExDti0lvLPgN3q2GcIMKTQczhxm5mBp3U1M8scT+tqZpYtyzIcsNScuM3MwC1uM7PMceI2M8sYP0jBzCxb/MxJM7OsceI2M8sYjyoxM8sYt7jNzDLGidvMLFuiyl0ly22rDQ8rdQhlb1Hl4lKHUPamfDWr/krWNLjFbWaWLR4OaGaWNU7cZmYZk50ubiduMzOAqMxO5nbiNjMDt7jNzLLGFyfNzLImQy3uZqUOwMysKYjqKHjJR1J3SU9KmiDpbUnHp+XnSPpE0th02Ttnn1MlTZL0nqR+9cVacItbUg+gZ0Q8Jqk1UBER8wvd38ysSWu4FnclcGJEvC6pHfCapNHptksj4qLcypI2AAYAvYHVgcck9YqIOicIL6jFLenXwJ3AdWlRN+CeZXknZmZNWVQWvuQ9TsSMiHg9XZ8PTAC65tmlPzA8IhZFxEfAJGCrfOcotKtkMLA9MC8NZiLQqcB9zcyavKgufCmUpDWBTYGX0qJjJY2TdJOkldOyrsDUnN2mkT/RF5y4F0XENznBVADZuQRrZlaf6sIXSYMkvZqzDFr6cJLaAiOB30fEPOAaYB2gDzADuLimai3R5M2vhfZxPyXpNKC1pL7Ab4H7CtzXzKzJW5aWdEQMBYbWtV1SC5KkPSwi7kr3+TRn+/XA/enLaUD3nN27AdPznb/QFvcpwGxgPHA08GBEnF7gvmZmTV5DdZVIEnAjMCEiLskp75JT7afAW+n6KGCApFaS1gJ6Ai/nO0ehLe7jIuJy4PqcII5Py8zMMi+qauux+EG2Bw4Dxksam5adBhwsqQ9JN8hkkkYwEfG2pBHAOyQjUgbnG1EChSfugcDSSfqXtZSZmWXSsnSV5D1OxLPU3m/9YJ59hgBDCj1H3sQt6WDgF8BakkblbFoJ+KzQk5iZNXVR3WAt7qKrr8X9PMnVzw58ewUUYD4wrlhBmZk1toZqcTeGvIk7Ij4GPpa0O/B1RFRL6gWsR3Kh0sysLERkp8Vd6KiSp4EVJHUFHgeOAG4uVlBmZo2tGDfgFEuhiVsR8RXwM+AfEfFTYIPihWVm1riqq1TwUmqFjiqRpG2BQ4CjlnFfM7Mmr5wuTtb4PXAqcHc65nBt4MmiRWVm1sjKLnFHxFPAUzmvPwR+V6ygzMwaW2Ro9qX6xnFfFhG/l3QftUx6EhH/V7TIzMwaUTm1uG9Nf16Ut5aZWcZlaThgfeO4X0tXmwMvpiNLzMzKTlUTGC1SqEIvTv4SuFbSZ8Az6fJsRHxRrMDMzBpT2bS4a0TE4QCSVgcOAK4ieTaahwSaWVkopz5uACQdCuwIbATMAa4kaXWbmZWFshlVkuMy4APgWuDJiJhcrIDMzEqh7FrcEdFBUm9gJ2CIpJ7AexFxWFGjMzNrJFXVhc4AUnoFRSppJWANoAewJvAjksdm/s9qu1Jb/n7D+dz1zG2MfHoYG2/em169e3LLA0MZ/tjNDHvkRnpvun6pw8ysbt1W57FH72D8uDG8OfYJjjs2mWnh5z/flzfHPsE3/53K5pttXOIoy0OzZs0Y/fRIbh1+DQAnnf47nnjuHh575i6G33UDq3XuWOIIG0dE4UupKQqIQtI44Nl0eToiphU7sE07b98Efj11+/MVZ/DGi29y9233UdGighVar8Dfhp7HsKH/4bknXmSH3bZl4OBf8OufHVfqUOs0/vPJpQ6hTp07d6JL5068MfYt2rZdkZdfepifH3AkEUF1dXDNVRdy0snn8drrTXta+A5tVip1CPU6evBANumzIe3ateWwAb+hbbsVWTB/IQBHHX0ovX68DiefcG6Jo8xv5twJy93PMbbH/xWcc/p8PKqk/SqFfjc4JCJ+GxG3NUbSbupWbNuGzbbZhLtvSx50X7m4kgXzFhARrNhuRQDatluR2TPnlDLMTJs5cxZvjE2epbpgwULefXciXVfvzLvvTuL99z8ocXTlo8vqq7H7Hj9h2K13LimrSdoAbdq0ruWe6fIUoYKXUiv04uQ1klqSzMF9W0TMLVpEGdC1R1e++Gwu515+Or02WJcJ497jb2dexkVnXc5Vt1/CH84aTLNmzfjlfkeXOtSy0KNHN/pssiEvvfxGqUMpO+f95VTOO+si2qYNjhqnnHE8Bw7oz/x5C/j5fgNLFF3jagpdIIUqqMUdETsAhwLdgVcl3Sap7w85oaQj8mwbJOlVSa/O+WrmDzl8o6ioaM56G/Xijpvv5uC+R/D1V19z5LGHceDAn3Lx2f9gr81/xkVnX8HZl5xa6lAzb8UV2zDiP9dzwh/PZv78BaUOp6z07bczc2Z/zrg33/netgvPv5zNN9yVkXfcx5GDDilBdI2vOlTwUmoFX0aNiPeBM4CTgZ8AV0h6V9LPlvGcdXaWRcTQiNgiIrbo0KbzMh628Xw6fRazZszmrTeSf/CP3T+G9Tbuxb4H7cXjD4wBYPSoJ+i9qZ81sTwqKiq44z/Xc/vtd3PPPQ+VOpyys+XWm7LHXrvwyrjHuPbGi9l+p6258rq/fqfO3Xc+wD777VGiCBtXVXWzgpd8JHWX9KSkCZLelnR8Wr6KpNGSJqY/V87Z51RJkyS9J6lffbEWOqpkY0mXAhOAXYH9ImL9dP3SWuqPq2MZD6xWyDmbss9mf87MT2bRY501ANhqx8358P3JzJ45h8232zQp22Fzpnw4tZRhZt71Qy9mwruTuOzyoaUOpSxd8OdL2az3Lmy58e4cc9SJPPf0Sxx79MmstXaPJXX67bULkyZ+WMIoG08sw1KPSuDENEduAwyWtAFwCvB4RPQkeQTkKQDptgFAb2BP4GpJzfOdoNA+7iuB64HTIuLrJW80YrqkM2qpvxrQD1h6LhORPDk+8/56+qVccPXZVLSo4JOPp3P27y9gzMPP8KfzjqeiojmLFn3D+X/6W6nDzKztt9uSww49gHHj3+HVVx4F4MwzL6Rlq5Zcfun5dOy4CqPu/Rdvvvk2e+/7v/FVvrGcfs4JrLvuWlRHNdOmTuekP5xT6pAaRUN1gUTEDGBGuj5f0gSgK9Af2DmtdgswhqQHoz8wPCIWAR9JmgRsBbxQ1zkKHQ54OMnTb+bnlO0bEffXUf9G4J8R8Wwt226LiF/Ud86mPhywHDTl4YDlIgvDActBQwwHfK7zAQXnnB0+HXk0MCinaGhEfO+roaQ1SR62viEwJSLa52z7IiJWlnQlyeyr/07LbwQeiog7lz5ejUJb3FcAJ0g6OCImpGV/BmpN3BFxVG3l6bZ6k7aZWWNbljsK0ySdtw9PUltgJPD7iJgn1fnZUtuGvB8ihV6c/Ag4ErhT0oF5TmZmlkmBCl7qI6kFSdIeFhF3pcWfSuqSbu8CzErLp5GM2KvRDZie7/iFJu6IiNdJRpMMknQRycMVzMzKQmWo4CUfJU3rG4EJEXFJzqZRQM2g+IHAvTnlAyS1krQW0BN4Od85Ck3cNR3tc0guOgZJn42ZWVlowBb39sBhwK6SxqbL3sCFQF9JE4G+6Wsi4m1gBPAO8DAwOCKq8p2g0NkB98lZrwb+lC5mZmWhoWbNSwdl1JXdd6tjnyHAkELPUeg47tGS2ue8XlnSI4WexMysqWvIPu5iK3RUScfc+Uki4gtJmb+RxsysRpbmqS40cVdJWiMipgBI6kG23qeZWV5VTaAlXahCE/fpwLOSnkpf78R3B5+bmWVahp5cVvDFyYclbUZy372AP6QjTMzMykJ1hlrchV6cFMnkJ5tFxH1AG0lbFTUyM7NG1ICTTBVdoeO4rwa2BQ5OX88HripKRGZmJVC9DEupFdrHvXVEbCbpDVgyqqRlEeMyM2tU1XXPJdLkFJq4F6fzwwaApI40jQ8eM7MGkfdWxSZmWWYHvBvoJGkIcADJ03DMzMpCOY4qGSbpNZLbNQXsnzO9q5lZ5pXjqJIbgRUi4qqIuDIiJkg6p7ihmZk1nnIcVdIPuDl9Ek6N/ytCPGZmJVGtwpdSKzRxzyK5W/JASVdJqsAPUjCzMpKl4YCFJm5FxLyI2A+YDTwF/Kh4YZmZNa4qFb6UWqGjSkbVrETEOZJeBU4oTkhmZo2vKbSkC1XoqJKzlyp6CLe4zayMZClx5+0qkbSSpFMlXSlpDyWOAz4EDmqcEM3Mii9U+FJq9bW4bwW+AF4AfkXyuLKWQP+IGFvc0MzMGk+WWtz1Je61I2IjAEk3AHOANSJiftEjMzNrRFm65b2+USWLa1bSpw5/5KRtZuWoIcdxS7pJ0ixJb+WUnSPpk6We/F6z7VRJkyS9J6lffcevr8W9iaR5NccGWqevBURErFT/WzAza/oauKvkZuBK4F9LlV8aERflFkjaABgA9AZWBx6T1CttLNcqb+KOiOY/JGIzs6xpyMQdEU9LWrPA6v2B4RGxCPhI0iRgK5Jri7Uq9AYcM7OytixzlUgaJOnVnKXQZ/AeK2lc2pWyclrWFZiaU2daWlYnJ24zM5atjzsihkbEFjnL0AJOcQ2wDtAHmAFcnJbX1muedy6rQu+cNDMra8UeVRIRn9asS7oeuD99OQ3onlO1GzA937GabOLee4UepQ6h7H3QYkapQyh7p7fbvNQhWIGqizxhq6QuEVHzP91PgZoRJ6OA2yRdQnJxsifwcr5jNdnEbWbWmBry4qSk24GdgQ6SpgFnAztL6kPSDTIZOBogIt6WNAJ4B6gEBucbUQJO3GZmQMM+ICEiDq6l+MY89YcAQwo9vhO3mRnldcu7mdn/hEo1hYeSFcaJ28yMpvEsyUI5cZuZ4a4SM7PMKfZwwIbkxG1mhrtKzMwyx10lZmYZU5WhNrcTt5kZbnGbmWVOuMVtZpYtbnGbmWWMhwOamWVMdtK2E7eZGQCVGUrdTtxmZvjipJlZ5vjipJlZxrjFbWaWMW5xm5llTFW4xW1mlilZGsfdrNQBmJk1BbEM/9VH0k2SZkl6K6dsFUmjJU1Mf66cs+1USZMkvSepX33Hd+I2MyPp4y50KcDNwJ5LlZ0CPB4RPYHH09dI2gAYAPRO97laUvN8B3fiNjMj6SopdKlPRDwNfL5UcX/glnT9FmD/nPLhEbEoIj4CJgFb5Tu+E7eZGcvWVSJpkKRXc5ZBBZxitYiYAZD+7JSWdwWm5tSblpbVyRcnzcxYtlElETEUGNpAp1Ztp8i3gxO3mRmNMqrkU0ldImKGpC7ArLR8GtA9p143YHq+A7mrxMyMBr84WZtRwMB0fSBwb075AEmtJK0F9ARezncgt7jNzGjYW94l3Q7sDHSQNA04G7gQGCHpKGAKcCBARLwtaQTwDlAJDI6IqnzHd+I2M6Nhu0oi4uA6Nu1WR/0hwJBCj+/EXaCf/W0QP951UxZ+No8r+p0MwO4nHMj6fTcnopoFc+Yx8o/XMn/WXABWW687+1/wK1q1bU1UV3NN/zOpXLS4hO8gW1q1asnDj/6Hlq1aUtG8Offe8zAXDLlsyfbjjv8VQy44jTXX2JzPP/uidIFm0G4X/Zo1d+vD15/N47bdTwVgqz/8jN6/2JmvP5sPwAt/HcHHT75Jr/23Y7Nj9lmyb4f1uzN8rzOY886UksReTOFb3svP63c+zYu3PMoBl/xmSdkzQ+/nsUvuAGDbX/Zj1+N/xr2n30Sz5s046NLB3HHC1cycMIXW7dtStbiyVKFn0qJF37Dv3oewcOFXVFRU8OhjIxj96BheeWUsXbt2Ydddd2DKlE9KHWYmTbjjacbdPJq+lx39nfKxNzzMG9c9+J2y9+95nvfveR6AVdfrxj43nFCWSRugyre8l5/JL7/LV18u+E7ZogVfL1lv0aYVNR/Y6+64MTPfncLMCck/8K/nLiCqs/OPoqlYuPArAFq0qKCiRcWSFtFf/noGZ55xYaZaSE3J9Jfe479zF9RfcSm9+m/H+6NeKEJETUND3oBTbG5xL6e+fzyIPj/bkUXzv+KGg88HoMPanYkIfvmvU1hxlXaMu+8Fnrnu/hJHmj3NmjXj6edGsfbaPbh+6L959dU32Wvv3ZgxYyZvjX+31OGVnY0H9mW9n+/ArHEf8ex5w1j05Vff2d5zv625/6hLSxRd8WWpIVC0Frek9STtJqntUuVL37+faaMvGsHftzuOsfc+x7YD9wCgWfPm9Njyx4w4/iqGHnAuG/TbkrW3613iSLOnurqaHbbdl/V7bcfmm29M7w3X408nDWbIeZeVOrSyM/7Wx/jXDidwe7/TWThrLjucech3tq/WZx0Wf/0Nn783rUQRFl+WWtxFSdySfkcyRvE44C1J/XM2X5BnvyW3kb4xf1IxQiuacfc+T+89k+kFvpz5OZNfmsBXX8xn8X+/4f0nx7L6hmuVOMLs+vLL+Tz7zEvss8/u9FizG8+9+ADj33marl0788xz99FptQ6lDjHzvp4zL+nOi+Dt255ktT5rf2d7z/7bMPHe8u0mgYadHbDYitXi/jWweUTsTzKW8UxJx6fbaru9E0huI42ILSJii03brVuk0BrOqmt2XrK+3u6bMfuD5GaniU+No/N6a9BihZY0a96MNbden9kTy7elUgyrdliFH/2oHQArrNCKnXfZnnFvvsM6a27FRhvsxEYb7MQnn8xkx+33Y9anc0ocbfa16dR+yfo6e27BZ7kta4me+2xd1v3bkNzyXuhSasXq424eEQsAImKypJ2BOyX1IE/ibsoOuuJY1t5mfdqs3I6TXvgHj186kl679KHj2l2I6mDuJ3O49/QbAfjvvIU8e8OD/GbU+RDBe0+O5b0nx5b2DWRM586duHbo32nevDnNmom7Rz7Iww8/UeqwykK/KwfTdZv1WWGVthzx8hW8dPFIum67Ph1694AI5k2bw5On3LSkftet12PBjM+ZN2V2CaMuvqbQBVIoFaNDXtITwAkRMTanrAK4CTgkIvLONQtw+pq/yM5vMaP+MfvFUodQ9oassl2pQ/ifcNzUfy93g3DbrrsUnHNe+OTJkjZAi9XiPpzk1s0lIqISOFzSdUU6p5nZD5alUSVFSdwRUWeHbkQ8V4xzmpktjyx1lXgct5kZDTvJVLE5cZuZAVWxHBO2NjInbjMz3MdtZpY57uM2M8sY93GbmWVMtbtKzMyyxS1uM7OM8agSM7OMcVeJmVnGuKvEzCxjGrLFLWkyMB+oAiojYgtJqwD/AdYEJgMHRcQPetK1nzlpZkZRHqSwS0T0iYgt0tenAI9HRE/g8fT1D+LEbWYGVEVVwcsP1B+4JV2/Bdj/hx7IidvMjOSW90KX3McspsugpQ8HPCrptZxtq0XEjPRcM4BOPzRW93GbmbFst7xHxFBgaJ4q20fEdEmdgNGS3l3e+HK5xW1mxrK1uAs41vT05yzgbmAr4FNJXQDSn7N+aKxO3GZmJKNKCl3ykbSipHY168AewFvAKGBgWm0gcO8PjdVdJWZmNOg47tWAuyVBkmNvi4iHJb0CjJB0FDAFOPCHnsCJ28yMhrvlPSI+BDappfwzYLeGOIcTt5kZfpCCmVnmeK4SM7OMcYvbzCxj/OgyM7OMcYvbzCxj/CAFM7OM8cVJM7OMcVeJmVnG+Ak4ZmYZ4xa3mVnGZKmPW1n6lGnqJA1K5+m1IvHvuPj8O276PK1rw1r6KRjW8Pw7Lj7/jps4J24zs4xx4jYzyxgn7oblfsHi8++4+Pw7buJ8cdLMLGPc4jYzyxgnbjOzjHHibgCS9pT0nqRJkk4pdTzlSNJNkmZJeqvUsZQrSd0lPSlpgqS3JR1f6pisdu7jXk6SmgPvA32BacArwMER8U5JAyszknYCFgD/iogNSx1POZLUBegSEa9Lage8Buzvf8tNj1vcy28rYFJEfBgR3wDDgf4ljqnsRMTTwOeljqOcRcSMiHg9XZ8PTAC6ljYqq40T9/LrCkzNeT0N/2O3jJO0JrAp8FKJQ7FaOHEvP9VS5v4nyyxJbYGRwO8jYl6p47Hvc+JeftOA7jmvuwHTSxSL2XKR1IIkaQ+LiLtKHY/Vzol7+b0C9JS0lqSWwABgVIljMltmkgTcCEyIiEtKHY/VzYl7OUVEJXAs8AjJxZwREfF2aaMqP5JuB14AfixpmqSjSh1TGdoeOAzYVdLYdNm71EHZ93k4oJlZxrjFbWaWMU7cZmYZ48RtZpYxTtxmZhnjxG1mljFO3NbgJFWlQ8neknSHpDbLcaybJR1QT50Fy3C8P0va/YfGY9YUOHFbMXwdEX3SWfy+AY7J3ZjOqFgSEXFWRDxWqvObNQQnbiu2Z4B1Je2czvV8GzBeUnNJf5f0iqRxko6G5O49SVdKekfSA0CntHw3SXfXHFRSX0nfuSVbUgdJL0jaJ319kqTxkt6UdGFatqQFL+ms9PxvSRqa3jlo1uQ5cVvRSKoA9gLGp0VbAadHxAbAUcCXEbElsCXwa0lrAT8FfgxsBPwa2C7d9wlgfUkd09dHAP/MOddqwAPAWRHxgKS9gP2BrSNiE+BvtYR4ZURsmX4zaA3s2zDv3Ky4nLitGFpLGgu8Ckwhmf8C4OWI+Chd3wM4PK33ErAq0BPYCbg9IqoiYjpJwiaSW3xvBQ6V1B7YFngoPVYL4HHgpIgYnZbtDvwzIr5K969tLu9dJL0kaTywK9C7Ad67WdFVlDoAK0tfR0Sf3IK0F2JhbhFwXEQ8slS9val7Wtx/AvcB/wXuSOeJAagkeVpLP+CpnOPXOZ+DpBWAq4EtImKqpHOAFep7Y2ZNgVvcViqPAL9JpxFFUi9JKwJPAwPSPvAuwC41O6Qt8OnAGcDNOccK4EhgvZxnfj4KHFkzokXSKkudvyZJz0nnn847csWsKXGL20rlBmBN4PX0ouBskj7pu0m6LcaTPMvzqaX2GwZ0XPo5iBFRJWkAcJ+keRFxtaQ+wKuSvgEeBE77tnrMlXR9ep7JJNPzmmWCZwe0TJF0JfBGRNxYb+Xa978PuCQinmzYyMwaj1vclhmSXiPpJz/xB+5/E9AGeLYh4zJrbG5xm5lljC9OmplljBO3mVnGOHGbmWWME7eZWcY4cZuZZcz/B2exk5uKv+ssAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='g')\n",
    "\n",
    "ax.set_title('Dane pierwotne - Klasa T - NN')\n",
    "plt.ylabel('Rzeczywiste')\n",
    "plt.xlabel('Predykcja')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6fb48aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.477\n",
      "Precision: 0.493\n",
      "F1score: 0.478\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'],  average='macro')\n",
    "recall = recall_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'],  average='macro')\n",
    "f1score=f1_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'], average='macro')\n",
    "print('Recall: %.3f' % recall)\n",
    "print('Precision: %.3f' % precision)\n",
    "print('F1score: %.3f' % f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdf5ee",
   "metadata": {},
   "source": [
    "## dr. 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e8621312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39946012837547457,\n",
       " 0.4590458453261094,\n",
       " 0.5268597934535271,\n",
       " 0.5028003613369466,\n",
       " 0.2530650604468367]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b7fe715b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4282462377877789"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cvscores)/len(cvscores) #0.032977654217233775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60620ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(dane_do_zapisu['obs'], dane_do_zapisu['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7cb27479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='g')\n",
    "\n",
    "ax.set_title('Dane pierwotne - Klasa T - NN')\n",
    "plt.ylabel('Rzeczywiste')\n",
    "plt.xlabel('Predykcja')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc41f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.450\n",
      "Precision: 0.458\n",
      "F1score: 0.449\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'],  average='macro')\n",
    "recall = recall_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'],  average='macro')\n",
    "f1score=f1_score(dane_do_zapisu['obs'], dane_do_zapisu['pred'], average='macro')\n",
    "print('Recall: %.3f' % recall)\n",
    "print('Precision: %.3f' % precision)\n",
    "print('F1score: %.3f' % f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0ae0a691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001A387712708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <keras.regularizers.L2 object at 0x000001A38E446608> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <keras.regularizers.L2 object at 0x000001A38E935608> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <keras.regularizers.L2 object at 0x000001A38E95BE08> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <keras.regularizers.L2 object at 0x000001A38FDDA548> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <keras.regularizers.L2 object at 0x000001A38FE05D88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001A393D795E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A390032C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A390032CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A38E93C1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A38E93C318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A390032EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A38E93C9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A38E93CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A38E93CDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A38FE07318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001A38FE07438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "model.save('D:/STUDIA/ROK_II/Magisterka/Modele/Dane pierwotne/Modele sieci neuro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601291eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
